avg_salary,body,company,location,salary,title
,"DATA ENGINEER

NEW YORK

As a Data Engineer at our firm, you will play a critical role in managing vast amounts of data that drive our automated trading platform. You will be tasked with developing architectures that will help qualify, process, normalize, and integrate data into machine learning systems. You will be the advocate of all things data and its potential to drive our research and trading efforts.

The nature of the problems we work on are really challenging, hence we hire some of the world's top talent to develop novel AI methods and trading strategies. Our team of talented researchers and technologists have been recognized as leaders in their field. We are passionate about hiring the best and the brightest, empowering them with the tools and mentorship needed to be successful. Our environment is highly collaborative and open, fostering innovation and growth.

If you possess the following, we would love to explore what is available for you with our team:
• Bachelor's degree... in Computer Science or related field from a top tier university
• 3+ years' experience in data engineering or related software engineering
• Demonstrable experience with Python and SQL. C++14/17 is beneficial
• Experience with columnar time series databases such as KDB would be welcome
• Experience with distributed systems (Condor, Hadoop, Spark) in a Linux environment
• Experience with data warehousing, ETL processes, and database schema design
• Ability to troubleshoot any performance, system, or data-related issue
• Previous experience in the design, optimization, and support of big-data ecosystems
• Strong communication skills to effectively collaborate with machine learning researchers, traders, and other engineers
• Have an interest and enthusiasm for learning about financial markets (previous experience not required)

While we are serious about our work at our company, we also promote a fun startup environment!

You can expect:
• Ping pong and poker games
• Team outings, such as go-karting and cooking classes
• Power stretching and floatation therapy
• Unlimited office snacks
• Free breakfast, lunch, and dinner
• Gym Membership
• Meditation",Quantitative Systems,"New York, NY",,Data Engineer for Trading Firm
164.1,"The Role

At WeWork, data sits in the center of our business, providing insights into the effectiveness of our physical and digital product & features. We believe data brings everything together and it is the only way we make decisions.

As a Data Engineer focused on Data Modeling in the Engagement Mission, you will build robust ETL pipelines and data models which power the products that foster connections between our members and the things that make them happier & more successful.

You will work closely with other engineers, analysts and product managers to evolve WeWork’s member experience by delivering technology that allows us to create intelligent environments and connected, consciously-engineered communities in all of our spaces.

Who You Are

You're a data engineer with extensive experience designing, implementing data solutions for various business challenges, and leading database and data warehousing initiatives.

You're motivated to enable and collaboration with engineering... data science, machine learning, and product teams to tackle the most challenging business needs of a hyper-growth business.

Responsibilities
• Build well-structured data models, using the right tool for the job
• Implement systems tracking data quality and consistency
• Develop tools supporting self-service data pipeline management (ETL)
• Provide analysts and data scientists with technical support related to data modeling
• Work closely with infrastructure teams on design and implementation of data solutions
• Work with various team and executive stakeholders to define mission-critical metrics and key performance indicators (KPIs)
• Drive training on metrics and self-service data platforms to enable a stronger data culture
• Own and provide BI development tools for internal use
• Help implement and maintain a comprehensive data dictionary

Requirements

We are after individuals who are highly motivated, intelligent, and have a great track record. In addition to strong analytical and quantitative skills, you should have:
• 5+ years functional experience in a data engineering or business intelligence role
• Proficient in data modeling and systems design skills
• Proficient in any flavor of SQL
• Proficient in at least one scripting language (ideally Python)
• Experience building and maintaining robust ETL pipelines
• Experience with version control systems (we use Git)
• Experience in at least one of our BI platforms (Looker and Tableau)
• Excellent written and verbal communication skills
• Demonstrable ability to collaborate with data scientists and software engineers, as well as with non-technical business users
Nice to Have
• Experience with AWS
• Experience with Apache Airflow, Spark
• You enjoy telling a compelling story with data
WeWork Mission

WeWork is the platform for creators, providing hundreds of thousands of members across the globe space, community, and services that enable them to do what they love and craft their life's work. Our mission is to build a world where people work to make a life, not just a living, and our own team members are central to that goal.

We are building software for physical space that uses data and technology at each point in the building process, such that it can make better sourcing decisions, improve its designs, build faster and smarter, and ultimately improve the lives of the people working within its buildings.

Our state of the art technologies and dedication to technological innovation are key reasons why WeWork has been able to scale from operating one workspace location to more than 210 in just over eight years.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status",WeWork,"New York, NY",$91.6k–145k,Data Engineer
191.5,"Manager, Data Engineering, Mass Media Entertainment Company

New York, NY

$140,000 - $200,000 + excellent benefits

THE COMPANY

Are you looking for an exciting opportunity to leverage technology at mass media entertainment company? The perfect person for this role is an experienced hands-on data engineer who will be lead and contribute to the collaboration with other teams to design and build data-forward solutions.

THE ROLE

This is the perfect role for you if you are a big picture thinker who is keen to solve problems and develop data solutions. You are an excellent communicator and confident presenting technical details to colleagues ranging from junior engineers to senior-level management. This is a very hands-on role with the Engineering team.

In particular you can expect to be involved with the following:
• Developing efficient and robust programming in Python or Scala
• Play a leading role in designing and building out Big Data solutions
• Application and development of... cloud architecture and data pipelines
• Managing a small team
• Working in a prodcution environment
• Leading the entire build out of backend data structures

YOUR SKILLS AND EXPERIENCE

The successful Data Engineer will likely have the following skills and experience:
• Bachelor’s or Master’s degree in Computer Science, Engineering, or related technical disciplines
• Very advanced expertise in programming with Java, Python, or Scala
• Highly experienced in Big Data solutions, particularly Hadoop and Spark
• Exposure to cloud computing technologies such as AWS
• Have experience in Machine Learning and AI development

THE BENEFITS

The salary for this role is up to $200,000 depending on your experience. You will be privy to great benefits and a fantastic healthcare package.

HOW TO APPLY

If you are interested, please express your interest by applying via the link on this page.

KEYWORDS

Engineer | Python | AWS | ETL | SQL | Hadoop | Engineering | Data | Analytics | Startup | HealthCare ",Harnham,"New York, NY",$107k–169k,Manager Data Engineering
164.1,"The Company:

GIPHY is the world’s largest platform to search, discover, and create all the GIFs. We're the leading brand in short-form entertainment and visual communication.

GIPHY is integrated in iMessage, Facebook, Instagram, Snapchat, Twitter, Tinder, Slack, WhatsApp and many more, serving over 7 billion GIFs daily to over 500 million people across the globe. We are also creating the largest distributed ad platform for short form media.

We’re a creative and passionate group of GIF-obsessed individuals continuing to build out what we believe is the future of communication. We have big goals and are looking for like-minded, talented people to join us.

The Opportunity:

We’re looking for a Data Engineer who thrives at the intersection of data, product, and engineering. The ideal candidate will have a strong technical and analytical background.

You will provide insight/support to the product and engineering teams that power both internal and external products including GIPHY.com... GIPHY mobile apps, and our many API integrations such as Facebook, Instagram, Snapchat, and Slack. Our public API services billions of requests and traffics many petabytes of media per month.

You'll be joining a wildly creative, fun and talented group of individuals who are passionate about building out the future of micro entertainment.

What You'll Do:
• Architect pipelines and infrastructure to manage the growing scale and diversity of data at GIPHY
• Design and build systems that measure and maintain data integrity
• Collaborate across the engineering organization to lead data-driven initiatives and define data engineering best practices
• Support data scientists in doing in-depth investigations leveraging large and complex data sets
• Work with languages such as Scala, Python; and tools/libraries such as Spark, Databricks, Luigi, Redshift, Athena, Elasticsearch, Kinesis, Kubernetes/Docker

Who You Are:
• Requirements 2+ years data engineering experience
• BA/BS in Computer Science, Math/Finance, Physics, Applied Economics, Statistics or other technical field preferred or relevant experience
• Strong communication skills, creative problem-solving and curiosity is a must
• Ability to decompose large, complex problems into smaller actionable parts
• Experience with open source data technology preferred
• AWS experience a plus
• Sense of humor and love of GIFs

This is a full time salaried position, including stock options, fully covered health insurance, 4% 401K match, 4 month maternity leave (additional 2 months of transition), 1 month paternity leave (additional 2 months of transition), free lunch every day, free gym membership and lots of other fun perks",GIPHY,"New York, NY",$91.6k–145k,Data Engineer
202.0,"Help us Build the Future of Money
Gemini Trust Company, LLC (Gemini) is a licensed digital asset exchange and custodian. We built the Gemini platform so customers can buy, sell, and store digital assets (e.g., Bitcoin, Ethereum, and Zcash) in a regulated, secure, and compliant manner.
Digital assets and blockchain technology have the power to transform the world for good. This truth, along with our core values, form the bedrock of our company and culture. At Gemini, no job is too small and no project too big as we endeavor to build the future of money. We are a mission-driven, team-based, inclusive, and determined community of thought leaders who invest in each other and the long game. Join us in our mission!
THE DEPARTMENT: DATA ENGINEERING
THE ROLE: SENIOR DATA ENGINEER
As a member of our data engineering team, you’ll shape the way we approach data at Gemini by using your engineering, analytical and communication skills to work with teams across the business. You know how to ask the... right questions and are passionate about using data to support and drive informed business decisions. You are ready to roll up your sleeves and are excited to take on challenging opportunities and projects. You’ll mentor data engineers and analysts and guide our internal teams to use data to improve the product and achieve KPIs. Communicating your insights with leaders across the organization is paramount to success.
RESPONSIBILITIES:
Design, architect and implement best-in-class Data Warehousing and reporting solutions
Lead and participate in design discussions and meetings
Mentor data engineers and analysts
Design, automate, build, and launch scalable, efficient and reliable data pipelines into production
Build real-time data and reporting solutions
Design, build and enhance dimensional models for Data Warehouse and BI solutions
Research new tools and technologies to improve existing processes
Develop new systems and tools to enable the teams to consume and understand data more intuitively
Partner with engineers, project managers, and analysts to deliver insights to the business
Perform root cause analysis and resolve production and data issues
Create test plans, test scripts and perform data validation
Tune SQL queries, reports and ETL pipelines
Build and maintain data dictionary and process documentation
MINIMUM QUALIFICATIONS:
7+ years experience in data engineering with data warehouse technologies
7+ years experience in custom ETL design, implementation and maintenance
7+ years experience with schema design and dimensional data modeling
Experience building real-time data solutions and processes
Experience building and integrating web analytics solutions
Advanced SQL skills is a must
Skilled in programming languages Python and/or Java
Experience with one or more MPP databases(Redshift, Bigquery, Snowflake, etc)
Experience with one or more ETL tools(Informatica, Pentaho, SSIS, Alooma, etc)
Strong computer science fundamentals including data structures and algorithms
Strong software engineering skills in any server side language, preferable Python
Experienced in working collaboratively across different teams and departments
Strong technical and business communication
PREFERRED QUALIFICATIONS:
Kafka, HDFS, Hive, Cloud computing, machine learning, text analysis, NLP & Web development experience is a plus
NoSQL experience a plus
Experience with Continuous integration and deployment
Knowledge and experience of financial markets, banking or exchanges
It Pays to Work Here
We take a holistic approach to compensation at Gemini, which includes:
Competitive base salaries across all departments
Ownership in the company via profit sharing units
Amazing benefits, 401k match contribution, and flexible hours
Snacks, Perks, Wellness Outings & Events

Gemini is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity, or Veteran status. If you have a disability or special need that requires accommodation, please let us know",Gemini,"New York, NY",$114k–176k,Senior Data Engineer
201.5,"DESCRIPTION

The AWS Well-Architected Tool team is hiring Data Engineers!!

Imagine if you could help shape the future of architecture, and go on a journey where few have tread before. AWS Well-Architected aims to help our customers develop technical expertise in AWS services, learn how to architect their cloud applications, and provide a great experience for customers and partners.

AWS is one of Amazon’s fastest growing businesses. More than a million active customers, from Airbnb to SAP, use AWS Cloud solutions to deliver flexibility, scalability, and reliability. As a Data Engineer at Amazon, your Primary responsibilities will be
· Design, implement and support an analytical data infrastructure providing ad-hoc access to large datasets and computing power.
· Interface with other technology teams to extract, transform, and load data from a wide variety of data sources using SQL and AWS big data technologies.
· Creation and support of real-time data pipelines built on AWS... technologies including EMR, Glue, Kinesis, Redshift/Spectrum and Athena
· Continual research of the latest big data, elasticsearch and visualization technologies to provide new capabilities and increase efficiency
· Working closely with team members to drive real-time model implementations for monitoring and alerting of risk systems.
· Collaborate with other tech teams to implement advanced analytics algorithms that exploit our rich datasets for statistical analysis, prediction, clustering and machine learning
· Help continually improve ongoing reporting and analysis processes, automating or simplifying self-service support for customersBASIC QUALIFICATIONS

· Bachelor's degree in computer science, engineering, mathematics, or a related technical discipline
·
· 4+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets
·
· Demonstrated strength in data modeling, ETL development, and data warehousing
·
· Experience using big data technologies (Hadoop, Hive, Hbase, Spark etc.)
·
· Experience using business intelligence reporting tools (Tableau, Business Objects, Cognos etc.)
·
· Knowledge of data management fundamentals and data storage principles
·
· Knowledge of distributed systems as it pertains to data storage and computing
PREFERRED QUALIFICATIONS

· Experience working with AWS big data technologies (Redshift, S3, EMR)
·
· Proven success in communicating with users, other technical teams, and senior management to collect requirements, describe data modeling decisions and data engineering strategy
·
· Experience providing technical leadership and mentoring other engineers for best practices on data engineering
·
· Familiarity with statistical models and data mining algorithms
·
· Knowledge of software engineering best practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations
·
· Masters in computer science, mathematics, statistics, economics, or other quantitative fields.

Amazon is an Equal Opportunity-Affirmative Action Employer – Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation",Amazon,"New York, NY",$127k–149k,Data Engineer
,"Cherre is upending the real estate sector with a cutting edge data science approach to traditional real estate. We built a platform to enable data driven real estate investment. We provide both real time real estate valuation, analytics and information to help investors decide on property purchase opportunities. Over the past 3 years we have become the realtime backend to the largest MLS systems, insurance, finance approval systems. We are also known for cutting edge web applications which allow agents to design & prepare their marketing materials & portfolio & research from their browser, tablet and phone.

We are looking for an enthusiastic senior data engineer who is interested in working with a fast-growing team in building industry-leading real estate data services. You will be part of designing and implementing server side services to ingest, organize, analyze, and display real estate data and insight. You will be working in a small team and be a real partner in the design and... implementation of all aspects of our product.

You will
• Develop and implement ETL processes
• Design data warehouse solutions to support ETL processes and data analytics applications
• Write SQL/NoSQL database queries, stored procedures, triggers, user defined functions, analytic functions, etc.
• Own features that you develop end to end, develop and test your code, implement new processes in production, and maintain and support them over time
• Drive our data platform and help evolve our technology stack and development best practices
• Develop and unit test assigned features to meet product requirements

Required
• BS/MS/PhD in CS or related field
• Strong experience in database technologies and data warehousing
• Strong experience in Javascript or Python
• Experience with service oriented architecture and good understanding of distributed systems, data stores, data modeling, and indexing (experience with Event Sourcing and/or CQRS preferred)
• Hands on experience developing APIs and SDKs
• Hands on experience with MongoDB, PostgreSQL, and large-scale distributed storage and database systems
• Ability to deal with ambiguity and communicate well with both technical and non-technical teams

We'd love the following experience
• React
• GraphQL
• Elasticsearch
• BI tools (i.e. Looker)

Our culture isn't for everyone and that's ok. Our goal is to find people who share our values and can enhance our environment by teaching us the best parts of what they've learned previously.

At the top of the mountain we are all snow leopards - Hunter S. Thompson

Note to Agencies: Please don't send us unsolicited resumes. If we don't have an agreement in place, we will consider that submission a donation. Thank you for understanding",Cherre,"New York, NY",,Senior Data Engineer
,"Our data engineers build tools and infrastructure to gather, prepare, analyze, and visualize art auction data. They work closely with data scientists and software engineers to productionize models, and with art experts to incorporate domain knowledge.

The data is composed of numerical, categorical, image, text, and temporal variables stored primarily in Postgres and analyzed in Docker images on a Kubernetes cluster on Google Cloud. We use the Python data science and machine learning stack: Python 3, jupyter, pandas, numpy, scikit-learn, seaborn, PyTorch, TensorFlow, NLTK, gensim, SpaCy, OpenCV, PIL, and others.

Responsibilities

We’re looking for a Data Engineer to:

- Work with our software engineers to refine the data pipeline and productionize models

- Interface with the operations team to incorporate art domain expertise into our models

- Perform analysis and investigations of our data to better understand what drives art prices

- Build out infrastructure, testing, model... validation, and data visualization systems

- Incorporate knowledge and algorithms from many different domains

Qualifications

Candidates ideally have most of the following:

- BS/MS in Computer Science, Data Science, Engineering or some equivalent

- 1-2 years of work experience, preferably startup experience

- Software architecture experience

- Experience working with Data Science Python libraries

- Experience working in professional environments

- Intrinsic motivation and the ability to learn quickly

- Strong communication skills and ability to work with a team

- Experience working at a start-up is a plus

- Art knowledge or industry experience is a plus

- Business/finance knowledge is a plus

Benefits

- Unlimited Vacation

- 401K

- Health Insurance

- Snacks and team bonding events

- Flexible scheduling and option for remote work if needed

Location

All positions are competitively compensated and based in SoHo, NYC",Arthena,"New York, NY",,Data Engineer
,"Our data engineers build tools and infrastructure to gather, prepare, analyze, and visualize art auction data. They work closely with data scientists and software engineers to productionize models, and with art experts to incorporate domain knowledge.

The data is composed of numerical, categorical, image, text, and temporal variables stored primarily in Postgres and analyzed in Docker images on a Kubernetes cluster on Google Cloud. We use the Python data science and machine learning stack: Python 3, jupyter, pandas, numpy, scikit-learn, seaborn, PyTorch, TensorFlow, NLTK, gensim, SpaCy, OpenCV, PIL, and others.

Responsibilities

We’re looking for a Data Engineer to:

- Work with our software engineers to refine the data pipeline and productionize models

- Interface with the operations team to incorporate art domain expertise into our models

- Perform analysis and investigations of our data to better understand what drives art prices

- Build out infrastructure, testing, model... validation, and data visualization systems

- Incorporate knowledge and algorithms from many different domains

Qualifications

Candidates ideally have most of the following:

- BS/MS in Computer Science, Data Science, Engineering or some equivalent

- 1-2 years of work experience, preferably startup experience

- Software architecture experience

- Experience working with Data Science Python libraries

- Experience working in professional environments

- Intrinsic motivation and the ability to learn quickly

- Strong communication skills and ability to work with a team

- Experience working at a start-up is a plus

- Art knowledge or industry experience is a plus

- Business/finance knowledge is a plus

Benefits

- Unlimited Vacation

- 401K

- Health Insurance

- Snacks and team bonding events

- Flexible scheduling and option for remote work if needed

Location

All positions are competitively compensated and based in SoHo, NYC",Arthena,"New York, NY",,Data Engineer
,"About Your Role:

As a Data Engineer, you will empower users at Dotdash to make data-driven decisions and gain valuable insights into our business. The DataOps team facilitates the free and open use of data across the organization and encourages data culture through weekly meetings, outreach and technical consulting. We are a small, passionate team looking for a data geek with a broad skill set and data chops.

About Your Contributions:
• Build and maintain data integrations
• Deploy code and machines to AWS to create and improve our existing data tools
• Curate our Data Lake to constantly improve our user experience
• Understand scalability issues and adapt systems to evolving user and business needs
• Find ways to use new and exciting technology to solve engineering challenges

About You:
• Experience building data integrations with Python, Spark or Java
• Experience with engineering in the Cloud – preferably on AWS
• Experience with Hadoop, Hive, Spark and/or AWS EMR
• Comfortable... writing SQL queries to analyze data
• Proficient in the Linux CLI
• Ansible, Puppet, Chef experience a plus
• Strong analytical and problem solving skills
• Bachelor’s degree in Computer Science, Computer Engineering, Information Technology, or a closely related technical field or foreign equivalent

About Us:

For more than 20 years, Dotdash brands have been helping people find answers, solve problems, and get inspired. We are one of the top-20 largest content publishers on the Internet according to comScore, a leading Internet measurement company, and reach more than 30% of the U.S. population every month. Our brands collectively have won more than 20 industry awards in the last year alone and, most recently, Dotdash was named Publisher of the Year by Digiday, a leading industry publication. Our brands include Verywell, The Spruce, The Balance, Investopedia, Lifewire, Byrdie, MyDomaine, TripSavvy and ThoughtCo.

Dotdash embraces inclusivity and values our diverse community. We are committed to building a team based on qualifications, merit and business need. We are proud to be an equal opportunity employer and do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status",Dotdash,"New York, NY",,Data Engineer
164.1,"About Your Role:

As a Data Engineer, you will empower users at Dotdash to make data-driven decisions and gain valuable insights into our business. The DataOps team facilitates the free and open use of data across the organization and encourages data culture through weekly meetings, outreach and technical consulting. We are a small, passionate team looking for a data geek with a broad skill set and data chops.

About Your Contributions:
• Build and maintain data integrations
• Deploy code and machines to AWS to create and improve our existing data tools
• Curate our Data Lake to constantly improve our user experience
• Understand scalability issues and adapt systems to evolving user and business needs
• Find ways to use new and exciting technology to solve engineering challenges

About You:
• Experience building data integrations with Python, Spark or Java
• Experience with engineering in the Cloud – preferably on AWS
• Experience with Hadoop, Hive, Spark and/or AWS EMR
• Comfortable... writing SQL queries to analyze data
• Proficient in the Linux CLI
• Ansible, Puppet, Chef experience a plus
• Strong analytical and problem solving skills
• Bachelor’s degree in Computer Science, Computer Engineering, Information Technology, or a closely related technical field or foreign equivalent

About Us:

For more than 20 years, Dotdash brands have been helping people find answers, solve problems, and get inspired. We are one of the top-20 largest content publishers on the Internet according to comScore, a leading Internet measurement company, and reach more than 30% of the U.S. population every month. Our brands collectively have won more than 20 industry awards in the last year alone and, most recently, Dotdash was named Publisher of the Year by Digiday, a leading industry publication. Our brands include Verywell, The Spruce, The Balance, Investopedia, Lifewire, Byrdie, MyDomaine, TripSavvy and ThoughtCo.

Dotdash embraces inclusivity and values our diverse community. We are committed to building a team based on qualifications, merit and business need. We are proud to be an equal opportunity employer and do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status",Dotdash,"New York, NY",$91.6k–145k,Data Engineer
167.5,"At Bloomberg, our technical leaders view every problem as an opportunity to change the status quo. They have an entrepreneurial spirit and elevate those around them. They wade through the technology to focus on solving real business problems and creating new products.
Our Team

Enterprising – and not defined by conventional roles. Our goal is to innovate, and therefore we welcome diverse skills, experiences, and thinking. In Global Data’s Technical Operations, our focus is on data engineering and how data is acquired, processed and validated.
This is not your typical Data Engineering position.

We are looking for a Data Engineering Tech Lead to own the technical direction of our team from the ground up. You will be a developer, an architect and a thought-leader of our team focused on system integrations and data enrichment. Your projects will be focused on evolving our data processing capabilities to allow us to on-board and manage data efficiently and accurately. You’ll collaborate... with engineers, data analysts and business stakeholders to solve large-scale problems.
We’ll trust you to:
• Lead the development of systems that span across open source, proprietary tools and legacy applications.
• Build reusable client libraries for interacting with internal APIs and systems
• Design data driven processes leveraging enterprise-wide ML and NLP solutions for information extraction and anomaly detection.
• Engage with the broader technical community within Bloomberg and represent Global Data at industry engagements including conferences, meetups, hackathons, blogs and written articlesAs a leader, we expect you to:
• Facilitate collaboration and provide strategic leadership from single component design to the overall architecture and strategy of our technology stack
• Provide technical guidance and mentorship to junior team members, leading by example and promoting best software development practices such as continuous integration and deployment
• Communicate and maintain relationships with non-technical business stakeholders throughout development lifecycleYou’ll need to have:
• A BA/BS degree or higher in Engineering, Information Systems, Mathematics, or relevant data technology field
• A proven passion for teaching and motivating
• Demonstrated history of collaborating across varied groups of people, including both technical and non-technical audiences
• Previous engagement experience within the industry and open source community
• You have 2-5 years of professional Python development experience and familiarity with distributed systems, REST APIs, databases and linux",Bloomberg LP,"New York, NY",$75k–185k,Data Engineer - Tech Lead
,"We’re looking for a data engineer to join our 6-person engineering team. Here, you’ll work on our comprehensive metrics & data analytics platform. Our team has built a lot without having prior data science experience, but as we scale internationally, we need a talented data engineer to take us to the next level.

You will be the go-to on all matters data engineering, owning projects from technical design to execution and working closely with our product & client teams to create products you’ll be proud of. The vast majority of our business decisions are powered by our data, and our clients see it as a core reflection of our business.

Why Join

- You love responsibility, ownership, and making decisions.

- You’re ambitious and move fast.

- You want to contribute throughout the company, not just within engineering.

- You think voice will be as big as mobile, and you want to be a part of it.

What You Have

- BS degree in Computer Science, Applied Statistics, similar technical field... of study or equivalent practical experience.

- Prior leadership in a team-based technical environment.

- Experience with frameworks and technologies such as: Spark, scikit-learn, Pig, Hive, Tableau, Chart.io, etc.

- Analytical programming experience with libraries for cleaning, reshaping, exploring and visualizing data - Python preferred. Version control (Git) proficiency.

- Prior data engineering experience (1+ years) where you were involved in many aspects of the pipeline.

- Fluency in verbal and written English.

- You’ve spent hours talking with bad IVRs & voice systems and want to help us improve the experience!

Bonuses

- Experience with noSQL and GraphQL.

- DevOps knowledge & prior experience with AWS services, Ansible, Jenkins, Docker, and/or other tools.

- An entrepreneurial drive with previous startup experience under your belt.

- You’ve got passion to spare and love working on side projects.

About Us (read more @ angel.co/redroute)

- Founded @ Cornell '15

- First product launched October '17

- Grown 3x in the last 6 months

- $4M revenue/year

- 100k daily users",RedRoute,"New York, NY (+2 others)",,Data Engineer
,"Spotify is looking for a Data Engineer to join our RoaR engineering org. We work with royalties and reporting which means that we make sure that the rights holders (record labels, publishers and collecting societies) are getting paid correctly for all the content that is streamed every day on Spotify globally. Product built within RoaR have a lot of impact directly on the financial numbers for the whole Spotify. Which is both a big challenge in verifying the correctness of the numbers and also a big source of satisfaction when we are making improvements that impact Spotify’s margin.

You will build data-driven solutions to bring music and digital media experiences to our 100 million active users and millions of artists either by working directly on product features, publishing and insight tools for artists, or by improving the quality of our data tools and large scale data infrastructure. You will take on complex data-related problems using some of the most diverse datasets available... — user behaviors, acoustical analysis, revenue streams, cultural and contextual data, and other signals across our broad range of mobile and connected platforms. Above all, your work will impact the way the world experiences music.
What you’ll do
• Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.
• Use best practices in continuous integration and delivery.
• Help drive optimization, testing and tooling to improve data quality.
• Collaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.
• Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.

Who you are
• You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.
• You know how to write distributed, high-volume services in Java or Scala.
• You are knowledgeable about data modeling, data access, and data storage techniques.
• You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.
• You understand the value of partnership within teams.

We are proud to foster a workplace free from discrimination. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our users and our creators. This is something we value deeply and we encourage everyone to come be a part of changing the way the world listens to music",Spotify,"New York, NY",,Data Engineer – Royalties
,"Spotify is looking for a Data Engineer to join our RoaR engineering org. We work with royalties and reporting which means that we make sure that the rights holders (record labels, publishers and collecting societies) are getting paid correctly for all the content that is streamed every day on Spotify globally. Product built within RoaR have a lot of impact directly on the financial numbers for the whole Spotify. Which is both a big challenge in verifying the correctness of the numbers and also a big source of satisfaction when we are making improvements that impact Spotify’s margin.

You will build data-driven solutions to bring music and digital media experiences to our 100 million active users and millions of artists either by working directly on product features, publishing and insight tools for artists, or by improving the quality of our data tools and large scale data infrastructure. You will take on complex data-related problems using some of the most diverse datasets available... — user behaviors, acoustical analysis, revenue streams, cultural and contextual data, and other signals across our broad range of mobile and connected platforms. Above all, your work will impact the way the world experiences music.
What you’ll do
• Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.
• Use best practices in continuous integration and delivery.
• Help drive optimization, testing and tooling to improve data quality.
• Collaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.
• Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.

Who you are
• You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.
• You know how to write distributed, high-volume services in Java or Scala.
• You are knowledgeable about data modeling, data access, and data storage techniques.
• You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.
• You understand the value of partnership within teams.

We are proud to foster a workplace free from discrimination. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our users and our creators. This is something we value deeply and we encourage everyone to come be a part of changing the way the world listens to music",Spotify,"New York, NY",,Data Engineer – Royalties
,"Come to our newest innovation center at 10 Hudson Yards to Help Lead Software Innovation through Game-Changing Products

You can turn great ideas into code and turn that code into groundbreaking and beautiful new applications. You also crave an opportunity to work with the best and brightest engineers, designers, and entrepreneurs to create new digital experiences in an environment that’s both collaborative, and free from micromanagement.

Here’s Your New Opportunity

We’re a global team of the most accomplished designers, engineers, and product experts you’ll find anywhere. The new business ventures we create build strategic advantages for some of the world's most important companies and help them get to the next horizon of digital innovation.

Help Build Cutting Edge Data Platforms

You can see the big picture, and get excited about building a stable, scalable system for it to run on. You also crave an opportunity to work with the best and brightest engineers, designers, and... entrepreneurs to create new digital experiences in an environment that’s both collaborative, and free from micromanagement.

As a Senior Data Engineer, you will have the following experience:

5+ years of experience in Software Engineering / BI / Data Warehouse design and development using modern Relational and NoSQL databases like MySQL, PostgreSQL, and MongoDB.

2+ year of experience in Big Data Engineering using tools like Hadoop, Hive & Map Reduce

Creating systems that ingest data from various sources

Experience with Apache Spark or similar fast/real-time tools

Workflow flexibility and strong teamwork skills

Experience with distributed systems and cloud architecture using AWS

Additional valued capabilities include:

Bachelors degree or greater in Computer Science or related areas, or equivalent practical experience

Data Modeling and Data Science experience building predictive models

Experience with Python, Unix, or statistical software packages (R, SAS) for data manipulation

Confidence with analytical tools such as Excel, R, Stata, or Matlab

To learn more, visit us at careers.bcgdv.com

Interested applicants may apply through the careers section of the website at BCGDV.com. Interviews will take place after resumes have been screened for minimum requirements. Please note that this position is not restricted solely to the responsibilities listed above and that the job scope and responsibilities are subject to change.

BCG DIGITAL VENTURES IS AN EQUAL OPPORTUNITY EMPLOYER. ALL QUALIFIED APPLICANTS WILL RECEIVE CONSIDERATION FOR EMPLOYMENT WITHOUT REGARD TO RACE, COLOR, AGE, RELIGION, SEX, NATIONAL ORIGIN, DISABILITY, PROTECTED VETERAN STATUS, OR ANY OTHER CHARACTERISTIC PROTECTED UNDER FEDERAL, STATE OR LOCAL LAW.

https://jobs.lever.co/bcg/970af3ea-b613-4651-814d-f01bbec7b5cc?lever-origin=applied&lever-source%5B%5D=AngelList",BCG Digital Ventures,"New York, NY",,Senior Data Engineer
164.1,"A Technical contributor who brings hands-on knowledge of media data and all phases in building large-scale cloud based distributed data processing systems and applications to Build and Maintain Data/Analysis Infrastructure, and Data Science models for AMRLD audience data, audience engagement data (social, web, email, mobile, etc.), and other consumer data.

This role requires partnering closely with a team of data scientists, business analysts & data engineers leading the cloud-based Big Data & Analytics strategy . The successful candidate will implement complex AWS-based big data projects with a focus on collecting, parsing, managing, analyzing, querying, and visualizing large sets of data to turn information into insights using multiple technology platforms.

This role also requires an understanding of how a secure big data cloud environment is architected to gain real insights faster, with less friction and complexity.

Key Functions:

The successful candidate will perform the... following tasks:

• Lead the design, implementation, and continuous delivery of pipelines using distributed AWS-based big data technologies supporting data processing initiatives across batch and streaming datasets

• Development and design using Python language and Big Data Frameworks such as Spark, AWS Athena, Presto, APIs, etc.

• Create Use Case analytics in our cloud data environment writing SQL, Python, and D3 queries to extract and visualize necessary data points on Nielsen's AMRLD

• Assist in building and maintaining Data Science models in AWS

• Providing administrative support on deployed AWS platform components

• Identifying, evaluating and implementing cutting edge big data pipelines and frameworks required to provide requested capabilities to integrate external data sources and APIs

• Reviewing, analyzing and evaluating market requirements, business requirements and project briefs to design the most appropriate end-to-end technology solutions

• Processing and managing high volume real time customer interaction streams

• Providing architectural support by building Proof of Concepts & Prototypes

WHAT YOU'LL LEARN:

As the media industry grapples with fragmentation and disruption, a key facet of Research is to be the driving force in helping their company capture, report on, monetize, and understand audiences. An opportunity lies in Research to work across divisions and departments to create a unified language that ensures that internal clients understand-and take advantage of-complex metrics. The key is giving the necessary context so that decisions can be made. While this sounds disarmingly simple, it is a challenge to get the majority of the company to speak the same language and understand new vernacular. Research is a central force within the broader organization in gaining an objective perspective for a wide array of complex questions. It may sound academic – and certainly there is a portion of the job that requires a love of learning – but the primary attribute that will bring you success is curiosity. For example, you'll gain a deeper understanding of what motivates someone to tune into a show, buy a product based on placement of an ad, become a fan on social media, or why certain types of programs are successful across regions of this country and abroad. Ultimately, each research candidate gets a better understanding of the rigor that goes into making business decisions by uncovering not boulders, but pebbles along the path to connecting to A E Networks core asset, its content.

YOUR STORY: [ BEHAVIORS] (what you need to have)

The successful candidate will possess the following Skills and Experience:

• 8 years professional Software Industry experience in the Media Industry (Linear Television, required, Omniture Digital experience also highly desired)

• Extensive experience with Nielsen AMRLD

• Proficiency working with structured, semi-structured, and unstructured data sets including linear data, social, web logs and real time streaming data feeds

• Extensive experience with Social, Web, Email, and other audience engagement data

• 5 years development experience with AWS services - specifically collecting, processing and analyzing large volumes of data

• Must have hands-on experience with AWS: S3 / EMR / Data Pipeline / Lambda, EC2, RedShift, Snowflake, Cloud Formation and CLI experience

• Expert in Software development and design in Python

• 5 years' experience in SQL (must), and Python (must)

• Hands on experience with Apache Spark / Hadoop

• Extensive Knowledge and Use of Visualization D3/JavaScript (must), and Tableau (must)

• Able to tune Big Data solutions to improve performance and end-user experience

• Experience in Data Science (Machine Learning/AI, Tensor Flow, Cluster Analysis, Decision Trees, Combining multiple trees into an Ensemble model, GBM/xgboost models, calculating WAPE and R-squared values, etc.)

• Expert level usage with GitHub is preferred

• Spark Developer / AWS Developer certification is a plus

• Bachelor' degree or higher in Computer Sciences or similar

THAT SOMETHING EXTRA:

If you've read this far, you're likely a great fit for us…and maybe you're fluent in sign language, have studied the circus arts or are currently writing a screenplay. If you have a special skill or backstory that might directly or indirectly help you succeed in this role, we'd love to hear about it in your cover letter",A+E Networks,"New York, NY",$91.6k–145k,Data Engineer
123.9,"The Whitney Museum seeks a Data Engineer to join its Business Systems department. The Business Systems team sets standards for data collection, accessibility, and governance to support functional department goals and inform decision making processes across the museum. The Business Systems Data Engineer is responsible for all phases of database reporting, administration, and scripting for the organization’s core constituent systems.

Responsibilities
• Responsible for designing, testing, deploying, maintaining, and documenting stored procedures, automated scripts, system configurations, and cross-system integrations
• Developing and providing ad hoc and automated reports and data extractions
• Maintain and create data flows feeding into the Customer Data Platform
• Perform baseline database administrative tasks
• Execute and oversee both manual one-time and repeated tasks
• Liaise with internal stakeholders and external partners to align on goals and priorities related to data... collection, accessibility, and governance
• Automate tasks and processes within the department to help optimize our internal workflow

Requirements
• B.S. in Computer Science, Information Technology, or a related discipline
• 5+ years developing SQL reports, stored procedures, functions
• 3+ years of experience in administering MS SQL Server 2008 or 2012
• 2+ years writing reports and maintaining SQL Server Reporting Services
• 2+ years creating data flows using Visual Studio and SQL Server Integration Services
• 1 year working with document-oriented NoSQL DB
• 1 year working with AWS, specifically RDS
• Proven manager who has the ability to work collaboratively across departments
• Project management experience a plus
• Experience in a fundraising and/or ticketing environment will be strongly considered
• Familiarity with Tableau or similar Business Intelligence and data visualization tool

Whitney’s Data Environment

Systems
• Raiser’s Edge (MS SQL Server)
• Siriusware (MS SQL Server)
• CounterPoint (MS SQL Server)
• Historical Customer Data (MongoDB)
• Customer Data Platform (Postgres hosted on AWS)

Custom Integrations
• Raiser’s Edge - Siriusware
• Siriusware – Raiser’s Edge
• Raiser’s Edge - CounterPoint
• Raiser’s Edge – Siriusware – CounterPoint to Customer Data Platform

About the Whitney

The Whitney Museum of American Art, founded in 1930 by the artist and philanthropist Gertrude Vanderbilt Whitney, houses the foremost collection of American art from the twentieth and twenty-first centuries. From her vision arose the Whitney Museum of American Art, which has been championing the most innovative art of the United States for 86 years. The core of the Whitney’s mission is to collect, preserve, interpret, and exhibit American art of our time and serve a wide variety of audiences in celebration of the complexity and diversity of art and culture in the United States. Through this mission and a steadfast commitment to artists themselves, the Whitney has long been a powerful force in support of modern and contemporary art and continues to help define what is innovative and influential in American art today.

EEO Statement

The Whitney Museum of American Art is an Equal Opportunity Employer. The Museum does not discriminate because of age, sex, religion, race, color, creed, national origin, alienage or citizenship, disability, marital status, partnership status, veteran status, gender (including gender identity), sexual orientation, or any other factor prohibited by law. The Museum hires and promotes individuals solely on the basis of their qualifications for the job to be filled. The Museum encourages all qualified candidates to apply for vacant positions at all levels. This description shall not be construed as a contract of any sort for a specific period of employment",Whitney Museum of American Art,"New York, NY",$30.4k–187k,"Data Engineer, Business Systems"
167.5,"At Bloomberg, our products are fueled by powerful information. We combine data and context to paint the whole picture for our clients, around the clock – from around the world. In Global Data, we’re responsible for delivering this data, news and analytics through innovative technology - quickly and accurately.
Our Team

Enterprising – and not defined by conventional roles. Our goal is to innovate, redefine, and break boundaries in expanding our modern data business. In Global Data’s Technical Operations, our focus is on data engineering and how data is acquired, processed, validated and stored. We’re constantly innovating to create better, more efficient systems to handle the huge variety of data we acquire and deliver to our clients.

We're looking for a Data Engineer to join our Data Engineering team in New York. You’ll be joining a small dynamic team responsible for using and developing cutting-edge technologies to enhance our acquisition, data processing workflows and automation... methods. You will design and develop systems that improve internal processes and produce unique client-facing content. You’ll work closely with data analysts, data scientists and infrastructure engineers to solve ambiguous and multi-faceted problems and lead the development of robust data systems from scoping to production.

Your projects will involve designing and implementing ETL systems, deploying machine learning and NLP algorithms, and calibrating automated quality control processes. You'll be expected to be able to work independently and to manage your own priorities, while also collaborating with other team members and colleagues across various businesses. We're looking for someone who embraces the challenge of understanding the business problems that your code is solving, so you don't just develop: you have a vision, you design, you simplify, you visualize, and you effectively communicate the business impact of your work.
As a valued member of our team, we’ll trust you to:
• Design, develop, and deploy data processing pipelines written in Python to power Bloomberg’s product offerings.
• Identify opportunities and build flexible automated solutions for data acquisition, ETL and ML pipelines, and human-in-the-loop data processing.
• Promote the use of software development best practices and DevOps by participating in technical discussions, knowledge sharing sessions, and engaging with the NYC open source software community.
• Gather requirements (and of course, refine and clarify ambiguous requirements).
• Communicate consistently with project stakeholders throughout development lifecycle.
• Collaborate with data analysts, data scientists and infrastructure engineers to develop creative solutions.
• Monitor, maintain and troubleshoot issues for the production systems you are responsible for.
• Analyze and present results through dashboards, web apps and static documentation.Successful applicants will likely possess:
• A BA/BS degree or higher in Computer Science, Mathematics, or relevant data technology field, and up to 5 years of professional work experience in software development, data engineering, data science or information technology.
• A self-starter attitude, creative problem solving skills, and experience with diverse data processes and systems.
• Applied Python development experience (additional knowledge of C++ and JavaScript is a plus).
• Familiarity with git, unix, python ecosystem, web services, and API usage.
• Experience with SQL & NoSQL database systems as well as distributed big data technologies including Hadoop and Spark.
• Experience with data science and data visualization technologies for detecting anomalies, measuring and reporting on Machine Learning systems, and telemetry systems like Splunk.
• Full spectrum of project management skills: ideation, prioritization, communication, delivery.
• Excellent communication skills, especially when explaining technical processes and solutions to business stakeholders and management",Bloomberg LP,"New York, NY",$75k–185k,Data Engineer - Technical Operations
291.0,"Senior Data Engineer - Games - 32864

Technology and Engineering - USA New York, New York

Explore and Discover Nielsen! With offices located in 110 countries, we are a global independent measurement and big data analytics company focused on your future. Nielsen Portfolio is one of three core Nielsen businesses.

The Portfolio division is defined as a high-growth, innovation-based business that supports the needs of our clients. Portfolio includes Gracenote, Nielsen Sports/Esports/Games, Nielsen Music, Brandbank, SuperData, & Nielsen Book, making Nielsen Portfolio the largest supplier of metadata, data, measurement, and insights to the global Entertainment Industry.

Our SuperData team is seeking a Senior Data Engineer:

SuperData is the leading provider of data and market intelligence on digital games and interactive media. Our research methodology is driven by behavioral and digital point-of-sale data collected directly from game publishers and developers.

SuperData publishes... industry relevant key performance indicators for benchmarking, competitive analysis, and performance optimization. We specialize in helping our clients use data to identify actionable growth and investment opportunities.

Innovation Team

The Innovation Team sits at the nexus of the Data Engineering, Insights, and Analytics teams at SuperData Research– the team specifically pioneers the data research powering new features and products. The Senior Data Engineer is a visionary in data analysis and a pragmatist in research execution. Nothing of value within a data set eludes the Senior Data Engineer. The role leads a small group of Data Scientists & Engineers.

The ideal candidate has a background in Data Science/Engineering and demonstrates mastery of data mining, statistical modeling, machine learning and predictive analytics across structured and unstructured data. Successful candidates are expected to independently design, test, and scaling out successful prototypes into products across direct reports. Strong understanding of the tools that power analytics is a must, particularly infrastructure and computer engineering concepts.

Responsibilities:

Design and execute research projects from start to finish.

Design scrapers and ETL processes to bring data into SuperData databases.

Pre-process and clean data ahead of analytics.

Optimize storage architectures and databases for fast response.

Break down large problems into modular pieces to enable parallelization of processing where possible - looping over modules slice aggregating for custom visibility.

Automate batch and cron jobs.

Initiate feedback loops with QA to inform forward strategy.

Manage Innovation Projects:

Define scope for new Innovation projects.

Review open projects weekly and provide feedback/advisory as required to junior team members.

Mentorship:

Keep abreast of developments in the field.

Enable team members with specialized learning plans.

Communication & Collaboration:

Presentation of results to stakeholders.

Scope out new opportunities with internal teams and external companies.

Key Concept Requirements:

Mastery of Python:

Data Import/Export & Manipulation – csv, json, data frames

Scraping/ETL – Selenium, Beautiful Soup, etc.

Analysis Packages

Mastery of MySQL:

Nested Queries, Joins, Temporary Tables, Stored Procedures, Events

Mastery of Algorithms & supporting Mathematics:

Big O Notation

Algorithmic Design and Modifications

Compute Fundamentals:

Processor Speed

Thread Management

Memory Fundamentals:

Database Caching

Local Caching

Memory Management

Storage Fundamentals:

NAS/Block Store Latency

Connection Pools

Database Architecture

Cloud Compute in AWS EC2:

Windows Server 2012/2016

Amazon Linux 2

This best describes you:
• You have a degree (Undergraduate or Masters) in Finance, Economics, Statistics, Business, Computer Science, or Computer Engineering.
• You are a strong problem solver – you solve problems of a particular type once, not individual problems. Your solutions are dynamic and scalable.
• You are always learning newer, faster, and better ways of doing things.
• You test and tune your own work.
• You have extreme attention to detail.
• You know what you know, you know what you don’t know, and you know how to figure out what you need to know.
• You have 5+ years’ work experience in Data Science/Data Engineering roles or equivalent.
• You are proficient in English.

Nice-to-have:
• Familiarity with video games and understanding of how games earn revenue a plus. (Virtual economies, exchange rates, how to make a purchase, what virtual storefronts look like, etc.)
• Impress us with a ridiculous high-score in your favorite game, your gaming stats, your unbeatable Magic deck, or your gaming rig specs.
• Show us analytics websites, tools, databases, or dashboards you have developed have collaborated on.

About Nielsen:

Nielsen N.V. (NYSE: NLSN) is a global performance management company that provides a comprehensive understanding of what consumers Watch and Buy. Nielsen’s Watch segment provides media and advertising clients with Total Audience measurement services across all devices where content — video, audio, and text — is consumed. The Buy segment offers consumer packaged goods manufacturers and retailers the industry’s only global view of retail performance measurement.

By integrating information from its Watch and Buy segments and other data sources, Nielsen provides its clients with both world-class measurement as well as analytics that help improve performance. Nielsen, an S&P 500 company, has operations in over 100 countries that cover more than 90 percent of the world’s population. For more information, visit www.nielsen.com

Nielsen is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class.

Job Type: Regular

Primary Location: New York,New York

Secondary Locations: , , ,

Travel: No",Nielsen,"New York, NY",$189k–204k,Senior Data Engineer - Games
167.5,"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.Do you like working with big data? Do you want to use data to influence product decisions for products being used by hundreds of millions of people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet... Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. You will be working with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.

This is a full time position based in our office in New York.

Responsibilities:
• Inform, influence, support, and execute our product decisions and product launches
• Manage data warehouse plans for a product or a group of products.
• Interface with engineers, product managers and product analysts to understand data needs.
• Partner with Product and Engineering teams to solve problems and identify trends and opportunities.
• Build data expertise and own data quality for allocated areas of ownership.
• Design, build and launch new data extraction, transformation and loading processes in production.
• Support existing processes running in production.
• Define and manage SLA for all data sets in allocated areas of ownership.
• Work with data infrastructure to triage infra issues and drive to resolution.

Mininum Qualifications:
• BS/BA in Technical Field, Computer Science or Mathematics.
• 4+ years experience in the data warehouse space.
• 4+ years experience in custom ETL design, implementation and maintenance.
• 4+ years experience working with either a Map Reduce or an MPP system.
• 4+ years experience with schema design and dimensional data modeling.
• 4+ years experience in writing SQL statements.
• Ability to analyze data to identify deliverables, gaps and inconsistencies.
• Communication skills including the ability to identify and communicate data driven insights.
• Ability in managing and communicating data warehouse plans to internal clients.

Preferred Qualifications:
• 4+ years experience using Python or Java",Facebook,"New York, NY",$75k–185k,"Data Engineer, Analytics"
164.1,"About Foursquare:
Since our inception in 2009, Foursquare has been a leading force in changing how location information enriches our real-world and digital lives. As a location intelligence company, Foursquare is comprised of two well-known consumer apps, Foursquare and Swarm, as well as thriving media and enterprise products. Our B2B offerings include Places (for developers), Pinpoint and Attribution (for marketers), and Place Insights (for analysts, based on the world's largest foot traffic panel). With more than 200 people across our offices in New York, San Francisco, and in sales offices around the globe, we’re dedicated to our trailblazing mission—enriching consumer experiences and informing business decisions with location intelligence.
About our Engineering Team:
As a member of Foursquare’s engineering team, we want you to bring experience building real products from the ground up. We're passionate about tackling tough challenges in the location space and look for others who... like to dive deep into code and help solve hard problems. You should be comfortable running with your own ideas and eager to learn new skills on a bleeding edge platform. We use a variety of tools, technologies, and languages to build software (Scala, Thrift, MongoDB, Memcached, JS/jQuery, Kafka, Pants, Hadoop, MR, Spark) but experience with equivalent ones will do just fine.
Join us and help bring our ideas (and your own!) off the whiteboard and into reality. You'll be a key member of our Attribution team, building a system that builds hundreds of machine learning models per day at scale to drive marketing decisions for many well-known companies. You'll build resilient services and tooling which drive all of our processing of petabytes of data
Responsibilities:
Develop and maintain our data pipelines using Hadoop, Scalding, Luigi, Spark, Mongo and more
Partner with the Data Science team to investigate and implement advanced statistical models and machine learning pipelines
Identify and implement performance improvements across all pipelines
Data investigations to validate assumptions or find the source of a problem
Assist client support and sales with client integrations
Qualifications:
3+ years of experience working with Hadoop MapReduce and/or other big data technologies and pipelines
You have a solid foundation in computer science fundamentals with particular expertise in data structures, algorithms, and design
You obsess over data: everything needs to be accounted for and be thoroughly tested
You are constantly thinking of ways to squeeze better performance out of the pipelines
Strong Java or other object-oriented programming experience or, even better, experience and/or interest in functional languages (we use Scala!)
Experience with Scala, Scalding, Luigi,Hive, machine learning pipelines and model training is a plus
Bachelors Degree or higher in Computer Science, Electrical Engineering or related field
Foursquare is proud to foster an inclusive environment that is free from discrimination. We strongly believe in order to build the best products, we need a diversity of perspectives and backgrounds. This leads to a more delightful experience for our users and team members. We value listening to every voice and we encourage everyone to come be a part of building a company and products we love.
Foursquare is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other characteristic protected by law",Foursquare,"New York, NY",$91.6k–145k,Data Engineer
164.1,"Want to help people get the financial protection they need — and feel confident in their choices? Policygenius is a NYC-based tech startup that makes it easy to compare and buy insurance online. Since 2014, we’ve raised over $52 million of venture capital, established ourselves as a pioneer in Fintech and helped more than 4.5 million people get vital coverage for their families.

We're rapidly growing and looking for people with grit, great attitudes, and creative problem-solving skills to join our powerhouse team. Come see why we were voted one of INC's best workplaces of 2018!

When you're a Data Engineer at Policygenius…

Policygenius continues to disrupt the insurance industry by delivering innovative technology-driven experiences. Our talented yet humble software engineering team is dogma-free and experiment-driven. We are relentless in our drive to reliably deliver outstanding products at scale. We are growing fast, but we can go further faster with experienced, collaborative... challenge-seeking engineers like yourself.

As a Data Engineer, your mission will be to build, deliver, and maintain the data warehouse and ETL pipelines. You will be playing a critical role in helping business stakeholders making decisions rooted in data (good data). Other engineers will rely on you to advise on data modeling, database optimizations, and business stakeholders will look at you to deliver reliable, consistent data for tracking business performance. At Policygenius, you will work alongside passionate engineers who continuously strive to improve our world, our teams and themselves.

You will...
• Design, develop and maintain Policygenius’ analytics infrastructure
• Build and deploy batch ETL pipelines using Airflow
• Develop documentation and data dictionaries to ensure clarity and consistency
• Collaborate and train other engineers to code with data in mind
• Support internal stakeholder groups like Business Intelligence and Product in using data to solve business problems
• Work with cutting edge technology, expand your toolset, and share your expertise

You have...
• 1+ years of experience with a public cloud provider (e.g. GCP, AWS, Azure, etc.)
• 2+ years of relevant experience in building and maintaining data warehouse, data pipelines, ETL systems
• Solid understanding of OLTP vs. OLAP databases and proven experience in data modeling
• Experience using BI tools (e.g. Tableau, Domo, Looker, etc.) and analytics tools (e.g. MixPanel, Segment, etc.)
• Proficiency in at least one programming language (e.g. Python, Java, Go, etc.)
• Knowledge of basic Machine Learning concepts and experimentation design
• Strong communication skills and the ability to comfortably articulate your thoughts and decisions
• Experience working in an agile environment

You’ll get...
• Company-paid health, dental, vision, life & disability insurance
• 401(k) plan, FSA & commuter benefits
• Flexible PTO
• Training, mentorship, and coaching from leadership
• The opportunity to grow alongside a company shaking up a big, old-fashioned industry
• Fun, diverse, open-minded coworkers
• Dog companionship
• Some fun surprises when you join… (Shhh… It’s a secret!)

Technologies You Will Use

Airflow, PostgreSQL, BigQuery, Google Functions, Tableau, Python, Node.js, Go, Ruby on Rails, Jupyter Notebook, Docker, Kubernetes, Terraform, BigQuery, gRPC, Buildkite, Microservices, GraphQL, Git

Policygenius currently spends much of its time using these tools, but we’re committed to working on the right tech for the job, and always open to fresh ideas, new technologies, and better ways of getting things done",Policygenius,"New York, NY",$91.6k–145k,Data Engineer
,"Spotify is looking for Data Engineers to join our Free Product team. You will build infrastructure to help scale Spotify’s Free Product and Ads ranking/targeting capabilities. This is an opportunity to work with massive scale real time data and build solutions to run predictive algorithms on production.

You will build data-driven solutions to bring music and digital media experiences to our 100 million active users and millions of artists either by working directly on product features, publishing and insight tools for artists, or by improving the quality of our data tools and large scale data infrastructure. You will take on complex data-related problems using some of the most diverse datasets available — user behaviors, acoustical analysis, revenue streams, cultural and contextual data, and other signals across our broad range of mobile and connected platforms. Above all, your work will impact the way the world experiences music.
• Please note: this posting represents multiple roles... across various teams, including a range of responsibilities and experience level*
What You’ll Do
• Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.
• Use best practices in continuous integration and delivery.
• Help drive optimization, testing and tooling to improve data quality.
• Collaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.
• Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.

Who You Are
• You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.
• You know how to write distributed, high-volume services in Java or Scala.
• You are knowledgeable about data modeling, data access, and data storage techniques.
• You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.
• You understand the value of partnership within teams.
• You are willing to work from our awesome office in New York. We offer relocation packages if you do not currently live in New York.

We are proud to foster a workplace free from discrimination. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our users and our creators. This is something we value deeply and we encourage everyone to come be a part of changing the way the world listens to music",Spotify,"New York, NY",,Data Engineer- Free Product
194.0,"New York Life Investments (NYLIM), an indirect, wholly owned subsidiary of New York Life Insurance Company, is a top 25 global asset management firm. With more than $500 billion in assets under management, NYLIM is a premier investment management firm serving a variety of client segments including retail, institutional, insurance and defined contribution and benefit on a global basis. New York Life Investments offers a diverse set of investment capabilities ranging from traditional equity and fixed income to alternative investment strategies and multi-asset solutions. Renowned for its premier investment acumen and client focus, NYLIM’s vision is to be one of the most trusted providers of investment management expertise and long-term financial security.

We are looking for a full stack Data Scientist to join New York Life Investment Management’s Business Intelligence & Analytics (BI&A) team. The BI&A team is a stand-alone center of excellence supporting our Retail Investment Management... business across Distribution, Marketing, Service and Senior Management. We partner with and support each area, leveraging data to drive business strategy, decision making and support of our distribution efforts.

This role requires an individual with a strong sense of ownership, technical and analytical expertise as well as the ability to effectively interact with both business and IT teams. The person in this role will be a data expert who will build and grow data science product/platform that enable better analytics across our data-driven organization.

Responsibilities:
• Identify, analyze, and interpret trends or patterns in complex data sets and develop graphs, reports, visualization and presentations of results and insights
• Responsible for research, delivery, implementation and support of data and analytical solutions
• Work on end-to-end model development; Use cutting-edge tools and machine learning techniques to develop propensity models, recommendation engines, client segmentation etc.
• Design and develop data marts, model pipelines, automated workflows, and analytical processing solutions in big data ecosystems

Qualifications:
• Master’s degree with 3+ years’ experience required, PhD preferred, in a quantitative discipline (Computer Science, Mathematics, Statistics, Economics, Physics, Engineering or related discipline
• Financial industry experience a plus
• Strong experience in Big Data processing using Spark, Hive, shell scripting, etc.
• Proficiency in SQL and one of Python/R for programming and modeling
• Strong data visualization skills for ""story-telling"" through data; Experienced in Tableau a plus
• Knowledge of machine learning algorithms and model development
• Strong analytical, problem-solving and organizational skills; Attention to detail a must
• Strong communication skills
• Ability to build relationships and work effectively with diverse stakeholders at all levels
• Ability to work efficiently in a fast-paced environment

#LI-MD1

EOE M/F/D/V

If you have difficulty using or interacting with any portions of this Web site due to incompatibility with an Assistive Technology, if you need the information in an alternative format, or if you have suggestions on how we can make this site more accessible, please contact us at: (212) 576-5811",New York Life Insurance Co,"New York, NY",$105k–178k,"Senior Associate, Data Scientist"
164.1,"Loeb Enterprises invests in promising early stage startups and helps them achieve their potential by providing in-house shared services. Loeb uses a shared services model consisting of a wide variety of teams, including marketing, product, accounting, administrative, data analysis, software engineering, etc. The team is available to assist and consult with all companies within the portfolio.

One of the shared services teams that has proven to be in high demand has been our Analytics Team. As it has grown in size it has been segmented into three main divisions: Business Intelligence, Data Science, and Data Engineering. As more companies have data-driven strategies and products, we are working to scale out all divisions, but most especially the data science team.

As a member of the Data Engineering team, you will have an opportunity to tackle the most difficult data challenges that Loeb Enterprises encounters. You will work with structured and unstructured data, from a wide variety of... different industries, involving many disparate tools. A good data engineer is able to discuss solution strategies for how to model and store data, as well as provide support in implementation. The day-to-day work for a data engineer spans activities that range from database administration to full-on development.

Success in the first 90 days
• Understand the datasets for the current portfolio companies that the Analytics team works with, and how these companies use their datasets. One should also understand how these datasets are constructed.
• Become familiar with and engage the project management processes and tools that are used (e.g. Trello, Jira).
• Analyze project roadmaps- request necessary resources and time adjustments based on expected work load to manage expectations.
• Understand and help solve some of the numerous short-term problems that the firm confronts with regards to ETL, data normalization or QA.
• Understand the high-level objectives of the Analytics team as they pertain to the portfolio companies that we serve.

Success in the first 180 days
• Write, test, and commit database or application code to either solve an existing problem or move the needle on one of the outstanding projects.
• Achieve mastery of the data model for 1-3 of the portfolio companies that we work with.
• Have a clear idea of what the business priorities for the portfolio companies are and how your projects relate to them.
• Suggest infrastructural changes where appropriate to facilitate future goals.
• Manage day-to-day operations for 1-2 of our smaller codebases.
• Provide input as to what, if anything, our team culture is lacking.
• Ability to follow instructions, but with the confidence needed to take a solution and “run with it.”
• Discipline – the ability to implement a solution and ensure that it solves the problem.
• Good time-management skills.
• Analytical thought processes- the ability to get to the heart of a problem and organize a structured approach to it.
• Good communication and listening skills. Presentation skills, understanding the audience.
• (preferable)- Degree in computer science, mathematics, science, or engineering.
• A minimum of 2-3 years of experience with at least one programming language and proficiency with at least one other. Should have interest in learning new languages and improving on current ones. Preferred languages include (but are not necessarily limited to): T-SQL, Java, C#, R, Python, and Visual Basic.
• Basic understanding of algorithms, from a practical and theoretical perspective.
• Familiarity with SQL and RDBMS concepts. NoSQL experience is good as well.
• Experience with cloud technologies preferable, with examples of projects run on AWS, Azure, or GCP.
• Experience with source control software, such as Tortoise, Git, GitHub, CVS, or others.
• Experience with ETL and/or architecture projects",Loeb.nyc,"New York, NY",$91.6k–145k,Data Engineer
,"MaestroQA helps teams across the world improve their customer experiences through streamlined quality assurance practices, customer support coaching, and data driven reporting.

Engineering at MaestroQA

With our customers at the center of our product decisions, we aim to provide them with intuitive interfaces and seamless integrations. Driven by insights gained from customer conversations, we work quickly and iterate to provide the best product possible while still focusing on well-designed architecture and readable code. We're looking for engineers who embrace these aspects and who are eager to help us build out our data infrastructure to help us scale and enable new products.

As a data engineer, you will have the opportunity to work with new and powerful technologies - the ones you and your small team identify as being the best for the job. One such project you will dive into includes re-designing and implementing the data pipelines core to our business, which affects everything... from who our customers can be to what features we offer and how. Another project offers the opportunity to dive into machine learning and NLP as we explore new and exciting opportunities with text analytics.

You will:

- Have the opportunity to work across the whole range of our application and infrastructure

- Iterate on and create major parts of our distributed systems, infrastructure, data processes, and core features

- Communicate directly with customers and internal teams to help define our roadmap

- Help or own launches onto new platforms

- Have input on tools and technical decisions

The best applicants will likely:

- Have experience building production software applications

- Have experience building distributed systems, ETL pipelines, or complex data systems

- Thrive in a dynamic environment with varied responsibilities

- Be excited about working directly with customers

- Have an eagerness to experiment, a determination to contribute, and a drive recognize problems and swiftly remedy them

Pay, Perks & Such:

At Maestro, we are in the family-phase. The biggest perk by far is the opportunity to learn by working on the ground floor and getting to actually build the roller coaster versus just ride it. We provide competitive salaries, stock options and benefits like health coverage, team events, and more",MaestroQA,"New York, NY",,Senior Data Engineer
164.1,"Big Data Engineer NYC, NY Right to Hire- 3 Openings

We are looking for a self-motivated Data Engineer to join our data platform team. The candidate will be responsible for expanding and optimizing our data platform and data pipeline architecture, as well as optimizing data flow and collection for cross functional teams. The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing and/or re-designing our data architecture to support next generation of products and data initiatives.

Responsibilities:
-- Create and... maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements.
-- Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, etc.
-- Build the infrastructure required for optimal extraction, transformation, and loading of data from traditional/legacy data sources.
-- Work with stakeholders including the Management team, Product owners, and Architecture teams to assist with data-related technical issues and support their data infrastructure needs.
-- Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.

Qualifications:
Experience building and optimizing 'big data' data pipelines, architectures and data sets.
Advanced hands-on SQL knowledge and experience working with relational databases for data querying and retrieval.
Experience with big data frameworks/tools: Hadoop, AWS, Kafka, Spark, etc.
Experience with relational SQL and NoSQL databases, including Oracle, Hive, HBase.
Experience performing root cause analysis on data and processes to answer specific business questions and identify opportunities for improvement.
Experience with data security.
Experience with building processes supporting data transformation, data structures, metadata, dependency and workload management.
Experience supporting and working with cross-functional teams in a dynamic environment.
Experience with Java and/or Python a plus",Enterprise Engineering,"New York, NY",$91.6k–145k,Big Data Engineer
168.0,"The members of our Data Engineering department build systems and infrastructure for collecting, storing, and analyzing huge sets of data in batch and streaming pipelines. As a sub-team within Data Engineering, the Business Data team is responsible for the quality, content, and timely delivery of our core analytics and business intelligence data sets. We build and maintain tools to help the rest of the company find, transform, store, and visualize their data. We value curiosity, enthusiasm, responsibility and generosity of spirit.

We're looking for a skilled engineer who cares about the data that flows through pipelines and the internal customers who will use that data. We work in Scala, Python, PHP, Java, and SQL, and we work with technologies like Hadoop, Kafka, Scalding, Airflow, Avro/Thrift, Vertica, Looker, GCS, Dataproc, and BigQuery. Experience with any of these is helpful but not required.

The technical staff at Etsy believes that code is craft, and that the work we do is... part of a larger creative culture that includes the hundreds of thousands of inspired artists and designers who make Etsy such a unique marketplace. We believe that small, empowered, self-motivated teams can do big things. We believe in measuring everything, taking advantage of our pioneering continuous deployment system to ship code early and often, and keeping up a blameless culture based on trust and a commitment to learning.

This is a full-time role located in Brooklyn, NY.

About The Role
• Our team is responsible for the daily delivery of hundreds of business-critical datasets in batch and streaming.
• We help define, maintain, and monitor the ETLs that generate our core business analytics datasets.
• We build and maintain internal tools for finding, querying, transforming, and visualizing data.
• We support the company's analytics database and work to establish best practices for users of it.

About You
• You understand that being an effective software engineer is about communicating with people as much as it is about writing code.
• You are willing to work with and improve code you did not originally write.
• You are generous with your time and experience, and can mentor other engineers.
• You are flexible with languages and tools and are willing to learn whatever is necessary to get the job done.
• You can take on unconstrained problems and know when to seek help.
• You have familiarity with a few of the following: Writing and scheduling ETL pipelines, either batch or streaming. Writing SQL queries for exploration and analysis. Building and modifying full-stack internal tool web apps

What’s Next

Interested in working with us? Send us a cover letter and your CV or resume explaining why you’d be great for the job. We value your unique talents and point of view, so feel free to tell us what you are all about. And if you write, draw, craft, or contribute to something you’re proud of, we’d love to hear about it.

At Etsy, we believe that a diverse, equitable and inclusive workplace makes us a more relevant, more competitive, and more resilient company. We welcome people from all backgrounds, ethnicities, cultures, and experiences. Etsy is an equal opportunity employer. We do not discriminate on the basis of race, color, ancestry, religion, national origin, sexual orientation, age, citizenship, marital or family status, disability, gender identity or expression, veteran status, or any other legally protected status",Etsy,"New York, NY",$109k–118k,Data Engineer
164.1,"Transfix is the leading freight marketplace that's transforming the $700 billion trucking industry, connecting shippers to a national network of reliable carriers. Fortune 500 companies such as Anheuser-Busch, Unilever, and Target rely on Transfix to handle their most important full truck load (FTL) freight needs. With instant pricing tools, guaranteed capacity, data-driven insights, and reliable service, Transfix is changing the world of transportation one load at a time. Come leave a positive impact on the environment as we help reduce the carbon footprint caused by the 65 billion wasted miles on the road.

Transfix was named one of Forbes' ""Next Billion-Dollar Startups"" in 2018.

Based in New York City, Transfix is backed by top VC firms including New Enterprise Associates (NEA), Canvas Ventures, and Lerer Hippeau. The company has raised more than $78.5 million in funding.

The problems we solve everyday are real and require creativity, grit, and determination. We are building a... culture that challenges norms while fostering experimentation and personal growth. We're hiring team members who are passionate and are energized by the vision to simplify and transform one of the largest and most complex industries through technology, data and a strong commitment to customers.

Our Data Engineers build mission-critical data pipelines and warehouses that power forecasting models (e.g. pricing, capacity, matching) and business insights. These products enable improvements in the lives of truckers and shippers across the country. If you are excited about reinventing an industry from the inside out, send us your resume.

What you'll do
• Design, build, and scale our data warehouse.
• Collaborate with Data Scientists, BI Analysts, Engineers, and Product Managers to gather requirements for specific datasets, reports, and model training.
• Research new data warehouse and data pipeline solutions. Identify requirements, build prototype implementations, propose adoption, and plan migration paths.
• Implement data pipelines that aggregate data from various sources and write to a variety of internal and external platforms. (Examples include S3, NetSuite, Salesforce, customer APIs.)
• Identify and resolve performance bottlenecks in SQL queries, workflows, and model training.
• Ensure availability of our production data stores and pipelines through building resilient systems, monitoring, alerting, team-wide on-call rotations, and triaging and resolving issues.
• Participate in blameless post-mortems and retrospectives that enable us to learn from both failures and successes in order to improve team process and software design.
• Our current data storage infrastructure is comprised of RedShift, PostgreSQL (Amazon RDS), DynamoDB, and Redis. We are hosted on AWS and leverage a variety of their services. We use Ruby and Python heavily throughout our production application and data infrastructure, along with bits of Java. We use Airflow to manage our ETL workflows and Amazon’s Database Migration Service for moving data around.
• Code review and continuous integration are important parts of all our production code, including data architecture. We use Github and CircleCI and write automated tests using RSpec and unittest.

About you
• Bachelor’s degree in a STEM or quantitative field, or have equivalent training or work experience.
• Minimum of 2-years professional software engineering experience (strong preference for experience in data engineering roles or data-intensive environments).
• Fluency in at least one programming language (Python, Ruby, C++, Java, or Go preferred).
• Proficiency with SQL, schema design, and query optimization.
• Experience with Linux command line and shell scripts.
• Working knowledge of data warehouses, data pipelines, and schedulers.
• Experience with cloud infrastructure on AWS, Google Cloud, or Azure.
• You take pride in your work and the quality of products you build.
• You value collaboration and enjoy teaching and learning from peers.
• You speak up when you see something wrong and support your opinions with data.

To all recruitment agencies: Transfix does not accept unsolicited agency resumes and will not be held responsible for any fees related to unsolicited resumes",Transfix,"New York, NY",$91.6k–145k,Data Engineer
164.1,"Responsibilities Integrate multi-technology data systems by building a unified back-end platform and a web-enabled front-end Follow best practices for software development and documentation, ensure designs meet requirements, and deliver high-quality results under time constraints Closely work with cross-functional team to enhance systems, trouble-shoot data issues. Involvement in all phases of software development from review of functional specification through assisting with test plans and final QA cycle. Actively participates in monitoring and troubleshooting of production platform related issues Requirements Bachelor s degree in Computer Engineering or equivalent is highly desired 7+ years of post-college working experience with full life cycle of ETL development in Data Warehouse and Data Integration projects using Informatica or any other similar ETL tool and Big Data 5+ years Linux/Unix/Perl experience, including scripting and version control with working knowledge of Oracle... database 2+ years of Hadoop using Map Reduce or Hive or Pig or any other Bid Data eco-system tool. 4+ years in Shell scripting, Unix, system/process automation is required Expertise in data research/analysis with a focus on data quality and consistency is required Keywords : Big data, Analytic, Hadoop, Data Engineer, ETL, Cloud, Map Reduce, Oracle, Unix Please refer to job #23402 and send MS Word attached resume to Denis Goncharov-Carey, ...@analyticrecruiting.com | For more opportunities, please visit www.analyticrecruiting.com
Global Ecommerce firm seeks Data Engineer to play a leading role in development, integration and support of technology across multiple platforms (Unix, Cloud, Hadoop and Oracle",Analytic Recruiting,"New York, NY",$91.6k–145k,Data Engineer
,"Azure Lead Data Engineer

New York

Ready to lead a team and charge into battle head first in the big apple?! you will be leading the charge for a company that isn't afraid to embrace the future! This is a place that likes to be in the front lines of tech. It will be hard to find a better place to show off your Technical skills and sharpen your trade with some of the best people in the industry! As well as developing a team that is just as eager as you! This client wants to know If you can develop, Construct, Test and maintain architectures, build Big Data Warehouses/Cubes from scratch, Establishing and implement data pipelines, Work with Azure Data Factory, Analysis and Visualization, Develop ETL, and hands on coding and developing in Spark. If you are up to the challenge then this is the place for you.

Experiences In:
• Azure (Data Factory, Data Warehouse)
• Spark
• Data Analytics
• Microsoft Business Intelligence
• Power BI and Blob Storage
• Azure SQL, Blob/ Lake
• implementing... and optimizing data pipelines
• SSIS, SSAS, SSRS
• Willingness to Travel
• Data Modeling
• Production experience a plus

What they can give you:
• competitive salary
• PTO
• gym memberships
• full health insurance packages
• 401K+ match
• Remote Possible
• flexible hours
• Opportunity for advancement and career growth
• Beautiful Location
• Monthly Networking Events
• Commuter benefits
• Rewarding company culture
• Bonus incentives
• Ability to grow your skill set

Please be sure to reach out to me as soon as possible so you don't miss out on this amazing opportunity! This role is not set to last! For more information, or to apply, call Michael at 646 863 7444, if you prefer email- email an up to date CV to m.morgan@nigelfrank.com.

Nigel Frank International is the global leader for Microsoft recruitment, advertising more Azure roles than any other agency. We deal with both Microsoft Partners & End Users throughout North America. By specialising solely in placing candidates in the Azure market I have built relationships with most of the key employers in The Greater New York area and have a complete understanding of where the best Azure opportunities are",Nigel Frank,"New York, NY",,Azure Lead Data Engineer
77.69999999999999,"Business Unit

Innovations

Business Unit Overview

Blackstone Innovations (BXi) is the technology team at the core of each of Blackstone’s businesses and new growth initiatives. Serving both internal and external clients, we work to build the next generation of systems that manage risk, create efficiency and improve transparency within the firm and across our broad community of investors and portfolio companies.

BXi is nimble and entrepreneurial – our open, iterative design processes and rapid pace of development mean that everyone on the team has the opportunity to make an impact from day one. We are problem solvers who can take projects from idea to implementation. We believe in active mentoring and developing excellence. We collaborate to find the best answers for our customers and for Blackstone. We are critical to the firm maintaining its competitive edge.

Job Title

Data Engineer

Job Description

Blackstone is looking to hire a passionate data engineer to be an integral... member of the Data Science and Engineering Team within Innovations, the firm's technology team. We are a new and growing team using data science and data engineering to provide a competitive advantage to our investment professionals and management teams, allowing Blackstone to be a more efficient investor and owner of assets.

As a data engineer, you'll design, implement, and extend core systems that enable data science and data visualization at one of the world's leading investment firms. These systems include data lake, data warehouse, and data pipelines, as well as platform tools that help data scientists and data analysts throughout the firm and Blackstone's portfolio companies. The team also owns Blackstone's broad and growing self-service data visualization stack, including the design, development, and curation of golden-copy analytics data sources.

Our data infrastructure is nascent, growing, and constantly improving. We primarily use Python and SQL, AWS cloud services, Snowflake data warehouse, and Airflow for workflow orchestration. We visualize data with Tableau. Our development environment leverages Vagrant and Docker. We practice infrastructure-as-code with Terraform, and we build continuous integration and deployment pipelines with Gitlab and TeamCity.

Key Responsibilities

 Full-stack design, development, and operation of core data stack including data lake, data warehouse, and data pipelines
 Build data flows for data acquisition, aggregation, and modeling, using both batch and streaming paradigms
 Consolidate/join datasets to create easily consumable, consistent, holistic information
 Design and implement machine learning models and prediction APIs, and ensure their operational performance over time
 Empower data scientists and data analysts to be as self-sufficient as possible by building core systems and developing reusable library code
 Support and optimize desktop and cloud environments for data scientists and data analysts
 Ensure efficiency, quality, resiliency of data science core systems
 Work with senior technical staff throughout Blackstone's portfolio companies to develop data flows

Qualifications

 Undergraduate or graduate degree in a technical or scientific field, such as Computer Science, Engineering, Mathematics, or similar
 2-5 years professional experience as a data engineer, software engineer, data analyst, data scientist, or related role
 Analytically-minded and detail-oriented: you actually like staring at data, looking for patterns and outliers, establishing data models, and rigorously answering questions
 Expertise in data engineering languages such as Python, Java, Scala, SQL.
 Data modeling and data governance experience; you've designed and implemented a data mart, a data warehouse, or the back-end database of an application
 Experience building ETL and data pipelines, especially via code-oriented systems like Spark, Airflow, Luigi, or similar, and with varied data formats
 Cloud-oriented but comfortable with on-premises infrastructure
 Experience operating in a secure networking environment (e.g. behind a corporate proxy) is a plus
 Creative problem-solving skills, especially in situations where ""nobody has tried this before""
 Excellent technical documentation and writing skills: you know Markdown syntax cold, and have published API documentation or similar
 You're not satisfied with ""close enough"" solutions, and you design long-term solutions that are robust over time
 You have a bias towards automation: ""one-time scripts"" eat away at your soul a little bit each time you write one
 Proficiency in statistics and machine learning is a nice-to-have, and interest in learning these is even better!
 Familiarity with visualizing data with Tableau and similar tools
 Great customer service and technical troubleshooting skills

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, disability, sexual orientation, national origin or any other category protected by law.

If you need a reasonable accommodation to complete your application, please contact Human Resources at 212-583-5000 (US), +44 (0)20 7451 4000 (EMEA) or +852 3656 8600 (APAC).

The Blackstone Group and its affiliates provide equal employment opportunity to all qualified employees and applicants for employment regardless of race, color, creed, religion, sex, pregnancy, national origin, ancestry, citizenship status, age, marital or partnership status, sexual orientation, gender identity or expression, disability, genetic predisposition, veteran or military status, status as a victim of domestic violence, a sex offense or stalking, or any other classification prohibited by applicable law.

To submit your application please complete the form below. Fields marked with a red asterisk * are required in order to enter into a possible employment contract (although some can be answered ""prefer not to say""). Failure to provide this information may compromise the follow-up of your application. When you have finished click Submit at the bottom of this form",The Blackstone Group,"New York, NY",$50.3k–54.8k,Innovations - Data Engineer
202.0,"Knowledge is our product, and data is our platform. We need engineers who look at a data set and want to unlock the answers it holds inside. Engineers who look at a data set and think about how to make sure it is correct. Engineers who look at a data set and want to make infrastructure to help build it better, faster, and stronger.
As a Data Engineer, you will work closely with oncologists and statisticians to build software that will help our customers discover novel insights into their data. You will design our data infrastructure, and use it to develop extensible, robust data and analytics pipelines, tools, visualizations, and services for accessible and flexible data analysis. You will learn more than you ever thought possible about how cancer is treated in the real world, and your work will directly support oncology research and publications.
Who you are:
You hold a BS, MS, or Ph.D. in computer science or related field
You have 2+ years work experience
You have experience with... languages like Python, C++, Java, or C#
You are passionate about performance, reliability, and scalability of systems
You are inspired by our mission to improve cancer research through technology
You seek simple approaches to complex problems
You like science and/ or medicine just because it’s cool
Bonus points if you have any of the following:
Have a good understanding of relational databases like PostgreSQL, MySQL or MSSQL
Have real passion for data and a strong understanding of statistics
Have developed distributed data processing systems against large, heterogeneous data sets
Have taken a leading role in delivering complex software systems all the way to production
You almost decided to go to med school
Flatiron is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, or Veteran status. If you have a disability or special need that requires accommodation, please let us know",Flatiron Health,"New York, NY",$114k–176k,Senior Data Engineer
,"Data Engineer, Social Integrations
Who we are
DoubleVerify is the leader in digital performance solutions, improving the impression quality and audience impact of digital advertising. Built on best practices, DoubleVerify solutions create value for media buyers and sellers by bringing transparency and accountability to the market, ensuring ad viewability, brand safety, fraud protection, accurate impression delivery and audience quality across campaigns to drive performance.
Since 2008, DoubleVerify has helped hundreds of Fortune 500 companies gain the most value out of their media spend by delivering best in class solutions across the digital ecosystem that help build a better industry.
Overview
As Data Engineer of Social Integrations Engineering Team, you will work with high volume datasets of hundreds of data points and research many aspects of this social integrations data for monitoring, anomaly detection, and various business use cases, helping our clients to make smarter... decisions that continuously improve their ad-impression quality.
What you’ll do
Design, develop, and test data-driven products and features
Explore new ways of producing, processing, and analyzing data in order to gain insights into our product features
Work with state-of-the-art data processing frameworks, technologies, and platforms
Analyze Data and Build large-scale batch and real-time data pipelines with data processing frameworks like Spark, Kafka, Kubernetes and the Google Cloud Platform.
Help drive optimization and tools to improve data quality.
Collaborate with other engineers, data analysts, and decision-makers, such as product owners, to build solutions and gain novel insights.
Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.
Act as the bridge between our backend and product teams and work on data management and build/maintain crucial data pipelines
Who you are
You have BS / MS in Computer Science/Engineering or relevant field
1-2 years of experience in data modeling, data access, and data storage techniques.
Proven experience in building Big Data pipelines using Spark, Kafka
Interested in being the glue between engineering and product
Don’t like leaving questions unanswered and you love exploring/understanding data
Have a passion for data and for transforming numbers into key business insights
Excellent SQL Skills preferably in Hive and/or Spark SQL
Love visualizing your data findings in a clear and easy to understand way and to capture corner cases of implementations
Care about agile software processes, data-driven development, and responsible experimentation
Passionate about crafting clean code and have a steady foundation in coding and building data pipelines",DoubleVerify,"New York, NY",,"Data Engineer, Social Integrations"
,"Spotify is looking for a Data Engineer to join our Creator Marketplace engineering organization. You will forge new ground in how we help labels and their artists and managers and help us fulfill our mission to understand the music industry through data. If you're interested in building out the next revolution in the music industry and helping artists succeed in their careers, come join us!

You will build data-driven solutions to bring music and digital media experiences to our 100 million active users and millions of artists either by working directly on product features, publishing and insight tools for artists, or by improving the quality of our data tools and large scale data infrastructure. You will take on complex data-related problems using some of the most diverse datasets available -- user behaviors, acoustical analysis, revenue streams, cultural and contextual data, and other signals across our broad range of mobile and connected platforms. Above all, your work will impact... the way the world experiences music.

What you'll do
• Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.
• Use best practices in continuous integration and delivery.
• Help drive optimization, testing and tooling to improve data quality.
• Collaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.
• Work in multi-functional agile teams to continuously experiment, iterate and deliver on new product objectives.

Who you are
• You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.
• You know how to write distributed, high-volume services in Java or Scala.
• You are knowledgeable about data modeling, data access, and data storage techniques.
• You appreciate agile software processes, data-driven development, reliability, and responsible experimentation.
• You understand the value of partnership within teams.

We are proud to foster a workplace free from discrimination. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our users and our creators. This is something we value deeply and we encourage everyone to come be a part of changing the way the world listens to music",Spotify,"New York, NY",,"Data Engineer, Creator Marketplace"
87.8,"Columbia University’s Mortimer B. Zuckerman Mind Brain Behavior Institute (Zuckerman Institute) supports interdisciplinary neuroscience research and discovery by scientists and scholars across the university and promises to be the most comprehensive institute for brain science. Located in a state of the art facility, approximately 50 independent world class labs are brought together to transform our understanding of the brain and mind. The Zuckerman Institute is seeking a highly motivated individual with an interest in neuroscience to fill the position of a Data Engineer. The Data Engineer will work directly with Zuckerman Institute faculty and researchers to facilitate and pilot new methods of data sharing and analysis, with an emphasis on reproducible research. Working closely with the Zuckerman Institute Research Computing group, the data engineer will facilitate data analysis and sharing across ten (10) dynamic laboratories investigating the computational and circuit mechanisms... underlying motor control. Responsibilities include, but are not limited to the following: * Works in close collaboration with the Principal Investigator (PI) faculty and researchers. * Assists with standardization, documentation, and packaging of open source code across multiple laboratories, including releases for the broader community. * Works directly with researchers developing novel algorithms (by researching algorithms, helping to translate proof of concept algorithms into well engineered software with proper APIs, and helping researchers unit test their code to ensure proper functionality) and subsequently supports the use of software by end users. * Establishes pipelines for data sharing (e.g., of electrophysiological data from multielectrode arrays or 4D largescale calcium imaging microscopy data) across laboratories. * Organizes training courses for scientists who wish to extend their knowledge of programming and best practices in software engineering. * Performs related duties and responsibilities as assigned / requested.Qualification:A bachelor’s degree required. A minimum of four (4) years of related experience is required. Strong software engineering skills, with proficiency in Matlab, Python, and Java as well as standard tools (e.g., git), and some familiarity with machine learning. The ability to confer and consult directly with senior faculty research staff, and students on technical topics? Excellent organizational and time management skills to support research faculty and staff? sound judgment with a collaborative style that fosters teamwork. Familiarity with at least one workflow/datasharing platform (e.g. osf.io) and expertise with low-level data representation? an interest in teaching. Applicants must submit a cover letter and CV/resume to be considered Significant research and/or industry experience, with demonstrated skills in algorithm development is preferred",Columbia University in the City of New York (CU),"New York, NY",$52.8k–70k,Data Engineer
164.1,"Spruce is looking for a Data Engineer to join the Data Science team in NYC. The Data Engineer will develop new digital capability for new title underwriting automation and data science tools in the real estate industry. You will have initiative, innovation and good judgment in a dynamic startup environment.
Responsibilities:
Build efficient codes to extract data and documents from various sources
Build OCR and NLP pipeline to retrieve data from unstructured data
Build standard reports from extracted data per business requirement
Conduct unit testing and document the finalized code set
Assist data scientists in transforming data into modeling datasets and provide ongoing data support
Assist production implementation in set up infrastructure and automated processes
You will possess strong social and interpersonal skills and be able to seamlessly collaborate with other members of the team and across functions. You will demonstrate attention to detail, organization, and work, including... driving projects to completion in accordance with established timelines.
This position will work with the Director, Data Science and will locate in midtown Manhattan.
Qualifications
Bachelor’s degree in quantitative fields is required, computer science or data science degrees are preferred
Have two years of Java, Python, and database programming experience. The Python experience should include web scraper development
Have one year of natural language processing work or research experience
Unix and OCR experience are a plus
Knowledge of insurance industry and statistics is a plus
Excellent written and verbal presentation skills
Highly motivated and creative problem solver; able to self-initiate and thrive in a results-oriented environment; positive attitude
Ability to convey sense of urgency and to work productively and cooperatively with team members",Spruce,"New York, NY",$91.6k–145k,Data Engineer
164.1,"Lawrence Harvey are currently partnered with one of the leading Real Estate firms in the world. With offices globally, they are looking to build out the Data Engineering division here in New York.

As the industry leader they are constantly on the forefront of integrating technological advances and industry insights to create the winning formula.

As a Data Engineer in this team, you will be tasked with the re-engineering of the entire data environment, building out a data processing and analytics platform, whilst collaborating with Data Scientists, Stakeholders and Clients.

Responsibilities will include:
• Build Analytics tools that will be used by a variety of internal teams including Leadership and Sales.
• Design an infrastructure that can be scaled to suit any situation.
• Build, Deploy and Maintain a Data Pipeline

Qualifications:
• An extensive background within Data Engineering.
• Experience with the “Big Data Stack” (Hadoop, Kafka, Spark Etc)
• A background in Databases (SQL... and NoSQL)
• Experience with a variety of Amazon Web Services (AWS EC2, AWS Redshift, AWS EMR)
• Proven abilities to work with Pipeline and workflow management tools (Airflow or Luigi is preferred).
• Python skills

If this is something you are interested in, please get in touch and apply.

Email: C.walker@lawrenceharvey.com",Lawrence Harvey,"New York, NY",$91.6k–145k,Data Engineer
,"About this job

Location options: Remote

Technologies

looker, database, sql, data-visualization, business-intelligence

Job description

REQUIREMENTS:
• Experience using LookML (Looker)
• Hands-on coding experience and expertise in back end related technologies, like Node or Python
• B.S. in Computer Science or equivalent experience followed by 5+ years work experience in using SQL and databases in a business environment
• Expertise in Data Visualization
• Deep experience in the latest libraries and programming techniques
• Familiar with SQL/NoSQL databases like MongoDB and their declarative query languages
• Knowledge in using BI Analytics and related technologies

WHO YOU ARE:
• You have accomplishments that showcase your capabilities by their success and technical depth.
• You own new features from idea to completion.
• Work well with a core team to design and execute major new features.
• Enjoy contributing to a fast moving exciting project
• Strong communicator and fluent in... English with excellent written and verbal communication skills.
• Thrive and excel in our diverse, distributed and agile team environment

Who We Are

Clevertech is a leading consultancy that is on a mission to build transformational digital solutions for the world’s most innovative organizations. Enterprise companies turn to Clevertech to help them launch innovative digital products that interact with hundreds of millions of customers, transactions and data points. By partnering with Clevertech these companies are propelling forward and changing their industries, business models and more.

Based in New York City with fully remote development teams, Clevertech has built core product offerings for clients whose value was revealed in transactions valued in excess of $100 million.

The problems we solve everyday are real and require creativity, grit and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. We are hiring team members who are passionate and energized by the vision of empowering our customers in a complex industry through technology, data and a deep understanding of client concerns. In order to grasp the scale of problems we face, ideally you have some exposure to Logistics, FinTech, Transportation, Insurance, Media or other complex multifactor industries.

Our Benefits
We know that people do their best work when they’re taken care of. So we make sure to offer great benefits that promote personal and professional growth!
• Competitive Vacation Package
• Annual Financial Allowance for YOUR development
• Flexible Family Leave
• Clevertech Gives Back Program
• Clevertech U (Leadership Program, Habit Building, New Skills Training)
• Clevertech Swag
• Strong Clevertech Community

How We Work
Why do people join Clevertech? To make an impact. To grow themselves. To be surrounded by developers who they can learn from. We are truly excited to be creating waves in an industry under transformation.

True innovation comes from an exchange of knowledge across all of our teams. To put people on the path for success, we nurture a culture built on trust, collaboration, and personal growth. You will work in small feature-based cross-functional teams and be empowered to take ownership.

We make a point of constantly evolving our experience and skills. We value diverse perspectives and fostering personal growth by challenging everyone to push beyond our comfort level and try something new.

The result? We produce meaningful work

Getting Hired
We hire people from a variety of backgrounds who are respectful, collaborative, and introspective. Members of the tech team, for example, come from diverse backgrounds having worked as copy editors, graphic designers, and photographers prior to joining Clevertech.

Our hiring process focuses not only on your skills but also on your professional and personal ambitions. We want to get to know you. We put a lot of thought into the interview process in order to get a holistic understanding of you while being mindful of your time. You will solve problems derived from the work we do on a daily basis followed by thoughtful discussions around potential fit. Whatever the outcome, we want you to have a great candidate experience.

Want to learn more about Clevertech and the team? Check out clevertech.careers.

Clevertech Culture Video",Clevertech,"New York, NY (+14 others)",,Back End Data Engineer
,"EXCITING OPPORTUNITY!!

Big Data Engineers are senior employees who solve big data problems by creating and engineering ETL's and data warehouses to create the best big data solutions.

You must be able to handle large scale databases, and be able to analyze the patterns of data to make accurate business decisions.

Skills

- Strong decision making in terms of data analysis, and the ability to architect big data.

- Machine learning is very important, it will provide the architect with an advantage to handle the large analysis needs.

- Knowledge and experience with Hadoop, EMR, S3, EC2, Redshift and Data Lakes

- Experience with Migrating from Teradata to Redshift on AWS Redshift scalability, performance and tuning

This job will offer the chosen candidate the opportunity to increase their level of experience working with AWS, with a specific focus on Data Pipelines. The ideal candidate will be able to translate this experience to other Big Data engineering challenges throughout... their career!

Our client is looking for a AWS Big Data Engineer for an exciting contract opportunity

Must have experience with S3, EC2, Redshift, and EMR

Languages- Python, Java

Proven experience with Data Pipelines

Opportunity type- Contract

If you or someone you know is interested in this position, please send your resume directly to P.Sandstrom@jeffersonfrank.com or call 212-731-8282. Ask for Paige! My client is looking to start the interview process as soon as possible.

Jefferson Frank is the Amazon Web Services (AWS) recruiter of choice. We work with organizations worldwide to find and deliver the best AWS professionals on the planet. Backed by private equity firm TPG Growth, we have a proven track record servicing the AWS permanent and contract recruitment market and, to date, have worked with over 30,000 organizations globally from our offices in North America, Europe, and Asia-Pacific.

At Jefferson Frank, our mission is simple: we want happy customers. Whether you're an AWS professional walking into your dream AWS job, or an organization hiring an incredible contractor for your cloud migration project, our goal is to deliver an unrivalled customer experience. Work with us and you'll get the personalized experience you deserve - one you'll simply not find at any other recruitment agency. At Jefferson Frank, we find great people great jobs in AWS.

I understand the need for discretion and would welcome the opportunity to speak to any Big Data and cloud analytics candidates that are considering a new career or job either now or in the future. Confidentiality is of the upmost importance. For more information on available AWS Big Data Jobs as well as the cloud market, I can be contacted at 212-731-8282. Please see www.jeffersonfrank.com for more information",Jefferson Frank,"New York, NY",,AWS BIG DATA ENGINEER - CONTRACT - NYC - EXCITING OPPORTUNITY
107.8,"Company Overview
Graphika maps and analyzes the complex fabric of social network structures, or what we call “cyber-social terrain.” We create large-scale maps of social media landscapes and provide in-depth analysis to help clients and partners understand and navigate complex online networks.
Our team of scientists, engineers, analysts, and technologists use Graphika’s unique mapping technology to deliver insights in a variety of fields, including digital marketing, influence analysis, disinformation detection, and electoral integrity.
Job Summary
Graphika seeks a collaborative, well-rounded Data Engineer to join our work developing and improving our unique mapping and clustering platform.
Responsibilities and Duties
Implement modules and contribute meaningful improvements to existing codebase of our network analysis and clustering platform
Partner with research scientists, data analysts and business teams including Product to build useful features for our end users
Work with the... tech team on system design and infrastructure
Contribute to improving the quality of our data and data infrastructure
Qualifications and Skills
You are a well-rounded engineer with 3+ years of professional programming experience (Python preferred)
You have experience doing quantitative analysis or statistical modeling
You have experience with the Python data science stack (numpy, pandas, matplotlib, sklearn, Jupyter, etc.)
You have knowledge of SQL and common relational database systems such as PostgreSQL and MySQL
You are excellent at debugging and optimizing code
You have a deep understanding of good software engineering practices, including version control, code reviews, testing, and refactoring.
Preferred Qualifications
You have experience with AWS services: S3, Lambda, Kinesis, SQS, etc
You have experience working in a data-focused organization
You have experience with social media analysis
You are familiar with natural language processing techniques
Benefits and Perks
Graphika offers a highly competitive, comprehensive benefits package for all full time employees, including but not limited to:
Outstanding health benefits for employees and their families including medical, dental, and vision
Flexible Spending Account (FSA) for medical and dependent care and pre-tax commuter benefit
Unlimited PTO with a required minimum as well as a flexible remote work policy
Generous company equity options",Graphika,"New York, NY",$33.8k–148k,Data Engineer (Mapping)
123.3,"The Role:

S&P Global Ratings is looking for an experienced Big Data Engineer to join Data Engineering team within Chief Data Office, a team of data and technology professionals who define and execute the strategic data roadmap for S&P Global Ratings. The successful candidate will participate in the design and build of S&P Ratings cloud based analytics platform to help develop and deploy advanced analytics/machine learning solutions.

The Team

You will be an expert contributor and part of the Rating Organization’s Data Services Team. This team, who has a broad and expert knowledge on Ratings organization’s critical data domains, technology stacks and architectural patterns, fosters knowledge sharing and collaboration that results in a unified strategy. All Data Services team members provide leadership, innovation, timely delivery, and the ability to articulate business value. Be a part of a unique opportunity to build and evolve S&P Ratings next gen analytics platform.

Our Hiring... Manager Says

If you are an individual that brings demonstrated experience of delivering big data projects as a data engineer, this is an excellent opportunity. I am looking for someone with sound technical knowledge, can be hands-on, worked on transformational initiatives, and can drive results.

Responsibilities:
• Design and develop efficient and scalable data pipelines between enterprise systems and analytics platform
• Work closely with Data Science team and participate in development of feature engineering pipelines
• Provide technical expertise in the areas of design and implementation of Ratings Integrated Data Facility with modern AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto and Spark
• Build and maintain a data environment for speed, accuracy, consistency and ‘up’ time
• Support analytics by building a world-class data lake environment that empowers analysts to determine insights into revenue and power products across the organization
• Work with the machine learning engineering team to build a data eco system that supports AI products at scale
• Ensure data governance principles adopted, data quality checks and data lineage implemented in each hop of the data
• Partner with the chief data office, enterprise architecture organization to ensure best use of standards for the key data domains and use cases
• Be in tune with emerging trends Big data and cloud technologies and participate in evaluation of new technologies
• Ensure compliance through the adoption of enterprise standards and promotion of best practice / guiding principles aligned with organization standards

Experience & Qualifications:
• BS or MS degree in Computer Science or Information Technology
• 5+ years of experience as data engineer at an innovative organization
• 3+ years of hands-on experience in implementing data lake systems using AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto and Spark
• Expert managing AWS services (EC2, S3, Route 53, ELB, VPC, cloudwatch, Lambda) in a multi account production environment
• Experience With Machine Learning Libraries and Frameworks (TensorFlow , MLlib) is an added advantage
• Exposure to R , SparklyR , and Other R packages is a Plus
• Experience with development frameworks as well as data and integration technologies such as Informatica, Python, Scala
• Expert knowledge of Agile approaches to software development and able to put key Agile principles into practice to deliver solutions incrementally.
• Monitors industry trends and directions; develops and presents substantive technical recommendations to senior management
• Excellent analytical thinking, interpersonal, oral and written communication skills with strong ability to influence both IT and business partners
• Ability to prioritize and manage work to critical project timelines in a fast-paced environment
• Financial services industry experience",S&P Global Ratings,"New York, NY",$60.8k–125k,Big Data Engineer (S&P Global Ratings)
,"-

A career at New York Life offers many opportunities. To be part of a growing and successful business. To reach your full potential, whatever your specialty. Above all, to make a difference in the world by helping people achieve financial security. It-s a career journey you can be proud of, and you-ll find plenty of support along the way. Our development programs range from skill-building to management training, and we value our diverse and inclusive workplace where all voices can be heard. Recognized as one of Fortune-s World-s Most Admired Companies, New York Life is committed to improving local communities through a culture of employee giving and service, supported by our Foundation . It all adds up to a rewarding career at a company where doing right by our customers is part of who we are, as a mutual company without outside shareholders. We invite you to bring your talents to New York Life, so we can continue to help families and businesses -Be Good At Life.- To learn more... please visit LinkedIn , our Newsroom and the Careers page of www.NewYorkLife.com .

-

The- Center for Data Science and Analytics -is the innovative corporate Analytics group within New York Life. We are a rapidly growing entrepreneurial department, which aims to design, create and offer innovative data-driven solutions for many parts of the enterprise. We are aided by New York Life-s existing business with a large market share in individual life insurance. We have the freedom to explore external data sources and new statistical techniques, and are excited about delivering a whole new generation of Analytical solutions.

In fact, we are designing and will build one of the first multivariate model-based continuous risk differentiations in the industry. This model will incorporate current underwriting best practices (including medical rules) as features and add other data sources, patterns/ideas and variables to essentially create a rating plan to support the next generation underwriting process at New York Life. This is just one of several projects with large business value. Geographic analytics on agents and customers, application fraud detection, agent success prediction and client prospecting analytics (off-line and on-line) are other exciting examples of enormous incremental value from analytics. Our products will be implemented into real-time core business processes and decisions that drive the company (e.g. underwriting, pricing, agent recruiting, prospecting, new product development).

We work with data ranging from demographics, credit and geo data to detailed medical data (medical test results, diagnosis, prescriptions) and social media information . We have a modern computing environment with a solid suite of data science/modeling tools and packages, and a large (but manageable) group of well-trained professionals at various levels to support you. Life insurance is on the verge of huge change. This is a chance to be part of, actually to drive, the transformation of an industry.

You will -be part of- Data & Platform- sub-function team under- Center for Data Science and Analytics . The- Data & Platform -team services internally to Data Scientists who focus on Statistical analysis.

You will -be part of a fast paced, high-impact team who will work with an entrepreneurial mindset using some of the best of breed tools as part of our Enterprise Data Lake (Hadoop) using R, Spark and Python.

You will -apply your data engineering skills to design, develop and enhance data strategy across and within the data science domain. -This role provides strategic support to internal teams and leads the design, build and implementation of model ready data pipelines. -Experience in a fast paced data engineering role with multiple and complex data sets.

Responsibilities

-
• Provide strategy and guidance for the architecture and build of data ingestion jobs to process disparate and unique data sources to form a high integrity model ready data set.
• Action oriented and comfortable working with complex data.
• Thinks strategically and works to define overall direction for solving complex business issues.
• Monitor and track data quality metrics
• Proactively address and resolve technical issues to support key business initiatives.
• Utilize emerging technologies to address business issues through technical innovation.
• Functions as data expert, contributes to analytics/solutions design and productizing decisions
• Can work independently with some supervision and be part of a collaborative team
• Work with Project Managers and Scrum Masters to provide milestones and stories
• Proactively and effectively communicates in various verbal and written formats with senior level member of the team and partners

-

Required qualifications
• Graduate-level degree in computer science, engineering, or relevant experience in the field of Business Intelligence, Data Mining, Database Engineering, Programming
• 8-10 years of overall experience working in the field of data wrangling and programming with a minimum of 2 years experience with ingesting, cleaning, merging and applying necessary data wrangling logic in Hadoop
• Excellent command of SQL - best practices, optimization, troubleshooting and debugging
• Strong Knowledge of Enterprise Platforms, Cloud Technology and high performance computing.
• Proficiency using Python for all data related work such as Numpy, Pandas, PySpark
• Experience working with Linux Operating System
• Experience working with data visualization tools or packages
• Experience building Exploratory Data Analysis reports such as Histograms, Box plots, Pareto, Scatter Plot using R, Python or a Data Visualization tool such as Tableau, Spotfire

-

Preferred: -
• Understanding of statistical modeling concepts, designs and analytics-based products
• Any experience in using ETL tools such as Ab Initio, Talend, Informatica, Pentaho
• Any experience working with Data Warehouses and/or Data Marts
• Any experience in Life Insurance business

-

LI-TK1

EF-TK1

EOE M/F/D/V

-

If you have difficulty using or interacting with any portions of this Web site due to incompatibility with an Assistive Technology, if you need the information in an alternative format, or if you have suggestions on how we can make this site more accessible, please contact us at: (212) 576-5811.

Date: Tue, 16 04 2019 00:00:00 GMT
Department: Data Analytics",New York Life Insurance Company,"New York, NY",,Data Engineer CVP- Center of Data Science Team
164.1,"About the right team member:

As a Data Engineer at Betterment, you will tackle a diverse set of data-oriented problems, working with a wide range of teams and learning a great deal about finance in the process. One morning you might work with our Finance team to construct data domains in Redshift that will allow them to hone and quickly tweak our financial model, and then spend the afternoon strategizing with our front-end application team on how to coordinate internal APIs that allow us to serve up historical data to customers in our applications. The next day you might assist our Operations team in automating the production of equities reconciliation reports, then close out the evening by hopping over to our Data Science team to get them the data they need to compare the effectiveness of TV advertisements in San Francisco and New York. If there’s data involved, you’re the one curating it - making it accessible, and ensuring it’s correct. A Data Engineer at Betterment can expect to... approach tasks such as these on a daily basis, leveraging our existing processes and using their prior experience to improve the way we handle data at Betterment.

This role can be based out of our headquarters in NYC or our brand new Philadelphia office.

At Betterment you will get to:
• Work on increasing the efficiency of our ETL processes as the size of Betterment’s data grows 10x annually
• Explore new technologies that will allow us to keep our internal API response times low even as throughput grows
• Move quickly to provide analysts with new data before they ask for it
• Investigate how we can enhance our logging and monitoring to discover and resolve issues before they cause problems
• Think about scale and new technologies that will enable us to achieve a high level of service as Betterment is managing hundreds of billions of dollars

You will be effective if you have:
• Have deep expertise in at least one object-oriented language, such as Java, Ruby, or Python
• Know how to handle an explosion of data without missing a beat
• Can optimize a query with the best of them
• Are the person at your current job that everyone goes to for database help, even though you aren’t necessarily a DBA
• Are so good at automating things that you’re constantly programming yourself out of a job
• Have a passion for software engineering, and for creating what doesn’t exist
• Know how to make the tradeoffs required to ship without compromising quality
• Appreciate agility and pragmatism in software development
• Thrive in a startup environment
• Have the grit to see projects through to their conclusion

Tools in your belt:
• Development: OO languages such as Python, frameworks such as Flask or Ruby on Rails, Advanced SQL
• Datastores: Redshift or other columnar stores, Postgres, MySQL, DynamoDB or other NoSQL stores
• Technologies: Event Streaming, Caching tools, MapReduce
• Platforms: AWS!

Betterment’s Data Engineering team spends most of its time with the tools above, but we cast a much wider net in other parts of the engineering team. We strive to always choose the best tool for the job. We maintain most of our ETL and orchestration in Python, but we serve up data to customers through APIs in a lightweight caching application built in Rails. The person for whom we’re looking will be a pro who can guide both our data pipeline development as well as our customer-facing APIs.

About Betterment

Betterment is the largest independent online financial advisor with more than $16 billion in assets under management. The service is designed to help increase customers’ long-term returns and lower taxes for retirement planning, building wealth, and other financial goals. Betterment takes advanced investment strategies and uses technology to deliver them to more than 400,000 customers across its three business lines: direct-to-consumer, Betterment for Advisors, and Betterment for Business. Learn more (http://www.betterment.com) .

Come join us",Betterment,"New York, NY",$91.6k–145k,Data Engineer
164.1,"We're seeking an experienced Data Engineer to join our growing data team. You will team up with our data engineer to build, scale and improve the core of our data stack. In addition, you will be working closely with our analytics team to productionalize statistical and machine learning models. We are looking for someone who’s excited to help us ask the right questions, dig deep into the answers, and build solid systems that address them.
A little about us
The Paperless Post Data team plays a crucial role in our product’s success. We get to dive into advanced data work—from machine learning to visualization, classifier creation to ETL pipelines—that solve complex business problems and increase the functional capabilities of everyone throughout the company. On the reporting side, we help ensure that all teams have the information they need to make sound business decisions. Above all, we’re a team that is excited about what we do.
What you’ll do here
Build efficient and reliable data... infrastructure and tools.
Develop production code for real-time data processing, ETLs and machine learning services.
Analyze the data we collect to generate important insights with our analytics team.
Deploy and monitor production systems.
About you
As an ideal candidate, you have a good blend of programming, analysis, and communication skills.
This describes you
You are an expert in SQL, Python, and/or JavaScript.
You have an expert knowledge with AWS (Data Pipeline, Lambda, Redshift) and best practices.
You have experience with modern ETL and data modeling technologies.
You have a solid understanding of both relational and NoSQL database technologies.
You have a strong understanding of data security and protection.
You have great communication skills, both written and verbal.
You have the desire and willingness to take on new challenges and learn new things.
This is a bonus
You have experience with Hadoop / MapReduce or Spark.
You have experience working on and productionizing machine learning models.
You have experience with real-time data processing.
You have experience with database architecture.
Company-wide we enjoy an amazing ecosystem of an even gender split and a healthy balance of engineers and designers. Because Paperless Post isn’t supported by ad revenue, we can focus our efforts on building and improving on the ideal version of our platform, product, content, and partnerships for our users.
We are proud that Paperless Post helped over 70 million people connect in the real world last year. Our product is global, and we are committed to being a company where everyone belongs. We encourage people of all backgrounds, races, genders, and abilities to apply",Paperless Post,"New York, NY",$91.6k–145k,Data Engineer
164.1,"Are you interested in products that make lives better for those most in need and help reshape healthcare in the US? We are looking for an energetic and highly motivated Data Engineer to help launch customer implementations and analyze our database of patient medical record data. You will be joining a fast-growing VC-backed company, with a unique team of seasoned professionals with a vast combined expertise in both the technical and the healthcare space.

Key Responsibilities:
• Launch new customer implementations of our population health and value based contracting products
• Analyze customer patient medical record data to mine a complete patient profile from Electronic Health Records
• Develop quality measure analytics that provide insights about patient populations
• Enhance our data lake and data pipelines as we continue to normalize new data sources and leverage our Big Data assets

Qualifications and Experience:
• Bachelor Degree in computer science preferred
• Traditional... relational DBMS as well as No-SQL
• Extract, Transformation and Load (ETL) tools and practices
• Data and object modeling
• General software engineering principles such as OOP, SOLID, etc.
• Quick and eager to learn new technologies
• Strong individual contributor who is able to thrive in a dynamic and fast paced environment
• Nice to have: Big Data technologies in the cloud such as Hadoop, Pig, Hive, etc.

Competitive salary and benefits provided. This position is located in New York City",MediQuire,"New York, NY",$91.6k–145k,Data Engineer
,"Minimum Required Skills:
Python, Postgres/Redshift, AWS, SQL, Hadoop, Algorithms, Machine Learning, Data Mining

Manhattan-based education-advocacy group needs hands-on Data Engineer for growth oriented team solving complex problems.

Python - Data - Lead Engineer

We are seeking a smart and engaged Data Engineer to help us build our platform solutions addressing problems in the ed-tech and advocacy space! We are developing a platform that supports data aggregation for our partner organizations to efficiently and effectively campaign for the increased quality and quantity of education in this country. Our current team is growing and needs to add a Data Engineer to help us build a data processing pipeline.

Ideally, this person will be passionate about data and engineering - Python, SQL, and AWS - and will have just a couple years of professional experience at this point in their career.

What You Need for this Position

- Python
- AWS
- SQL (Postgres / Redshift)

What You Will Be... Doing

- Developing our cloud based data warehouse.
- Data processing pipeline development using Python and SQL.
- New database solutions architecture.
- Strategic planning and external client interaction.

What's In It for You

- An opportunity to join a very smart, motivated team and to directly impact the product portfolio!
- Excellent benefits and vacation policy.
- Growth potential and a great working environment.If you are a Data Engineer looking to be on a collaborative team at a growing and socially important startup, please apply today!

Applicants must be authorized to work in the U.S.Please apply directly to by clicking 'Click Here to Apply' with your Word resume!

Looking forward to receiving your resume and going over the position in more detail with you.

- Not a fit for this position? Click the link at the bottom of this email to search all of our open positions.

Looking forward to receiving your resume!

CyberCoders

CyberCoders, Inc is proud to be an Equal Opportunity Employer

All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.

Your Right to Work - In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

Copyright 1999 - 2019 . CyberCoders, Inc. All rights reserved",CyberCoders,"New York, NY",,Data Engineer
188.0,"Role: Cloud Data Engineer
Location: New York, NY
Term: Contract to Hire Position

Must Have:
• AWS data pipeline - ETL
• AWS Glue
• AWS Lambda
• Athena
• Devops - Terraform
• Strong python experience - panda
• Strong database / warehouse skill MySQL or sql server
• Experience in dealing with the structured, semi-structured and unstructured datasets.
• Excellent communication skills to collaborate with the data engineering, analytics and science teams
• Experience in Social Media Datasets such as Twitter, YouTube, Facebook, Instagram is a plus",Harvey Nash Inc.,"New York, NY",$103k–170k,Cloud Data Engineer
167.5,"Role overview
To help us achieve our goals, we are seeking a Director, Data Engineering to join our team. The Director of Data Engineering will lead the development and execution of highly complex and large-scale data structures and pipelines that link and enrich data, generating insights whose value are exposed via APIs, analytical models and artificial intelligence, thus increasing the value of our WW data. You will leverage your knowledge to build a Platform for all off WW data to flow into a Data Lake using modern engineering practices such as Agile development, Continuous Integration and Deployment, Micro-services, etc. As a Director at WW, your architectures, designs and decisions will be highly visible, and will directly impact and delight our users; both internal and external.
Key responsibilities
Develop a high-level view of current and future state business and technology
Provide leadership in developing standards, architectural design patterns and best practices
Lead and... help build a team of high performing engineers
Evaluate and identify the benefit case for using new technologies
Provide leadership role by participating in architecture review conversations to help guide development of Content Management solutions that are strategically aligned to the overall architecture vision
Work with other IT staff and review product requirements, solution alternatives, and designs and standards compliance
Infuse architectural concepts like responsive, resilient, elastic, reusable, standards-based, fault-tolerant, scalable and manageable solutions
Provide support to other tech staff in high priority issue resolution
Participate in Oversight of delivery of final solutions including Coding, Vendor Engagement and performing code reviews of technical team members' contributions
Experience required
Understanding of Dataflow/Beam or other data processing framework such as Spark or Flink
Understanding of Python and Java
Understanding of BigQuery or other SQL engines such as Hive, Impala, Athena, or Presto
Understanding of object stores such as S3 and GCS
Experience building data pipelines in a cloud environment such as GCP or AWS
Experience integrating data warehouses with BI tools such as Looker, Tableau, or Data Studio
Experience working with Test-Driven Development
Experience working in an Agile environment
Strong knowledge of profiling techniques, memory management, troubleshooting and performance tuning
High level of self-motivation and initiative and the ability to excel in a fast-paced environment and quickly acquire proficiency with new skills
Excellent written and verbal communication skills
Proven ability to establish effective and professional working relationships with co-workers.
Ability to manage parallel work streams
Computer science degree or equivalent academic or professional background.
Nice to have
Understanding of Airflow or other workflow scheduler, such as Luigi or Amazon SWF
Some knowledge of Kubernetes and/or Docker
Knowledge of ACLs in GCP/AWS
As a company, our purpose is to inspire healthy habits for real life. And as an employer, we inspire the greatest people to do their best work. We provide benefits for real life to help protect your health, finances and overall well-being, including:
Competitive compensation and profit sharing plan
A 401K plan to help you plan for your future, plus company match
Health care coverage starting on your first day
Tuition reimbursement and online courses to help you reach your career aspirations
Commuter benefits
Yearly well-being allowance for your physical, financial, social and emotional well-being
Free WW membership for you plus 3 free WW memberships for your friends and 3 for your family
Free fruit, snacks and coffee to get you through your day
Summer Fridays, happy hours, and company outings
Robust employee referral bonuses
Developmental opportunities and assignments to grow your career
WW is an equal opportunity employer. WW does not discriminate on the basis of sex, race, color, creed, national origin, marital status, age, religion, sexual orientation, gender identity, gender expression, veteran status, or disability.
Any offer of employment is contingent upon the satisfactory results of reference and background checks.#LI-TN1",WW (the new Weight Watchers),"New York, NY",$75k–185k,Director of Data Engineering
121.4,"The candidate will primarily be working with data ingestion, querying, sanitizing and monitoring. He/she would offer a combination of strong SQL and ETL capabilities and help shape our large-scale data platform, reporting initiatives and offer support to front/middle/back office teams for analyzing residential mortgage datasets. Experience with fixed income/mortgage data/loans would be a plus.

Responsibilities
• Design, develop and support Extract Transform Load (ETL) processes using SSIS
• Optimize and tune ETL applications to manage high volume batch data transfer
• Troubleshoot data issues, recommend, test and implement solutions
• Assist with data analysis and investigation
• Document technical requirements and solutions
• Engage in project planning and delivering to commitments
• Interact with cross-functional teams to ensure complete delivery of solutions
• Assist with configuration and deployment
• Work with other developers to provide datasets which would drive Tableau views... and SSRS reports
• Partake in weekly planning and review sessions",Bright Metro,"New York, NY",$59.9k–123k,Data Engineer (SSIS)
,Focus Capital Markets is 40 years young and our Big Data engineering team is growing. We will give bright articulate Python developers the opportunity to work in Big Data engineering and machine learning. You will work with some of the top data engineers in the world and turn banking and finance customer data into useful data streams that will have a direct impact on revenue growth. We can provide an opportunity to build data pipelines and work in the cloud space if you can provide us with at least a year of commercial expertise in Python and a burning desire to learn new things,Focus Capital Markets,"New York, NY",,Python Data Engineer
164.1,"Please apply to the position here: https://vigilant.breezy.hr/p/411acbbab7f6-data-engineer?source=angellist

About the job:

The Data Engineer will join Vigilant’s growing engineering team to make major contributions to both technical and business sides of the company. This role is key to our ability to provide increasingly sophisticated data products and expand into new verticals. The person who fills this position will be part of a small data team within engineering and will handle multiple responsibilities.

The ideal candidate has experience with:

- Building and maintaining end-to-end ETL pipelines, namely:

-- Implementing data from a range of sources such as downloadable datasets, API endpoints, massive Excel/CSV files, and non-standardized sources such as PDFs and database dumps

-- Transforming, normalizing, cleaning, and mapping extracted data both at the extraction stage as well as in the destination data warehouse

- Analysis of multivariable datasets to understand the... distinctions, whether the datasets encompass each other, and to sort, aggregate or separate the individual datasets as needed

- Advanced database administration, including experience maintaining, migrating a cluster, access controls, and implementing subsequent versions

- Conducting analysis of data quality, assurance, coverage, and answering specific business queries as needed

About you:

- You’re comfortable writing generalized data transformation tools to be used by engineers and developers on the team, with thorough documentation

- You can write and/or update models based on database-derived datasets

- You’re able to spin up instances of common databases with Docker and configure production tables, indices, and access controls

- You’re comfortable getting hands-on with complicated data, and understand that you will not use pre-processed datasets

What you’ll do:

- Assist with the data warehouse integration into various products and applications

- Communicate with the data provider and determine a resolution to the requisite acquisition, integration, and implementation

- Administer and optimize our Linux deployments

- Implement integrated automated test frameworks, to improve code management and product quality

- Provide analysis of performance metrics, and diagnose and troubleshoot potential infrastructure issues as they arise

- Work to reduce the development defect cycle time, and conduct statistical and analytical assessments

- Establish and develop naming conventions and consistent schemas to ensure clarity and usability in the data aggregated
• Vigilant's Values*:

Constant learning and insatiable curiosity. We organize information, and we’re always keen to know more. We’re committed to learning and growth and enjoy working with those who share that commitment.

We’re guided by a deep empathy for our customers and their needs. We think carefully about how our users - and our world - may be affected by every decision we make. We’re constantly aiming to drive improvements for our users.

We aim high. We’re taking on large, difficult problems, and are looking to build meaningful solutions and scale a substantial company that will have lasting impact. Ambition is a compliment in our book.

Relentless tenacity. We do the work. We tend to see obstacles as existing to be overcome. When we decide to do something, we work at it until it gets done.

Real humility. We know that we aren’t doing this alone. We hold ourselves accountable for our missteps, and gladly share credit for our successes. We recognize that we don’t have all the answers, or even most of them.

We draw the owl. We’re growing, and there isn’t usually a defined set of instructions for any given task. We figure it out. We work, iterate, improve. (In the interest of full disclosure, we shamelessly borrowed this notion from Twilio’s corporate values).

We have a bias towards action. If we notice an issue is affecting the business, we work to fix it immediately, even if it’s not in our wheelhouse.

We save our paperclips. We have an ownership mentality and act in the company’s best interest. We’re conscious of how our actions and behavior impact the business and the company’s bottom line. We celebrate efficient solutions.

We follow through. We do what we say. We get the job done",Vigilant,"New York, NY",$91.6k–145k,Data Engineer
164.1,"Big Data Engineer

Manhattan,NY

Vedio

GC/USC

JD-

NEED SUBMISSION WITH 2 PROFESSIONAL REFERENCES AND SKYPE ID ALSO LINKDIN IS A BID PLUS

Background: Ideally someone from media background. Financial background people aren't good culture fit.

""Strong experience with data, python OR scala. Not looking for a data scientist. Looking for a softwaredeveloper/engineer with AWS experience. Your run of the mill data technologies:spark, Hadoop, etc.""

Job Description:

We are lookingfor a talented Sr. Data Engineer for an exciting opportunity on the dataengineering team. You would be involved with designing workflows for data andanalytics tools that are a big part of the road-map for 2019 while managingdata and infrastructure to efficiently query data in the billions. Candidatesconsidered based on their ability to design large distributed technicalsolutions, architect, manage, monitor and optimize data pipeline projectsresulting in actionable data and data pipelines which support the... largerorganization.

Responsibilities:
• Architect, Design and Maintain Data Pipelines through the lifecycle of the product.
• Optimize and Monitor existing data pipelines using AWS infrastructure.
• Write Python/Scala applications for data processing and job scheduling.
• Understand and Manage massive data-stores.
• Integrate products from data projects into APIs built in Ruby/Rails
• Expose large data sets
• Enjoy being challenged and solve complex problems on a daily basis
• Design efficient and robust ETL workflows
• Manage real time streaming application and data flow
• Investigate, procure and ramp up to new technologies
• Ability to work in teams and collaborate with others to clarify requirements
• Build analytics tools that utilize the data pipelines to provide meaningful insights into data",Tek inspirations llc,"New York, NY (+2 others)",$91.6k–145k,Big Data Engineer
216.0,"About this job

Location options: Visa sponsor, Paid relocation

Technologies

python, hadoop, sql, etl

Job description

As a member of our Data Engineering team, you’ll sit with a cross-functional team of software engineers, data scientists and business analysts, working closely with your team to turn complex sources into understandable data. The data sets, pipelines and tools that you build will be involved in pivotal, company-level decisions for years to come. Not only that, but you’ll be working on a team where constant learning and team-wide knowledge sharing is a core part of our culture.

RESPONSIBILITIES
• Build and maintain data processing services
• Write, test, and review primarily microbatch or streaming ETL
• Continuous improvement of our system, tests, and data quality indicators
• Influence our technical decisions
• Keep yourself informed and up-to-date with technologies
• Encourage the technical growth of your teammates

QUALIFICATIONS
• You love working directly with... the people whose problems you're solving
• You are deeply skilled at data modeling, storage, security, and retrieval
• You're motivated and driven. You naturally take responsibility for a project and ask for help when needed
• You enjoy working with dynamic programming languages, relational databases, and distributed systems. Our platform is ever evolving, but currently is a combination of Python, Java, Postgres, Kubernetes, Spark, Presto, Kafka, and Mongo
• You gain a deep understanding of the products and tools you work with
• You check your work and stand behind what you’ve built",Squarespace,"New York, NY",$140k–152k,Senior Data Engineer
164.1,"About this job

Location options: Paid relocation

Technologies

python, node.js, postgresql

Job description

A Better opportunity

Help us hack a thirteen trillion dollar industry by building a product that will allow more people than the status quo to own a home and build wealth rather than rent for life. Our tech team is small, and you will be a big part of defining the technical direction and culture. We encourage proposals for projects off the beaten path, experimentation with different frameworks and libraries, and doing as you see fit to solve problems. We also offer above-market compensation and equity, as well as full benefits.

Some projects you could be working on
• Work closely with our product team to understand funnel drop off and come up with product ideas
• Work closely with the marketing team to optimize our acquisition funnel
• Present conclusions to the executive team that can impact the strategic direction of the company
• Build a lead scoring model to help our... customer support team prioritize.
• Model the time-lag of conversions using fun math like Gamma distributions
• Design an experiment to understand the causal impact of an outbound phone call on conversion rates
• Build web scrapers to track price data for other mortgage lenders
• Migrate our data warehouse to Redshift
• Work on our underwriting engine, which turns out to be NP-complete and can be posed as a mixed integer programming problem
• Transcribe all our phone calls using speech-to-text and figure out ways to optimize customer support.

Better Technology
• We do continuous deployment and we ship code 50-100 times every day
• The data stack is all in Python 3.6
• We use Node.js, Python and Scala for services
• Postgres for the database
• Kubernetes, for deployment and devops
• AWS for infrastructure, leveraging EC2, S3, SWF, CloudFront, Route53, and much more

The team
• The tech team is currently 30 engineers but growing quickly
• Erik Bernhardsson (CTO) used to run the data team and the music recommendation team at Spotify. He is the open source author of a few popular projects like Annoy and Luigi and writes a blog about (mostly) data",Better,"New York, NY",$91.6k–145k,Data Engineer
141.6,"Fluent is a leading performance marketing company with an expertise in delivering measurable results. We interact with 1MM US consumers daily across our owned and operated websites. Our team uses the insights gathered from these interactions to drive growth for our partners. Through engaging digital experiences, Fluent creates meaningful and lasting connections between brands and consumers.

The performance analytics team is currently searching for a talented Junior Data Engineer who will work hard and help us develop the next generation of Fluent’s technologies. Our online presence touches millions of people per day, requiring the candidate to possess both cutting edge technical acumen and best in-class software development/design practices. We’re an agile group, but we know how to work hard and have fun. We are looking for a candidate that is analytically minded, open to new ideas, and actively demonstrates grit, often finding new and creative ways to solve problems. This role... provides a strong opportunity for personal and career growth.

What You’ll Do:
• Design and Implement efficient data pipelining solutions
• Develop API integrations with third-party vendors.
• Optimize and Tune Database Processes
• Build Large Scale, High-Performance Analytical Frameworks to leverage Fluent’s unique product offerings.

Education & Experience:
• About 2 years of professional experience excluding internships and academic work
• Bachelors Degree in Computer Science, Engineering, or Equivalent Program ( Mathematics, etc.)
• Familiarity building and shipping production software
• Previous work with distributed system technologies ( Spark, Hadoop, EMR), NoSQL ( MongoDB, Cassandra, Neo4J), and RDBMS(MySQL, PostgreSQL).
• Proficiency in one or more of the following ( Python, Java, Scala, C#, or C++)
• Clear Communication Skills and a strong desire to challenge the status quo

At Fluent, we like what we do and we like who we do it with. Our team is a tight-knit crew of go-getters; we love to celebrate our successes! In addition to two fully-stocked kitchens, catered breakfast and lunch, and free workout classes on site, our office manager keeps the calendar stocked with activity-filled events. When we’re not eating, working out, or planning parties, Fluent folks can be found participating in recreational sports leagues, networking with She Runs It, and bonding with across teams during quarterly outings to baseball games, fancy dinners, and pizza-making classes. And we have all the practical benefits, too…
• Competitive salary
• Discounted gym memberships
• Ample career and professional growth opportunities
• Open office space for team collaboration
• Health, dental, and vision insurance
• 401K with generous employer match
• The amazing opportunity to work for a high-flying performance marketing company",Fluent,"New York, NY",$79.1k–125k,Junior Data Engineer
164.1,"At Kensho, we hire talented people and give them the autonomy and support needed to build amazing technology and products. To do this, we look for people who insist on a bias towards action to minimize unhelpful hierarchy and process. We collaborate using our teammates' diverse perspectives to solve hard problems. Our communication with one another is open, honest and efficient. We dedicate time and resources to explore new ideas and, as a result, we produce technology that is scalable, robust, and useful.

As a Data Engineer at Kensho, you are a thoughtful, collaborative, and dynamic technologist who loves working on a variety of systems for wrangling data and making it available and useful for other teams in Kensho. You think deeply about the implications, relationships, edge cases, and failure modes, and you are passionate about correctness, uptime, and stability.

What You'll Do:
• Build a robust data platform for analytics and machine learning with a focus on scalability and... test-driven reusable code
• Dig into data and think deeply. Analyze, transform, structure, and crunch financial data that is growing in size, diversity, and complexity, detecting anomalies and handling variations
• Design interfaces, APIs, and services that address immediate needs What We Look For:
• Desire to work on diverse sets of financial and related data
• Practical understanding of algorithms, data structures, and design patterns
• Effective coding, documentation, and communication habits
• Thoughtful and collaborative code-reviewing and teamworkTechnologies We Like:
• Python, Kubernetes, Docker, Kafka, RabbitMQ, Elasticsearch, Postgres, Redis, AWS, microservices architecturePerks:
• Medical, Dental, and Vision insurance with 100% premium covered.
• Unlimited vacation days.
• Paid Parental Leave.
• 401(k) plan with 6% employer match.
• Free snacks and drinks.
• Membership and easy access to Equinox gym near One World Trade Center.
• Citibike (bike sharing program) membership.
• Phenomenal view of NYC from the 76th floor of One World Trade Center.

All qualifiedapplicantswill receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin.

kensho.com",Kensho,"New York, NY",$91.6k–145k,Data Engineer
164.1,"Our client, a Global Insurance Company, is seeking a Data Engineer in their Midtown Manhattan office.

Responsibilities
• Ingests, merges, prepares, tests, documents curated datasets from various novel external and internal datasets for a variety of advanced analytics involving multi-variate models
• Utilizes data wrangling/data matching/ETL techniques while to explore a variety of data sources, gain data expertise, perform summary analyses and curate datasets
• Functions as data expert, contributes to analytics/solutions design and productizing decisions
• Collaborate with Business leaders to understand business challenges and devise solutions by using business acumen and mining vast amounts of data to draw insights
• Can work independently with some supervision and be part of a collaborative team
• Work with Project Managers and Scrum Masters to provide milestones and stories
• Proactively and effectively communicates in various verbal and written formats with senior level member of... the team and partner
• Actively participates in proof of concept tests of new data, software and technologies. Shares knowledge within the team
• Follows industry trends and related data/analytics processes and businesses. Attends conferences, events, and vendor meetings as needed

Required qualifications
• Graduate-level degree in computer science, engineering, or relevant experience in the field of Business Intelligence, Data Mining, Database Engineering, Programming
• 3-5 years of overall experience working in the field of data wrangling and programming with a minimum of 1 year experience with ingesting, cleaning, merging and applying necessary data wrangling logic in Hadoop
• 1+ years in writing complex SQL queries in any of the following and/or similar databases - Oracle, SQL Server, DB2, MySQL
• Proficiency using Python for all data related work such as Numpy, Pandas, PySpark
• Experience working with Linux Operating System
• Experience working with data visualization tools or packages
• Experience building Exploratory Data Analysis reports such as Histograms, Box plots, Pareto, Scatter Plot using R, Python or a Data Visualization tool such as Tableau and Spotfire

Preferred:
• Understanding of statistical modeling concepts, designs and analytics-based products
• Any experience in using ETL tools such as Ab Initio, Talend, Informatica, Pentaho
• Any experience working with Data Warehouses and/or Data Marts
• Any experience in Life Insurance business

Visit us at www.phytontalent.com for more information.

Phyton Talent Advisors is a proud winner of the Inavero's 2017 Best of Staffing® Client Award & Named by Forbes as One of America's Best Professional Recruiting Firms

As an Equal Opportunity / Affirmative Action Employer, by choice, Phyton Talent Advisors will not discriminate in its employment practices due to an applicant's race, color, creed, religion, sex (including pregnancy, childbirth or related medical conditions), sexual orientation, gender identity or expression, age, national origin, marital status, citizenship, physical and mental disability, criminal record, genetic information, predisposition or carrier status, status with respect to receiving public assistance, domestic violence victim status, a disabled, special, recently separated, active duty wartime, campaign badge, Armed Forces service medal veteran, or any other characteristics protected under applicable law",Phyton Talent Advisors,"New York, NY",$91.6k–145k,Data Engineer
,"Azure Big Data Engineer - NYC - $160K Base + Bonus

Come join this client in the city that never sleeps! Come and work for a Tech Savvy company that stays ahead of the curve. You will not find a better place to show off your Technical skills and sharpen your trade with some of the best people in the industry! This client wants to know if you can develop, Construct, Test and maintain architectures, build Big Data Warehouses/Cubes from scratch, Establishing and implement data pipelines, Work with Azure Data Factory, Analysis and Visualization, Develop ETL, and hands on coding and developing in Spark. If you are up to the challenge then this is the place for you.

Experiences In:
• Azure (Data Factory, Data Warehouse)
• Spark
• Data Analytic
• Microsoft Business Intelligence
• Power BI and Blob Storage
• Azure SQL, Blob/ Lake
• implementing and optimizing data pipelines
• SSIS, SSAS, SSRS
• Willingness to Travel
• Data Modelling
• Production experience a plus

What they can give you:
•... competitive salary
• PTO
• gym memberships
• full health insurance packages
• 401K+ match
• Remote Possible
• flexible hours
• Opportunity for advancement and career growth
• Beautiful Location
• Monthly Networking Events
• Commuter benefits
• Rewarding company culture
• Bonus incentives
• Ability to grow your skill set

Please be sure to reach out to me as soon as possible so you don't miss out on this amazing opportunity! This role is not set to last! For more information, or to apply, call Michael at 646 863 7444, if you prefer email- email an up to date CV to m.morgan@nigelfrank.com.

Nigel Frank International is the global leader for Microsoft recruitment, advertising more Azure roles than any other agency. We deal with both Microsoft Partners & End Users throughout North America. By specializing solely in placing candidates in the Azure market I have built relationships with most of the key employers in The Greater New York area and have a complete understanding of where the best Azure opportunities are",Nigel Frank,"New York, NY",,Multiple Azure Big Data Engineers - NYC - $160K Base + Bonus
118.25,"Knowledge is our product, and data is our platform. We need engineers who look at a data set and want to unlock the answers it holds inside. Engineers who look at a data set and think about how to make sure it is correct. Engineers who look at a data set and want to make infrastructure to help build it better, faster, and stronger.

As a Data Engineer, you will work closely with oncologists and statisticians to build software that will help our customers discover novel insights into their data. You will design our data infrastructure, and use it to develop extensible, robust data and analytics pipelines, tools, visualizations, and services for accessible and flexible data analysis. You will learn more than you ever thought possible about how cancer is treated in the real world, and your work will directly support oncology research and publications.

Who you are:
• You hold a BS, MS, or Ph.D. in computer science or related field
• You have 2+ years work experience
• You have experience... with languages like Python, C++, Java, or C#
• You are passionate about performance, reliability, and scalability of systems
• You are inspired by our mission to improve cancer research through technology
• You seek simple approaches to complex problems
• You like science and/ or medicine just because it's cool

Bonus points if you have any of the following:
• Have a good understanding of relational databases like PostgreSQL, MySQL or MSSQL
• Have real passion for data and a strong understanding of statistics
• Have developed distributed data processing systems against large, heterogeneous data sets
• Have taken a leading role in delivering complex software systems all the way to production
• You almost decided to go to med school

Flatiron is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, or Veteran status. If you have a disability or special need that requires accommodation, please let us know",Flatiron Health,"New York, NY",$76.3k–83.9k,Senior Data Engineer
,"JPC - 43 Cloud Engineer

Contact. Amruta Kukde..973 933 4506. amruta@zenart.com

Looking for Cloud Data Engineer consultant with 6 months right to hire to work on Social Media Data analytics / Warehouse project. MUST have Social Media/ Fantasy Football experience.
Location: NYC
Expertise / skills :
AWS data pipeline - ETL
Aws Glue
Aws Lambda
Athena
Devops - terraform
Strong python experience - panda
Strong database / warehouse skill MySQL or sql server",ZNA Infotek,"New York, NY",,Cloud Data Engineer CTH position in NYC
164.1,"We are looking for a talented Data Engineer for an exciting opportunity on the data engineering team. You would be involved with designing workflows for data and analytics tools that are a big part of the road-map for 2018 and managing data and infrastructure to efficiently query data in the billions. Candidates considered based on their ability to design large distributed technical solutions, manage and optimize data pipeline projects resulting in actionable data and data pipelines which support the larger organization.

This position can be based in New York City, Chicago or Boston.

You will:
• Designing Data research projects through the life-cycle of the product
• Write Java/Scala applications for data processing and engineering
• Manage and Administer massive data-stores and pipelines
• Integrate products from data projects into APIs built in Ruby/Rails
• Enjoy being challenged and solve complex problems on a daily basis
• Excellent oral and written communication skills
• Manage... real time streaming application and data flow
• Investigate, procure and ramp up to new technologies
• Ability to work in teams and collaborate with others to clarify requirements
• Build analytics tools that utilize the data pipelines to provide meaningful insights into data",Publicis Groupe,"New York, NY",$91.6k–145k,Data Engineer
177.0,"Job Location: New York, NY

Responsibilities:
• Leverage data engineering experience to build data platforms and promote data-driven solutions
• Develop data driven apps on one of the massive dataset's
• Balance independent and collaborative problem-solving
• Communicate findings and results with the team
• From day one, you think and execute

Requirements:
• 5+ years of financial services industry experience
• BS or MS degree in Computer Science
• Industry experience working with big data platforms is required
• Mastery in developing data solutions using SQL, Hive, HDFS, Spark and data modeling
• Detailed experience in Unix and Shell Scripting
• Experience in any one of the languages: Java and/or Python/Scala
• Hands on experience with cloud technologies: AWS/GCP
• Expertise in debugging and troubleshooting code
• Experience in optimizing data pipelines
• Experience in deploying models to production
• Strong presentation and communication skills
• Comfortable working in Scrum",Hudson Data,"New York, NY",$115k–124k,Senior Data Engineer
,"OVERVIEW

We’re hiring a Senior Data Engineer to join our growing Data Engineering Team to elevate Glossier as an inviting, fun way to buy beauty products. You’ll join a team focused on building a delightful e-commerce and in-store experience, and fostering the most welcoming community to discuss skin care and makeup.

As a Senior Data Engineer at Glossier you are responsible for (i) delivering and maintaining highly available computing platforms, (ii) creating data integration services, and (iii) building data products. Points (i) and (ii) regard systems that collect and transform data from a number of sources, storing it in highly optimized database systems. The services will largely be written in Python and data will be stored in a combination of Snowflake, Redshift, and Amazon S3. You will also maintain ongoing reliability, performance, and support of the infrastructure. This includes monitoring the computing environments and providing solutions based on application needs and... anticipated growth.

Point (iii) regards the building of custom services that are accessible and internal web-applications that use computed data to aid our business. These include search engines and recommendation systems, among other things.

As a Senior Data Engineer, you will also take an active role in designing the future of the data engineering practice at Glossier. A successful candidate will combine strong technical skills, a passion for creative problem solving, and an intense curiosity.

At Glossier we are also mindful of building an inclusive culture, where decisions are made transparently and we support each other's learning and growth. Data is a critical component at Glossier and ensuring consistent, reliable access to our data is a significant strategic priority.

We expect you to have good communication skills and collaborate across functions to deliver robust solutions in correspondence with the business needs of the company.

OUR DATA STACK
• Data Warehousing with Snowflake, AWS Aurora, Redshift
• AWS for serving infrastructure
• Python, JavaScript, and TypeScript
• DBT and Luigi for ETLs
• Fivetran & Stitch
• Segment
• Looker
• Docker

6 MONTH EXPECTATIONS
• Have contributed to the data pipeline process, creating new custom integrations that bring data into our systems
• Are comfortable working with our technology stack
• Have taken an active role in designing the future of our data engineering system
• Collaborate with team on best practices and overall business strategy

12 MONTH EXPECTATIONS
• Deliver data engineering system—including data pipeline and data warehousing components—that is simple, reliant, and performant
• Have made numerous contributions to our data pipelining system and data monitoring
• Have taken a lead role in managing the development and architecture design of our data infrastructure

SKILLS AND QUALIFICATIONS
• 3 - 5 years of experience working in software development, data engineering, or related STEM fields
• 3+ years of working experience with various relational databases and data warehousing
• Proven track record of excellence in delivering production-grade software that scales to thousands of users
• Strong programming skills in Python and SQL
• Practical experience in best practices for developing data pipelining frameworks
• Experience in Linux is a plus
• Ability to learn autonomously and quickly
• Analytical, creative and commercial mindset
• Extremely organized and detail-oriented with effective multitasking and prioritization skills
• Highly motivated, willing to take ownership of work, drive to solve problems and work effectively under pressure
• Excellent written and verbal communication skills, willing to proactively engage other team members in fostering a strong collaborative team-oriented environment",Glossier,"New York, NY",,Senior Data Engineer
202.0,"OVERVIEW

We’re hiring a Senior Data Engineer to join our growing Data Engineering Team to elevate Glossier as an inviting, fun way to buy beauty products. You’ll join a team focused on building a delightful e-commerce and in-store experience, and fostering the most welcoming community to discuss skin care and makeup.

As a Senior Data Engineer at Glossier you are responsible for (i) delivering and maintaining highly available computing platforms, (ii) creating data integration services, and (iii) building data products. Points (i) and (ii) regard systems that collect and transform data from a number of sources, storing it in highly optimized database systems. The services will largely be written in Python and data will be stored in a combination of Snowflake, Redshift, and Amazon S3. You will also maintain ongoing reliability, performance, and support of the infrastructure. This includes monitoring the computing environments and providing solutions based on application needs and... anticipated growth.

Point (iii) regards the building of custom services that are accessible and internal web-applications that use computed data to aid our business. These include search engines and recommendation systems, among other things.

As a Senior Data Engineer, you will also take an active role in designing the future of the data engineering practice at Glossier. A successful candidate will combine strong technical skills, a passion for creative problem solving, and an intense curiosity.

At Glossier we are also mindful of building an inclusive culture, where decisions are made transparently and we support each other's learning and growth. Data is a critical component at Glossier and ensuring consistent, reliable access to our data is a significant strategic priority.

We expect you to have good communication skills and collaborate across functions to deliver robust solutions in correspondence with the business needs of the company.

OUR DATA STACK
• Data Warehousing with Snowflake, AWS Aurora, Redshift
• AWS for serving infrastructure
• Python, JavaScript, and TypeScript
• DBT and Luigi for ETLs
• Fivetran & Stitch
• Segment
• Looker
• Docker

6 MONTH EXPECTATIONS
• Have contributed to the data pipeline process, creating new custom integrations that bring data into our systems
• Are comfortable working with our technology stack
• Have taken an active role in designing the future of our data engineering system
• Collaborate with team on best practices and overall business strategy

12 MONTH EXPECTATIONS
• Deliver data engineering system—including data pipeline and data warehousing components—that is simple, reliant, and performant
• Have made numerous contributions to our data pipelining system and data monitoring
• Have taken a lead role in managing the development and architecture design of our data infrastructure

SKILLS AND QUALIFICATIONS
• 3 - 5 years of experience working in software development, data engineering, or related STEM fields
• 3+ years of working experience with various relational databases and data warehousing
• Proven track record of excellence in delivering production-grade software that scales to thousands of users
• Strong programming skills in Python and SQL
• Practical experience in best practices for developing data pipelining frameworks
• Experience in Linux is a plus
• Ability to learn autonomously and quickly
• Analytical, creative and commercial mindset
• Extremely organized and detail-oriented with effective multitasking and prioritization skills
• Highly motivated, willing to take ownership of work, drive to solve problems and work effectively under pressure
• Excellent written and verbal communication skills, willing to proactively engage other team members in fostering a strong collaborative team-oriented environment",Glossier,"New York, NY",$114k–176k,Senior Data Engineer
164.1,"Justworks is seeking a Data Engineer to join our team. As a successful candidate, you have demonstrated the ability to build, deploy and maintain large-scale, data-driven solutions. You love to take on complex data-related problems and will help create and maintain large-scale batch and real-time data pipelines that will directly influence business decisions as well as contribute to improving the quality of our data tools and large scale data infrastructure.

We work with a variety of open-source/home grown tools in a variety of languages and we want you to be excited about learning/contributing to all of it.
What you'll do
• Collaborate with a cross-functional team to find solutions to customer challenges
• Create and optimize our data pipeline architecture
• Build and maintain data access platform for our data science team and company wide
• Design and implement ETL processes through cloud based solutions (S3, Redshift)
• Improve our data tools and contribute to scaling our data... infrastructure
• Keep extremely sensitive data compartmentalized and secureWho you are
• Minimum of 3 years of professional hands-on experience
• Knowledge of data modeling, data access and data storage techniques
• Experience working with complicated systems at scale
• Strong experience with databases, SQL, writing and debugging
• Working knowledge of bash scripting
• Experience with cloud-based data solutions (AWS preferred)
• Working understanding of code and scripting languages (Ruby, Python, bash)Technologies we use:

- Ruby on Rails, Python, MySQL, AWS, Git

- Elasticsearch, Docker, InfluxDB, Redshift",Justworks,"New York, NY",$91.6k–145k,Data Engineer
167.5,"Morgan Stanley Services Group Inc. seeks an Associate, Data Engineer in New York, New York

Evaluate Hadoop infrastructure requirements and design/deploy solutions including high availability, big data clusters, and elastic load tolerance. Develop automation as well as install and monitor Hadoop and data streaming ecosystem components. Dig deep into performance, scalability, capacity, and reliability problems to resolve issues. Data curation, prep, ETL (extract, transform, load), and analytics development of the many data sources ingested into streaming platform and Hadoop. Troubleshoot and debug Hadoop ecosystem run-time issues. Provide developer and operations documentation. Run proof of concept projects with customers.

QUALIFICATIONS

Requirements:

Requires a Bachelor’s degree in Information Technology, Computer Engineering, Computer Science, or related field of study and five (5) years of experience in the position offered or five (5) years of experience as a Technology... Consultant, Software Engineer, or related occupation. Requires five (5) years of experience with: Java/J2EE development; Junit testing framework; and Eclipse development tools. Requires four (4) years of experience with: Linux/Unix system administration; Maven software project management; and distributed version control systems including Git and SVN. Requires three (3) years of experience with: Hadoop ecosystem including Map Reduce, Hive, SQOOP, and PIG. Requires two (2) years of experience with: Hadoop administration, distributed cluster management and operations with MapR, or Cloudera Hadoop distributions using Yarn for cluster Resource Management; building and scaling Hadoop based or Unix hosted database infrastructure for an enterprise including software, network, and storage; and NoSQL databases. Requires one (1) year of experience with: IntelliJ development tools, development on distributed execution and data streaming platforms including Scala, Apache Spark, and Apache Kafka; and ElasticSearch distributed search and analytics engine; and Hadoop administration",Morgan Stanley Technology,"New York, NY",$75k–185k,"Associate, Data Engineer"
202.0,"Do you wake up early, eager to see how many of your ETL cron jobs failed overnight? Of course you don’t! You put an end to all of those sleepless nights by leveraging new technologies to schedule workflow dependencies and set up alerting ages ago. You’ve been getting a full 8-hours of sleep ever since!

Updater’s Data Team is an independent function that takes a strategic approach to ensuring the business is leveraging data across all units to drastically improve business outcomes. We are a group of Data Engineering and Data Science/Analytics professionals, working together closely to ensure we are a data-driven company. By building and maintaining the infrastructure that powers all of our Data Science, Product, Marketing, Sales, and Success analytical and operational functions, Data Engineering is one of the most highly leveraged teams at Updater!

What You’ll Do

As a Senior Data Engineer at Updater, you’ll lead the development of an innovative data environment by leveraging... industry best practices and cutting edge approaches to build a robust and scalable data platform. You’re excited to come on board and work with our Engineering & Tech Ops teams to understand our data model and construct a thoughtful approach to building a scalable data architecture that provides a foundation for teams across the company to self-serve their data needs.

You will lead the development, documentation, access, and quality of our centralized data warehouse solution (our stack includes Airflow, Fivetran, and Snowflake!), and will balance ready access to existing data features while incorporating new features in a timely manner. You’ll collaborate with Engineering on ensuring data capture for new product initiatives, with our business partners across all major functions at the organization to anticipate data needs, and proactively develop data solutions to address the toughest business problems.

Our ideal candidate isn’t satisfied until a project is seen through to meaningful impact. If you love working with data and want to see your work impact the entire organization, we want to talk to you!

What You Have

You’re a senior engineer who has extensive experience leading data engineering and data warehousing initiatives, and are excited to be a part of an industry-leading company growing at an extremely fast pace! We’re looking for someone with:
• Humility and a sense of humor
• Strong communication skills, deep levels of empathy, and a keen eye for innovation and pragmatism
• 3+ years experience building and maintaining data pipelines & data models as part of a Data Warehouse solution
• Expert Python and SQL skills
• Ability to manage full project lifecycles
• Experience working closely with Engineering, Product, and Analytics/Data Science teams
• Experience with modern ETL tools (Fivetran, Alooma, Stitch, etc)
• Experience with modern DWH environments (Snowflake, AWS Redshift and S3, BigQuery, etc)
• Experience with git, GitHub, and the pull request workflow

Bonus Points for:
• Experience with workflow management tools (Airflow, Luigi, etc)
• Experience with streaming data services (Kafka, Kinesis, Firehose, etc.)

Compensation & Benefits:

We're looking for top-tier talent, and we offer compensation packages that include competitive base salary & stock options. Our comprehensive benefits programs include health care, dental, transportation subsidies, and Flexible Paid Time Off (PTO).

About Updater:

Updater makes moving easier for the 17 million households that relocate every year in the US. With Updater, users seamlessly transfer utilities, update accounts and records, forward mail, and much more. Hundreds of the most prominent real estate companies in the US (from real estate brokerages to multifamily and relocation companies) rely on Updater’s real estate products to save clients hours with a branded and personalized Updater moving experience.

Headquartered in New York City, Updater has raised nearly $100 million from leading investors, including SoftBank Capital, IA Ventures, Commerce Ventures, Second Century Ventures (the strategic investment arm of the National Association of Realtors®), and more. Updater ranked #3 on Crain's 2016 Best Places to Work in NYC, ranking as the highest rated tech company on the list, and ranked #7 in 2018. For more information, please visit www.updater.com (http://www.updater.com/) .

Updater is proud to be an equal opportunity employer and will consider all qualified applicants regardless of color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital or family status, disability, gender identity or expression, veteran status, actual or presumed belonging to an ethnic group, or any other legally protected status. If you have a disability or special need that requires accommodation, please let us know",Updater,"New York, NY",$114k–176k,Senior Data Engineer
164.1,"VillageCare – Redefining Wellness

112 Charles St., New York, NY

VillageCareMax

Job Title: Data Engineer

Roles & Responsibilities:

The VillageCare Data Engineer will be responsible for helping to build ETL processes for Village Care’s Data Warehouse in an AWS cloud environment. The Data Engineer will build ETL pipelines, get analysis tools working properly, and stand up core data processing components in Village Care’s cloud-based data processing environment.
• Building python-based ETL jobs
• Light web application development of purpose-built internal tools
• Technical guidance in support of our project management team when defining the scope of data integration projects
• Contribute to designs of new components in modeling and data pipelines
• Remain current on emerging open source data processing projects and tools
• Must have experience with AWS services, Redshift data bases and HIPAA compliant architecture models.

Qualifications:
• Three or more years of experience working... with *nix-based, open source data processing tools
• Fluent in Python, SQL, Spark, Hadoop, AirFlow and/or similar technologies/toolsets
• Two or more years of experience developing production ETL applications
• Four or more years of experience in software development
• Three or more years of experience with SQL
• Curious, informed and opinionated about data processing technologies
• Experience with structured and unstructured data storage and modeling
• Deep understanding of database and filesystem storage/access
• Experience with various data engineering architecture patterns
• Interest in Data Science and Data Analysis

Preferred Education and Experience
• BS or MS in Computer Science
• Experience writing production Python
• Implementation experience with Airflow, Python, Spark etc.
• Experience with healthcare data formats (x12 EDI, HL7, etc)
• Experience implementing stream processing pipelines (Spark, etc)
• MapReduce/Hadoop ecosystem experience (Hive, HDFS/S3, Presto)
• Experience with source control technology like Git
• Experience with testing frameworks (unit and end-to-end)
• Familiar with the usage of Continuous Integration/Continuous Deployment frameworks in AWS like Jenkins, CircleCI, Code deploy and code commit
• Understanding of Docker implementation in AWS
• Understanding of AWS serverless services like Lambda/API gateway
• Good to have: AWS elastic beanstalk, AWS cloud formation

Integrity

You are a team member who serves as a positive example and reflection of why others trust the intentions of VillageCare by:
• Being honest and trustworthy
• Meeting your commitments and obligations
• Acknowledging your role in actions or events with unsatisfactory outcomes

Customer Focus/Cultural Awareness

You are a team member who understands the importance of strong customer service internally and externally and you demonstrate this by identifying customer needs and expectations, and responding to them in a timely and effective manner. You are consistently customer focused by:
• Demonstrating an awareness of the needs of individuals through recognizing multiple levels of connections
• Anticipates and prevents delays or other things that can adversely affect the customer.
• Keeping customers informed about the status of pending actions and inquires

Flexibility/Agility

You are a team member who adjusts quickly and effectively to changing conditions and demands. You understand that change is a necessary and an inevitable aspect of organizational life as well as an opportunity to learn new things. As such, you are flexible and agile by:
• Maintaining a positive view of potentially stressful situations
• Accepting and adapting to organizational or departmental changes
• Viewing change as opportunities for VillageCare to grow in a direction that better serves our clients and our employees

Result Oriented/Innovative Thinking

You are a team member who consistently looks for new and innovative approaches that will improve efficiency in your role. You champion new ideas and build upon existing processes by:
• Using data/fact-based information to make decisions relevant your role
• Understands that obstacles will occur and refuses to use them as an excuse for not achieving results

BEVital

You are a team member that consistently supports VillageCare’s larger organizational culture by displaying a commitment to the three cultural drivers that make VillageCare and our employees vital to the healthcare space by:
• Exceeding expectations in both internal and external customer service areas
• Using data and key information to inform decisions pertinent to your role (where applicable)
• Utilizing relationships, tools and positivity to enhance organizational performance through communication and collaborative team work

VillageCare is committed to superior outcomes in quality health care. Do you share a common commitment to* patient care, customer service and passion *for individuals’ well-being ?

Apply now!

VillageCare:

With over 25,000 people served in 2017, VillageCare’s mission is to promote healing, better health and well-being to the fullest extent possible.

VillageCare began in 1977 as a project by community volunteers to rescue and reorganize a for-profit nursing home slated for closure. It has become a much larger organization that provides post-acute care, community-based services and managed long-term care. As a result of this history, VillageCare has become a valued resource for the people we serve, their caregivers and other provider organizations with which we partner.

VillageCare is committed to the tenets of diversity and workforce that are strengthened by the inclusion of and respect for our differences. We offer our employees a highly competitive compensation and benefits package, a 403(b) retirement plan, and much more.

VillageCare is an equal opportunity employer. We promote recognition and respect for individual and cultural differences, and we work to make our employees feel valued and appreciated, whatever their race, gender, background, or sexual orientation.
• EOE Minorities/Women/Disabled/Veterans",Village Care of New York,"New York, NY",$91.6k–145k,Data Engineer
164.1,"About Our Client

Well known and respected multinational investment bank.

Job Description

- Perform proof-of-concepts for validating new tech or approaches

- Data architechture

- Data lake management

- Support on implementation related to cloud development in AWS

The Successful Applicant

- Hands on technical experience with data lake design

- Expert in integration of diverse data sources

- Strong experience with SQL technologies

- Minimum of 4 years experience in AWS technologies

What's on Offer

DOE",Michael Page,"New York, NY",$91.6k–145k,Big Data Engineer
151.3,"ASCAP is home to more than 700,000 music creator members across all genres - the greatest names in music, and thousands more in the early stages of their careers. We are the world leader in performance royalties, advocacy and service for music creators, and are the only PRO in the US run by its members including songwriters, composers and music publishers.

ASCAP technologists live our mission and we are passionate about what we do for our customers and we practice what we preach. Our technologists serve with humility and a deep respect for their responsibility in helping our business partners and members achieve their goals and realize their dreams. We have an infectious and lively culture and we recognize our successes monthly at our Thursday on-site social hour celebrations. We stand behind our mission and are committed to delivering the impossible.

Bottom line? We outthink ordinary. Discover what you can do with technology at ASCAP!

We're looking for a expert big data and cloud... engineer to build data solutions for our music analytics team in NYC. Do you have experience transforming and moving data in both on-prem and cloud big data platforms? Are you experienced in relational and dimensional modeling? Do you want challenging work that gives you the opportunity to build solutions on a diverse technology stack and own what you build? If so, this might be the role for you.

The Senior Data Engineer is responsible for processing structured and unstructured data, validating data quality, and developing and supporting data products. We are looking for someone with strong hands on experience in all layers of the full stack involving data. We especially need your experience with Cloudera Hadoop, Data warehousing and AWS Cloud technologies. The Data Engineer plays a significant role in Agile planning, providing advice and guidance, and monitoring emerging technologies. This is not a junior programmer position and requires extensive hands on coding and design experience.

Technology We Use
• Cloudera Hadoop, Impala, Hive, Kudu, Yarn, Hue, Spark
• Python, Java, Scala, Unix shell scripting
• AWS, EMR, Snowflake
• Jenkins, Terraform, and CloudFormation

Your Role
• Design, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result
• Derive an overall strategy of data management, within an established information architecture (including both structured and unstructured data), that supports the development and secure operation of existing and new information and digital services
• Plan effective data storage, security, sharing and publishing within the organization
• Gather and process raw, structured, semi-structured, and unstructured data using batch and real-time data processing frameworks
• Ensure data quality and implement tools and frameworks for automating the identification of data quality issues
• Collaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings
• Mentor and lead data engineers providing technical guidance and oversight
• Provides ongoing support, monitoring, and maintenance of deployed products

Qualifications
• 5 – 8 years of development experience at an Enterprise level with at least two of these languages: Python, Java, or Scala
• Experience in a Dev Ops environment including: Jenkins, Terraform, and CloudFormation or similar technologies
• Experience working with high volume heterogeneous data in Cloudera Hadoop
• Business objects, Tableau and experience with batch and real-time data processing frameworks
• Strong background with data modelling, data access, and data storage techniques
• Strong Experience with relational databases like Oracle, MS SQL server and DynamoDB
• Working knowledge of Red Shift, Athena, Snowflake and MySQL especially desired
• Experience in relational and dimensional modeling (star schema, snowflake schema, data marts)
• Extensive hands on experience writing complex SQL queries involving multi table joins.
• Bachelor’s degree in Computer Science or related field or equivalent combination of industry related professional experience and education

What We Look For
• Strong hands on experience in all layers of the full stack involving data, front end experience
• Expert level knowledge on Cloudera Hadoop and allied technologies.
• AWS Cloud Technologies, Unix, Master level SQL writing skills
• Cloud components including: EMR, Lambda, Glue, VPC Security groups, EC2, SNS, and S3
• Experience streaming datasets, Visualizations with Tableau, Sqoop, Data stage
• Excellent problem-solving skills, willingness to take ownership and risks, and enthusiasm in the face of technical challenges
• Active participation in the engineering community, up to date knowledge on new data technologies and best practices

What We Love About You
• You love our users. You deeply understand our users and put them at the center of everything you do. You aim to serve and delight them every day.
• You do the right thing. You are respectful and act with the highest integrity. If you see something that isn’t right, you say something.
• You debate it. You ask questions to understand a perspective and are comfortable respectfully challenging assumptions. You are not turned off by constructive conflict to get to the right answer.
• You own your outcomes. You set clear ambitious goals. You anticipate obstacles, persevere, and are accountable for your commitments.
• You make fast decisions. You are an effective and timely communicator. You understand how to collaborate, compromise, and escalate when needed.
• You get better every day. You welcome the gift of feedback. You never settle in your quest to grow and develop. By being here, you make our company stronger",ASCAP,"New York, NY",$45.3k–212k,Senior Data Engineer (contract)
191.5,"About Aaptiv

Aaptiv is a pioneer in the digital health space that helps its members live a healthier life by giving them access to the best workout experience anytime, anywhere. Through its use of humanized technology, Aaptiv removes the barriers many face when trying to make fitness a priority.

Launched in 2016, Aaptiv has garnered nearly a quarter million members and has raised over $55M from leading venture capital firms and top companies, including the Amazon Alexa Fund and Disney.

Want to join our team? We're looking for people who are passionate about continuing to improve the Aaptiv experience that our members around the world have come to love.

About the Role

At Aaptiv, we thrive on building data-driven solutions to help our members find workouts that best fit their lifestyles and allow them to pursue their wellness goals. We’re looking for a Principal Data Engineer/Data Engineering Manager to lead our Data Engineering team and help us continue to solve problems using... data. In this role, you will collaborate with cross functional teams, analysts and stakeholders, and your input will have a large impact on how our product is built.

As the Principal Data Engineer, you’ll manage a team of data engineers tasked with data integrations, data exploration and helping coworkers find actionable data. This is a hands on role and you’ll spend a large part of your time setting the technical direction and working in the code, whether that’s building new data pipelines, pairing with team members or reviewing pull requests to ensure high code quality. You will also work with your team to coordinate and execute online schema changes, debug production issues and perform root cause analysis should things not work as expected.

As part of this role, you will also spend time managing and coaching your team to help them grow as data engineers. In addition to this, you will also mentor other software engineers at Aaptiv in schema design and data best practices.

What You’ll Do
• Provide technical guidance in designing data warehouse architecture, schemas and data pipelines that accurately represent our business model, and store data efficiently to enable fast application access and easy reporting.
• Manage a team of data engineers. You’ll be responsible for providing your team members with constructive feedback and career guidance.
• Enforce data precision through data guidelines and supporting software engineers in writing software that accesses and stores data in a way that’s accurate and performant.
• Work closely with product managers, other data engineers and analysts to identify data gaps, brainstorm ideas, break down projects and estimate how long they will take.
• Lead a team that’s responsible for developing all data products, ensuring we are shipping only high quality code that is tested thoroughly and monitored appropriately.
• Be a champion of engineering best practices. We want to build high-quality software that’s easy to understand, easy to change and works the way it’s supposed to. You will need to work with your team, as well as with other Engineering Leads to cultivate these practices.

Who You Are
• You have 6+ years of experience building data solutions or software as an engineer, with at least a year or more in a more senior role.
• You’ve designed and ran data warehouses in a production environment previously.
• You have experience integrating non-standard data sources through development as needed.
• You’ve developed best practices around ensuring data integrity.
• You value a collaborative environment where you’ll work closely with the rest of engineering.
• You’ve leveraged toolkits to perform ETLs between data sources and data warehouses.
• You keep up to date with advances in data solutions and selectively choose new tools and approaches when appropriate to be more effective.
• Expert in data modeling.
• Strong Python programming skills
• Experience managing OLTP and OLAP data.
• Experience with both SQL and NoSQL datastores.
• Experience with Looker or Snowflake is a plus.
• Experience using AWS (RDS, EC2, Data Pipeline, Lambda) is a plus.

Not only will the work you do at Aaptiv be meaningful and rewarding, but you'll get to do it in a fun environment alongside a diverse group of friendly, talented people. In order to hire the best, we offer competitive salaries and equity, great benefits, and lots of perks, including catered breakfasts and lunches, unlimited vacation, and unbelievable views of New York City from our office at One World Trade Center.

It is the policy of Aaptiv to provide equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, Aaptiv will provide reasonable accommodations for qualified individuals with disabilities",Aaptiv,"New York, NY",$107k–169k,Principal Data Engineer / Data Engineering Manager
,"Our client builds innovative and compelling digital products that empower teachers and students across the country. They are looking for a Data Engineer to join their team in NY.

In this role, you’ll be:
• Building well-tested and optimized ETL data pipelines for both full and delta extraction.
• Collaborating with data scientists to store, aggregate, and calculate captured users’ work
• Contributing to leading industry data standards, such as Caliper Analytics or xAPI
• Improving our deployment and testing automation data pipelines

Requirements
• BS/MS in Computer Science, Data Science, or equivalent
• 2+ years of professional software development or data engineering experience
• Strong CS and data engineering fundamentals
• Proven fluency in SQL and a development language such as Python
• Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
• Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model... Star Schema, Data Vault)
• Strong communication skills in writing, conversation, and maybe silly gifs

Other desired skills (not required)
• Storage: AWS Storage Services (Redshift, Redshift Spectrum, S3, Glacier, DynamoDB), Parquet, Postgres
• ETL/BI: Matillion, Looker
• Experience with encryption at rest, including multiple approaches and tradeoffs",Dimensional Thinking,"Brooklyn, NY",,Data Engineer
164.1,"AWS Data Engineer - Remote - $145,000 - 165,000

I am currently looking for a AWS Data Engineer to work for a client of mine in the consulting industry. They provide resources, and cloud-based solutions. They offer casual dress, paid sick leave, life insurance, and flexible time off. Their team is growing and needing to expand more than ever. Join a wonderful team with a great culture.

Role & Responsibilities:
• Data migration from on prem to the AWS cloud
• Building repeatable solutions
• Moving BI from Oracle to SAP
• Integration of BI tools

Qualifications:
• AWS certifications a plus
• Experience pulling together databases and reports
• Hands-on proficiency in AWS & related cloud platforms
• Strong communication skills

If interested, please send me your updated resume at j.venditti@frgconsulting.com, or call me at 813-437-6915 (ext. 6915).

Jefferson Frank is the global leader in Architecture recruitment, advertising more Developer roles than any other agency. We deal with both... AWS Partners & End Users throughout North America. I have built relationships with most of the key employers in North America and have an unrivaled understanding of where the best opportunities are. I understand the need for discretion and would welcome the opportunity to speak to any candidates that are considering a new career or job either now or in the future. Confidentiality is of course guaranteed",Jefferson Frank,"New York, NY",$91.6k–145k,Data Engineer
,"Position:
We are looking for an accomplished big data developer with strong experience in the cloud AWS data implementation to help us build and integrate data-driven intelligent cloud solutions for EDL. This role will involve a close collaboration with our team of passionate and innovative big data specialists, application developers and product managers.

This is a unique opportunity to be a member of our corporate CRM and Analytics Team, tackling our toughest and most exiting data lake challenges across multiple divisions in Jefferies.

CRM & Analytics Team Overview:

The CRM & Analytics team is a highly strategic and cross-functional team responsible for leading the firmâ€™s global digitalization effort. This initiative, spanning all client-facing business units and corporate functions, will drive innovation and strategic change through technology, data science, and deep analytics. The team partners with key business leaders and industry experts to build transformational... technology to drive revenue, maximize efficiency, and optimize the allocation of resources. The CRM & Analytics team is at the forefront of Jefferiesâ€™ cloud initiative, leveraging best-in-class cloud-based technologies to replace legacy on-premises solutions to provide intelligent trend insights, actionable opportunities, decision support, and transparency into all client and business-related activities. This team is also responsible for Enterprise Data Lake.

Position Overview:
We are looking for an accomplished big data architect with strong experience in the cloud AWS data architecture and implementation.
This role will involve a close collaboration with our team of passionate and innovative big data specialists, application developers and product managers.

This is a unique opportunity to be a member of our corporate CRM and Analytics Team, tackling our toughest and most exiting data lake challenges across multiple divisions in Jefferies.

Enterprise Data Lake (EDL) Team Responsibilities:
The EDL team will oversee and support architecture and implementation of EDL for all Jefferies big data initiatives. It will drive the data governance and facilitate data onboarding. It will approve the design of data and software architecture, perform architecture review to pass EDL tollgates, evaluate and select cloud/AWS/Big Data tools for acceptance, and serve as a vendor liaison with data lake tool vendors and out internal infrastructure teams. Also, it will certify data for consumption, EDL patterns and processes, manage and govern data access controls, and will manage data lake and data governance training initiatives across enterprise.

The EDL team will become the center of excellence for the following EDL components and associated tools:
q EDL architecture and patterns
q EDL Data Stores
q EDL Governance
q EDL Data Discovery
q EDL Data Preparation
q EDL Reporting
q EDL Ingestion tools & other technologies and tools
q Educate teams to migrate and develop new cloud applications

Qualifications
Basic Requirements:
Â· A minimum of 5 years of hands-on technical experience with:

Â· big data implementation and technology offerings
Â· AWS/cloud big data modeling & data management
Â· analytics and ingestion architecture of big data
Â· data lake management and data architecture
Â· data lake design patterns & cloud best enterprise practices
Â· IoT and streaming, real time processing
Â· Big data related AWS technologies

• Experience in AWS technologies such as Kinesis, Lambda, EC2, Redshift, RDS, Cloud formation, EMR, AWS S3, AWS Analytics, Spark, Databricks
• Experience with at least one of the following languages Scala, Python, R and or Java
• Experience with designing, developing, and implementing complex integration for end-to-end solutions at a middleware and app level with focus on performance optimization
• Strong implementation skill in area of cloud development in AWS
• Demonstrated ability in implementing cloud scalable, real time and high-performance data lake solutions (AWS)
• Ability to quickly perform proof-of-concepts for validating new technology or approach
• Ability to exercise independent judgment and creative problem-solving techniques in a highly complex environment using leading-edge technology and/or integrating with diverse application systems
• Ability to lead and drive technology change in a fast-paced, dynamic environment and all phases of the entire software life cycle
• Strong experience with data catalog, data governance, Collibra, MDM and/or Data Quality (IDQ) toolset
• Strong experience with integration of diverse data sources (batch and real time) in the cloud
• Lead the design and sustainment of data pipelines and data storage
• Expertise in Structured, unstructured, SQL and No-SQL technologies
• Expertise with identifying and understanding source data systems and mapping source system attributes to the target
• Experience with design and automation of ETL\ELT processes
• AWS and cloud performance tuning and optimization experience
• Experience with effort estimation for new projects/proposals on an ongoing basis.
• Excellent communication skills across all levels; ability to communicate with ease the complex and technical concepts.
• Ability to work effectively in a fast-paced environment",Jefferies,"New York, NY",,EDL Big Data Engineer - Corporate- Information Technology (No Agencies)
210.0,"At Lyft, community is what we are and it’s what we do. It’s what makes us different. To create the best ride for all, we start in our own community by creating an open, inclusive, and diverse organization where all team members are recognized for what they bring.

Here at Lyft, Data is the only way we make decisions. It is the core of our business, helping us create an exceptional transportation experience for our customers and providing insights into the effectiveness of our product launch & features.

As a Data Engineer at Lyft, you will be a part of an early stage team that builds the data transport, collection, and storage, and exposes services that make data a first-class citizen at Lyft. We are looking for a Data Engineer that is passionate and motivated to make an impact in creating a robust and scalable data platform. In this role, you will have ownership of the company’s core data pipeline that powers Lyft’s top line metrics; You will also leverage data expertise to help... evolve data models in various components of the data stack; You will be working on architecting, building, and launching highly scalable and reliable data pipelines to support Lyft’s growing data processing and analytics needs. Your efforts will allow access to business and user behavior insights, leveraging huge amounts of Lyft data to fuel several teams such as Analytics, Data Science, Marketplace and many others.

Responsibilities
• Owner of the core company data pipeline, responsible for scaling up data processing flow to meet the rapid data growth at Lyft
• Consistently evolve data model & data schema based on business and engineering needs
• Implement systems tracking data quality and consistency
• Develop tools supporting self-service data pipeline management (ETL)
• SQL and MapReduce job tuning to improve data processing performance

Experience & Skills
• Extensive experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
• Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
• Good understanding of SQL Engine and able to conduct advanced performance tuning
• Strong skills in scripting language (Python, Ruby, Perl, Bash)
• Experience with workflow management tools (Airflow, Oozie, Azkaban, UC4)
• Comfortable working directly with data analytics to bridge business requirements with data engineering
Lyft is an Equal Employment Opportunity employer that proudly pursues and hires a diverse workforce. Lyft does not make hiring or employment decisions on the basis of race, color, religion or religious belief, ethnic or national origin, nationality, sex, gender, gender identity, sexual orientation, disability, age, military or veteran status, or any other basis protected by applicable local, state, or federal laws or prohibited by Company policy. Lyft also strives for a healthy and safe workplace and strictly prohibits harassment of any kind. Pursuant to the San Francisco Fair Chance Ordinance and other similar state laws and local ordinances, and its internal policy, Lyft will also consider for employment qualified applicants with arrest and conviction records",Lyft,"New York, NY",$135k–150k,"Software Engineer, Data Engineering - New York"
164.1,"About Foursquare:

Since our inception in 2009, Foursquare has been a leading force in changing how location information enriches our real-world and digital lives. As a location intelligence company, Foursquare is comprised of two well-known consumer apps, Foursquare and Swarm, as well as thriving media and enterprise products. Our B2B offerings include Places (for developers), Pinpoint and Attribution (for marketers), and Place Insights (for analysts, based on the world's largest foot traffic panel). With more than 200 people across our offices in New York, San Francisco, and in sales offices around the globe, we’re dedicated to our trailblazing mission—enriching consumer experiences and informing business decisions with location intelligence.

About our Engineering Team:

As a member of Foursquare’s engineering team, we want you to bring experience building real products from the ground up. We're passionate about tackling tough challenges in the location space and look for others... who like to dive deep into code and help solve hard problems. You should be comfortable running with your own ideas and eager to learn new skills on a bleeding edge platform. We use a variety of tools, technologies, and languages to build software (Scala, Thrift, MongoDB, Memcached, JS/jQuery, Kafka, Pants, Hadoop, MR, Spark) but experience with equivalent ones will do just fine.

Join us and help bring our ideas (and your own!) off the whiteboard and into reality. You'll be a key member of our Attribution team, building a system that builds hundreds of machine learning models per day at scale to drive marketing decisions for many well-known companies. You'll build resilient services and tooling which drive all of our processing of petabytes of data
Responsibilities:
• Develop and maintain our data pipelines using Hadoop, Scalding, Luigi, Spark, Mongo and more
• Partner with the Data Science team to investigate and implement advanced statistical models and machine learning pipelines
• Identify and implement performance improvements across all pipelines
• Data investigations to validate assumptions or find the source of a problem
• Assist client support and sales with client integrations

Qualifications:
• 3+ years of experience working with Hadoop MapReduce and/or other big data technologies and pipelines
• You have a solid foundation in computer science fundamentals with particular expertise in data structures, algorithms, and design
• You obsess over data: everything needs to be accounted for and be thoroughly tested
• You are constantly thinking of ways to squeeze better performance out of the pipelines
• Strong Java or other object-oriented programming experience or, even better, experience and/or interest in functional languages (we use Scala!)
• Experience with Scala, Scalding, Luigi,Hive, machine learning pipelines and model training is a plus
• Bachelors Degree or higher in Computer Science, Electrical Engineering or related field

Foursquare is proud to foster an inclusive environment that is free from discrimination. We strongly believe in order to build the best products, we need a diversity of perspectives and backgrounds. This leads to a more delightful experience for our users and team members. We value listening to every voice and we encourage everyone to come be a part of building a company and products we love.

Foursquare is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other characteristic protected by law",Foursquare,"New York, NY",$91.6k–145k,Data Engineer
,"Job Description

AWS Big Data Engineer - $145,000 + EQUITY - NYC - START UP

Are you a go-getter who takes initiative and thrives in a small team? Do you code with SQL and Python?

A rapidly growing start-up in NYC with backing from a private equity is looking to bring on an AWS Big Data Engineer! This is an exciting opportunity for someone who is dedicated to their job with a positive attitude! They use the AWS stack and you will be responsible for data integration and modelling for analytics and business intelligence purposes. The warehouse is based in Redshift and experience managing implementation and integration of data pipelines is a must for this role.

Responsibilities:
• Build dashboards using Tableau
• ETL and data pipeline maintenance, database management, data wrangling
• Develop data frameworks and models to drive business improvements

Requirements:
• Expert-level fluency in Python and SQL
• Experience working with AWS Redshift and Big Data
• Ability to manage a variety... of data problems quickly
• Well organized and an excellent communicator
• *The client is not willing to provide sponsorship at this time**

If you or someone you know is interested in this position, please send your resume directly to P.Sandstrom@jeffersonfrank.com or call 212-731-8282. Ask for Paige! My client is looking to start the interview process as soon as possible.

Jefferson Frank is the Amazon Web Services (AWS) recruiter of choice. We work with organizations worldwide to find and deliver the best AWS professionals on the planet. Backed by private equity firm TPG Growth, we have a proven track record servicing the AWS permanent and contract recruitment market and, to date, have worked with over 30,000 organizations globally from our offices in North America, Europe, and Asia-Pacific.

At Jefferson Frank, our mission is simple: we want happy customers. Whether you're an AWS professional walking into your dream AWS job, or an organization hiring an incredible contractor for your cloud migration project, our goal is to deliver an unrivalled customer experience. Work with us and you'll get the personalized experience you deserve - one you'll simply not find at any other recruitment agency. At Jefferson Frank, we find great people great jobs in AWS.

I understand the need for discretion and would welcome the opportunity to speak to any Big Data and cloud analytics candidates that are considering a new career or job either now or in the future. Confidentiality is of the upmost importance. For more information on available AWS Big Data Jobs as well as the cloud market, I can be contacted at 212-731-8282. Please see www.jeffersonfrank.com for more information",Jefferson Frank,"New York, NY",,"AWS Big Data Engineer - 140,000 + Equity - NYC - Start Up"
202.0,"Movable Ink powers meaningful experiences in email and on the web for the biggest brands in the world. Data is at the heart of these experiences - we are collecting many terabytes of data each quarter, and all of it must be partitioned and aggregated for many different use cases.
The Principal Data Engineer will be responsible for all data access patterns across the business. Data Scientists will want access to the billions of events tracked across our customers’ web sites each day. Data Analysts will want connect that usage back to configuration data in our relational database. The product itself will need to aggregate this constant flood of data in real time.
Fast-forward one year. Here’s what you will have accomplished:
Supported data initiatives in three different products using a combination of stream processing, messaging queues, and batch ETL
Become an expert in our existing storage technologies and our use cases to suggest and implement enhancements
Enabled the Data Science... team by providing them with the tools and the dataset they need to be effective
Connected product data to business data for ad-hoc analysis with BI tools
Performed a cost analysis for moving from a unified data storage approach to regional isolation
Partnered with Information Security to define and implement recommend procedures for data storage and access
Experience:
You’ve done a lot of work with Big Data tools, such as Spark, Storm, Hive, Hadoop, etc
You’ve implemented storage mechanisms for high-throughput workloads
You’re comfortable in AWS and have run production systems there",Movable Ink,"New York, NY",$114k–176k,Principal Data Engineer
,"About this job

Location options: Remote

Technologies

mysql, data-warehouse, etl, bigdata, infrastructure

Job description

Who are we looking for:

We are looking for a savvy Data Engineer to join our growing tech team.

You will support our software developers, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects.

The right candidate will be excited by the prospect of optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives.

Roles and Responsibilities:
• you will build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and other big data technologies
• you will design and model data structures to help analyzing our business and technical data
• you will support existing processes running in production
• you will work together with... people from other key areas to assist with data-related technical issues and support their data infrastructure needs

Skills & Requirements
• knowledge in relevant engineering best practices, data management fundamentals, data storage principles, and be current with recent advances in distributed systems as it pertains to data storage and computing
• 2+ years of experience in designing, building and maintaining data architecture(s) and infrastructure(s), both relational and non-relational
• 2+ years of maintaining data warehouse systems and working on large scale data transformation using SQL, Hadoop, Hive, or other Big Data technologies; experience with ETL tools is a plus
• 2+ years of data modeling experience, and able to use data models to improve the performance of software services
• experience with Cloud Based Solution (AWS Redshift, GCP Big Query) and programming language (Python, Java) is a plus
• experience communicating with colleagues from engineering, analytics, and business backgrounds
• degree in Engineering, Math, Statistics, Computer Science, or related discipline or equivalent experience is a plus.
• be able to legally work in Europe (you are the holder of a EU Passport or you are the holder of EU residency permit or you are the holder of a Schengen Work Visa",Supermercato24,"New York, NY (+14 others)",,Data Engineer
167.2,"AlphaSense provides a revolutionary search engine for knowledge professionals.
Our mission is to curate the world's investment & market research content, including the vast high-value content sets that traditional web search engines cannot reach. Our users can rapidly search & discover key data points & track impactful new information with intelligent, automated alerts.
Our 800+ clients include many of the world's largest investment & advisory firms, global banks & corporations - helping our clients become dramatically more productive & gain an information edge by discovering critical data points & trends that others miss.
We are seeking a candidate with a passion for data that can learn fast and apply the latest innovations in deep learning and NLP to the world’s most effective business insights platform.
The Role:
You will join our team of machine learning engineers developing the cutting edge AI & NLP systems that power AlphaSense Search. You will leverage Python to develop... scalable, performant ML pipelines.
Requirements:
A BS/MS degree in Computer Science or Computer Engineering is highly desired, but equivalent experience may be considered as well
2+ years developing data pipelines with Python with additional experience in Java, Linux, and scripting languages that interact with cloud resources
Demonstrated experience developing end-to-end NLP models to derive insights from text data using NLP libraries in Python
Experience with building back-end services and APIs in Django or Flask
Good grasp of data toolchains and best practices (such as Beam, Dataflow, Airflow, Spark, Kafka)
Experience with docker/kubernetes
Experience using SQL (relational), NOSQL and search databases (SOLR/Lucene, MySQL, Mongo/Cassandra, SOLR/Lucene, etc.)
Experience working with cloud computing (preferably AWS or GCP)
Experience with iterative Agile methodology and use of tools like JIRA, Confluence, Git
Familiarity with Deep Learning frameworks (i.e. not specifically write code for building NN models but being able to understand and review the existing code)
Demonstrated experience in the software development lifecycle, from requirements to design to development and testing
Strong communication skills and ability to build pipelines with little guidance in small teams and independently
Excellent organizational, problem-solving, debugging and analytical skills",AlphaSense,"New York, NY",$97.2k–140k,Data Engineer - AI Research
,"• APPLY HERE: https://hire.withgoogle.com/public/jobs/aaptivcom/view/P_AAAAAADAAAzICyJYr56dmp*

ABOUT AAPTIV

Aaptiv is a pioneer in the digital health space that helps its members live a healthier life by giving them access to the best workout experience anytime, anywhere. Through its use of humanized technology, Aaptiv removes the barriers many face when trying to make fitness a priority.

Launched in 2016, Aaptiv has garnered nearly a quarter million members and has raised over $55M from leading venture capital firms and top companies, including the Amazon Alexa Fund and Disney.

Want to join our team? We're looking for people who are passionate about continuing to improve the Aaptiv experience that our members around the world have come to love.

ABOUT THE ROLE

At Aaptiv, we thrive on building data-driven solutions to help our members find workouts that best fit their lifestyles and allow them to pursue their wellness goals. We’re looking for a Principal Data Engineer/Data... Engineering Manager to lead our Data Engineering team and help us continue to solve problems using data. In this role, you will collaborate with cross functional teams, analysts and stakeholders, and your input will have a large impact on how our product is built.

As the Principal Data Engineer, you’ll manage a team of data engineers tasked with data integrations, data exploration and helping coworkers find actionable data. This is a player-coach role and you will be expected to be very hands on. You will spend a large part of your time working in the code, whether that’s building new data pipelines, pairing with team members or reviewing pull requests to ensure high code quality. You will also coordinate and execute online schema changes, debug production issues and perform root cause analysis should things not work as expected.

As part of this role, you will also spend time managing and coaching your team to help them grow as data engineers. In addition to this, you will also mentor other software engineers at Aaptiv in schema design and data best practices.

WHAT YOU'LL DO

•Provide technical guidance in designing data warehouse architecture, schemas and data pipelines that accurately represent our business model, and store data efficiently to enable fast application access and easy reporting.

•Manage a team of data engineers. You’ll be responsible for providing your team members with constructive feedback and career guidance.

•Enforce data precision by implementing data guidelines and supporting software engineers in creating software that accesses and stores data in a way that’s accurate and performant.

•Coordinate with software engineers to deploy online schema changes with minimal customer impact.

•Work closely with product managers, other data engineers and analysts to identify data gaps, brainstorm ideas, break down projects and estimate how long they will take.

•Be part of a team that’s responsible for developing all data products, ensuring we are shipping only high quality code that is tested thoroughly and monitored appropriately.

•Be a champion of engineering best practices within your team. We want to build high-quality software that’s easy to understand, easy to change and works the way it’s supposed to.

WHO YOU ARE

•6+ years of experience building data solutions or software as an engineer

•You’ve administered data warehouses in a production environment

•You’ve previously held a more senior engineering position for longer than a year

•Expert in SQL and data modeling

•Expert in data warehouse performance analysis and optimizations

•Strong Python programming skills

•Experience in creating custom data integration tools to extract/load/transform data

•Experience in building data products with unit and integration tests

•Experience with both SQL and NoSQL datastores (We use PostgreSQL and MongoDB)

•Experience using AWS (RDS, EC2, Data Pipeline, Lambda) is a plus

•You keep up to date with advances in data solutions and selectively choose new tools and approaches when appropriate to be more effective.

Not only will the work you do at Aaptiv be meaningful and rewarding, but you'll get to do it in a fun environment alongside a diverse group of friendly, talented people. In order to hire the best, we offer competitive salaries and equity, great benefits, and lots of perks, including catered breakfasts and lunches, unlimited vacation, and unbelievable views of New York City from our office at One World Trade Center.

It is the policy of Aaptiv to provide equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, Aaptiv will provide reasonable accommodations for qualified individuals with disabilities.
• APPLY HERE: https://hire.withgoogle.com/public/jobs/aaptivcom/view/P_AAAAAADAAAzICyJYr56dmp",Aaptiv,"New York, NY",,Principal Data Engineer / Data Engineering Manager
216.0,"As a member of our Data Engineering team, you’ll sit with a cross-functional team of software engineers, data scientists and business analysts, working closely with your team to turn complex sources into understandable data. The data sets, pipelines and tools that you build will be involved in pivotal, company-level decisions for years to come. Not only that, but you’ll be working on a team where constant learning and team-wide knowledge sharing is a core part of our culture.
RESPONSIBILITIES
Build and maintain data processing services
Write, test, and review primarily microbatch or streaming ETL
Continuous improvement of our system, tests, and data quality indicators
Influence our technical decisions
Keep yourself informed and up-to-date with technologies
Encourage the technical growth of your teammates
QUALIFICATIONS
You love working directly with the people whose problems you're solving
You are deeply skilled at data modeling, storage, security, and retrieval
You're motivated and... driven. You naturally take responsibility for a project and ask for help when needed
You enjoy working with dynamic programming languages, relational databases, and distributed systems. Our platform is ever evolving, but currently is a combination of Python, Java, Postgres, Kubernetes, Spark, Presto, Kafka, and Mongo
You gain a deep understanding of the products and tools you work with
You check your work and stand behind what you’ve built

ABOUT SQUARESPACE
Squarespace empowers people with creative ideas to succeed. By blending elegant design and sophisticated engineering, we empower millions of people — from individuals and local artists to entrepreneurs shaping the world’s most iconic businesses — to share their stories with the world. Squarespace’s team of more than 800 is headquartered in downtown New York City, with offices in Dublin and Portland. For more information, visit www.squarespace.com/about.

PERKS
Health insurance with 100% premium covered
Flexible vacation & paid time off
Equity plan
401(k) plan with employer match
Free lunch and snacks
Dog-friendly workplace
Today, more than a million people around the globe use Squarespace to share different perspectives and experiences with the world. Not only do we embrace and celebrate the diversity of our customer base, but we also strive for the same in our employees. At Squarespace, we are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace",Squarespace,"New York, NY",$140k–152k,Senior Software Engineer - Data Engineering
,"Big Data Engineer - AWS Big Data - Hoboken, NJ - 6 Month Contract

Excellent new contract opportunity in New York City for a Big Data Engineer!

My client is a partner providing analytic services to multiple end users in New York City. Big Data Engineers develop, maintain, test and evaluate big data solutions within organizations. This position will implement and optimize the client's data and data pipeline architecture. They will also optimize their data flow and collection for cross-functional teams.

Responsibilities:
• Big Data pre-processing
• Reporting workflows with large data sets to turn the data into usable insights
• Testing models on Big Data
• Deploying learned models for both prediction and scoring

Preferred Qualifications:
• Hands-on experience in AWS Big Data technologies and understanding of concepts such as Redshift, Hive, MapReduce, Spark, Spark streaming and NoSQL databases
• Knowledge in one or more of Java, Scala, Python and Bash
• Ability to work in a... fast-paced and deadline driven environment
• Strong decision-making skills in terms of data analysis and the ability to architect Big Data

Opportunity Type: 6 Month Contract

Rate: 80-90/hr

If you or someone you know is interested in this position, please send your resume directly to P.Sandstrom@jeffersonfrank.com or call 212-731-8282 (ext 3205) and ask for Paige.

Jefferson Frank is the Amazon Web Services (AWS) recruiter of choice. We work with organizations worldwide to find and deliver the best AWS professionals on the planet. Backed by private equity firm TPG Growth, we have a proven track record servicing the AWS permanent and contract recruitment market and, to date, have worked with over 30,000 organizations globally from our offices in North America, Europe, and Asia-Pacific.

US Citizens and all parties authorized to work in the US are encouraged to apply",Jefferson Frank,"Hoboken, NJ",,AWS BIG DATA ENGINEER - 6 MO CONTRACT - HOBOKEN NJ - 80 - 90/HR
176.5,"About The Job

We focus on the software engineering related to data replication, storage, centralized computation, and data API’s. We provide customers and partners with data tools, shared frameworks, and data services. These are the foundational core of our group which enables ourselves and others to work with data from a common underpinning. Our tools and services enable our group to scale and avoid blocking others. We reduce data redundancy by creating systems and datasets that serve as sources of record. We enable discovery and governance of our data. We support key business goals like growing our digital subscriber base, understanding how our customers use our products, and retaining our print subscribers.

As a Data Engineer, You Will
• Run and support a production enterprise data platform
• Design and develop data models
• Work with languages like Java, Python, Go, Bash, and SQL
• Build batch and streaming data pipelines with tools such as Spark, Airflow, and cloud-based data... services like Google’s BigQuery, Dataproc, and Pub/Sub
• Develop processes for automating, testing, and deploying your work

About You

To thrive in this role, you are excited about data and motivated to learn new technologies. You are comfortable collaborating with engineers from other teams, product owners, business teams, and data analysts and data scientists. You are own and shape your technical domain area and move the related business goals forward. You are eager to resolve upstream data issues at the source instead of applying workarounds. You analyze and test changes to our data architectures and processes, and determine what the possible downstream effects and potential impacts to data consumers will be.

Benefits And Perks
• Make an impact by supporting our original, independent and deeply reported journalism.
• We provide competitive health, dental, vision and life insurance for employees and their families
• We support responsible retirement planning with a generous 401(k) company match.
• We offer a generous parental-leave policy, which we recently expanded in response to employee feedback. Birth mothers receive 16 weeks fully paid; non gestational parents receive 10 weeks, also fully paid.
• We are committed to career development, supported by a formal mentoring program and $8,000 annual tuition reimbursement.
• We have frequent panel discussions and talks by a wide variety of news makers and industry leaders.
• Join a community committed to the richness of diversity, experiences and talents in the world we cover, supported by a variety of employee resource groups.

The New York Times is committed to a diverse and inclusive workforce, one that reflects the varied global community we serve. Our journalism and the products we build in the service of that journalism greatly benefit from a range of perspectives, which can only come from diversity of all types, across our ranks, at all levels of the organization. Achieving true diversity and inclusion is the right thing to do. It is also the smart thing for our business. So we strongly encourage women, veterans, people with disabilities, people of color and gender nonconforming candidates to apply.
The New York Times Company is an Equal Opportunity Employer and does not discriminate on the basis of an individual's sex, age, race, color, creed, national origin, alienage, religion, marital status, pregnancy, sexual orientation or affectional preference, gender identity and expression, disability, genetic trait or predisposition, carrier status, citizenship, veteran or military status and other personal characteristics protected by law. All applications will receive consideration for employment without regard to legally protected characteristics",The New York Times,"New York, NY",$106k–141k,Data Engineer
202.0,"About 605

At 605 we are engineers, analysts, data scientists, media experts, marketing strategists and political operatives. Our team of data scientists pioneered the field of TV data analytics. We offer unique, independent audience measurement and analytics to build better marketing and programming initiatives within the media and entertainment industries.

Responsibilities Include

The Lead Data Engineer at 605 must have a broad and deep data skillset as well as people management experience. In addition to being a hands on contributor, this role is responsible for setting the technical direction of the team. Along with being a strong technical expert, this role requires a natural leader adept at earning the trust and respect of the team.
• Develop people through coaching, mentoring, and management support
• Ensure your team members are happy by providing them with career development guidance as it relates to both hard skills as well as the soft ones. Help them exploit their... strengths, handle their weaknesses and make sure they are properly equipped to perform their job the best they can
• Hold regular one on ones, conduct performance evaluations and manage the hiring process as well as other administrative tasks for your team.
• Create a culture on your team where anyone can speak up and share his or her concerns when it’s in the interest of the mission
• You and your direct reports hold each other accountable for the commitments you both make
• Grow the technical expertise of your team on performance, scalability and maintainable architecture
• Participate in technology architecture discussions for product development. Translate the business requirements into strategy and make sure it is well transferred to all team members
• Advocate for software best practices within your team as well as across engineering
• Work closely with dev teams, product owners, and stakeholders to ensure we deliver innovative products to our users
• Make technical decisions based on business needs without sacrificing product quality, integrity and security.
• Work on unique and interesting data challenges around architecting, building and managing pipelines that securely process hundreds of terabytes of data.
• Work closely with analysts and statisticians to ensure the validity of our processes.

Requirements

Our engineers are expected to wear a number of hats and have the opportunity to touch all parts of the stack. Our stack includes Apache Spark, Scala, Redshift and an ever-growing list of many other cool technologies.

Who you are
• You care about developing people as a team leader
• Experience wrangling terabytes of big, complicated, imperfect data
• Practical experience with Apache Spark
• Experience with AWS products (Redshift, EMR, S3, IAM, RDS, etc)
• Basic understanding of statistics and experience with statistical packages such as R, Matlab, SPSS, etc.
• You have a deep understanding of scalable systems and you have large-scale engineering experience in an Agile development environment
• Bachelor's degree in Computer Science or a related field (or 4 additional years of relevant work experience)
• At least 5-7 years in the data processing and analytics industry
• A strong understanding of data structures, algorithms, and effective software design
Significant development experience with a major modern language (e.g. Java, Scala, Python, Ruby, C/C++, etc.)
• Significant experience working with structured and unstructured data at scale and comfort with a variety of different stores (key-value, document, columnar, etc.) as well as traditional RDBMSes and data warehouses
• Experience with or interest in AWS Glue, Redshift Spectrum and any other tools that enable data querying at scale
• Experience writing unit and functional tests
• Comfort with version control systems (e.g. Git, SVN)
• Excellent verbal and written communication skills; must work well in an agile, collaborative team environment

Preferred Qualifications
• Master's in Computer Science or a related field
• Practical experience with supervised machine learning techniques
• Strong background with test-driven development

Benefits

Important and Standard
• Comprehensive health, dental and vision insurance for employees and their families
• Life & Disability insurance
• 401k plan with match, eligible for match after one year
• Pre-tax flexible compensation plan for medical, transit, parking or dependent care expenses
• PTO & Sick days—if you’re sick, you stay home
• Work-from-home Fridays

Other Cool Benefits
• A kitchen stocked with sodas, snacks, yogurt and other goodies
• A tight knit start up community who likes to eat! We celebrate everyone’s birthdays, have frequent team lunches, and do events in and out of the office
• 605 is an active participant in conferences
EEO STATEMENT

At 605, we’re just as passionate about diversity as we are about pioneering the field of TV data analytics. We are committed to cultivating an environment of mutual respect and equal opportunity. All hiring and advancement decisions are made on the basis of qualification, merit, and business need",605,"New York, NY",$114k–176k,Lead Data Engineer
202.0,"We are a former real estate startup, acquired by CBRE in 2017. We are uniquely positioned within the organization to access global data, tools, resources, and leaders throughout the industry. We have the stability of secure backing and the freedom to explore and experiment to develop leading-edge products. For more information about our team, please visit http://nyc.cbrebuild.com

We are looking for the first data scientist/engineer/analyst to help us understand and leverage all the data sources and products we have. This person will have a great deal of autonomy and will be expected to come to the table with their ideas and suggestions for making our products better with data.

Responsibilities:
• Analyzing proprietary commercial real estate data on availabilities, lease rates, and market statistics to identify trends and opportunities
• Refining predictive models for space estimation and time-forecasting for capacity planning
• Empowering user research and improving customer... engagement through development and analysis of user metrics
• Architecting data models and developing ETL functions to move data between RDMBS and NoSQL stores
• Leveraging machine learning and data mining to take multiple streams of data and create novel customer experiences
• Automating manual processes to generate data like lease availabilities and floor plan layouts
• Working closely with our heads of Product and Engineering as well as team leads across the company to address a wide variety of business problems
• Identifying new approaches and tackling ambiguous problems with creativity and improvisation

Requirements:
• Smart, productive and tolerant of sarcasm
• Self-motivated and curious, crafty and driven
• Interested in building a friendly, collaborative, and transparent work environment
• Programming experience in Python, Julia, Torch (Lua), or other programming languages/frameworks
• Bachelor's degree in a related field or equivalent work experience
• Minimum of 3 - 5 years of relevant experience requiredWhy work at CBRE Build?
• Great team - Diverse. Smart. Nice. We like to always keep learning.
• High impact - We’re a small team with a global reach in a trillion dollar industry.
• Growth and development - We invest in employee development. Time to explore. Machine shop with a laser cutter and 3d printer. Funding for education, conferences, etc.
• Benefits - Unlimited vacation. Flexible work hours. Work life balance. Health, vision, dental for employees and their dependents. 401k plan with a 3% match",CBRE,"New York, NY",$114k–176k,Sr. Data Engineer
164.1,"Who We Are

Braze (formerly Appboy) is a customer engagement platform that delivers messaging experiences across push, email, apps, and more. Braze is built specifically for today’s mobile-first world and tomorrow’s ambient computing future. Braze is set apart as the platform that allows for real-time and continuous data streaming, replacing decades-old databases that aren’t built for today’s on-demand, always-connected customer. With data, technology, and teams working together in unison, the Braze platform makes marketing more authentic, brands more human, and customers more satisfied with every experience.

Each month, tens of billions of messages associated with over 1.5 billion active users are managed through our technology. Braze is a venture-backed company with hundreds of employees in offices located in New York City, San Francisco, London, and Singapore. Most recently, we’ve been named a Leader in the Forrester Wave™: Mobile Engagement Automation, Q3 2017 evaluation. We’ve... been recognized by Forbes Cloud 100 at #85, ranked #225 on Inc.'s 500 Fastest Growing Private Companies, named a “Top 10 Upstart” by Business Insider, in addition to being #21 in the Deloitte Technology Fast 500 List. Learn more at Braze.com.

WHAT WE'RE LOOKING FOR

You are an engineer who loves working with data. The more data there is, the more excited you get. Data sources? The more, the better. Messy data doesn’t scare you. You want to learn and implement cutting edge technology that helps move, store, and analyze large and complex data sets. You are a data nerd. Or is it a data geek?

WHAT YOU'LL DO
• Design, build, and manage efficient, fault-tolerant, and high-volume data pipelines
• Build and maintain ETLs
• Manage databases and tables
• Design, build, and test solutions that touch every part of the data pipeline
• Work with engineering to collect new data
• Expose new data for data analysts and downstream users to use
• Enable more advanced analytics for data analysts and other stakeholders
• Collaborate with other teams at Braze and enable them with data

WHO YOU ARE
• 2+ years of data-focused, technical experience working with large and complex data sets
• Experience with creating, monitoring, optimizing, improving, and scaling databases
• Experience writing ETL/ELT
• Working knowledge of Python and/or Ruby
• Working knowledge of AWS and/or Azure and/or Google Cloud
• Security and privacy minded
• Fast learner, self-starter, go-getter, intellectually curious, all that jazz
• Bonus Points:
• Familiarity with analytics technologies (i.e. Looker) a plus
• Familiarity with non-relational databases (i.e. MongoDB) a plus
• Familiarity with Snowflake a plus
• Familiarity with Pandas a plus
• Familiarity with Kafka is a plus

WHAT WE OFFER

Tech startup vibe including free daily lunches, snacks, and group events. Inclusive and diverse culture. Complete support from your teammates across all departments and a real “get it done” attitude for our customers. An opportunity to join a market leading company and have an impact.
• Excellent medical insurance and life insurance coverage for you and your dependents
• Matching 401K
• Tuition reimbursement program
• Daily lunches, snacks, and beverages
• Collaborative, transparent, collegial, and fun loving office culture
• Flexible time off policy to balance your work and life in the way that suits you best

In addition, this position is exempt under the provisions of the Fair Labor Standards Act",Braze,"New York, NY",$91.6k–145k,Data Engineer
164.1,"A pioneer in K–12 education since 2000, Amplify is leading the way in next-generation curriculum and assessment. Our captivating core and supplemental programs in ELA, math, and science engage all students in rigorous learning and inspire them to think deeply, creatively, and for themselves. Our formative assessment products turn data into practical instructional support to help all students build a strong foundation in early reading and math. All of our programs provide teachers with powerful tools that help them understand and respond to the needs of every student. Today, Amplify serves more than three million students in all 50 states. For more information, visit amplify.com.

As an engineer at Amplify, you will join a talented team tackling the toughest problems in education with the best ideas in technology – including user experience, APIs and services, data analysis, and deployment pipelines. You’ll play an active role in imagining and improving product design and the classroom... experience.

We hire engineers “for the slope, not the intercept” – we’re looking for intellectual ability, flexibility and ability to learn, and commitment to work together in tight-knit teams.

What You’ll Do

Our data team builds, augments, and maintains the infrastructure that empowers teams across Amplify and our customers to make sense of and tell stories with their data. We believe strongly in teaching our teammates to serve themselves, within a safe, reliable, and agile environment. You’ll be building data systems, but also the sharing-and-learning culture so that every team uses these tools to improve their own lives, and those of our students and teachers.
• Impress the toughest customers around – seventh graders – by:
• helping teams create fun, compelling apps by leveraging millions of data points
• Make life better for passionate, overworked teachers by:
• helping teachers understand their students by building reusable data pipelines
• Make life better for passionate, overworked Marketing and Sales teams by:
• using REST APIs for sourcing/sending data to SAAS like Salesforce, Hubspot
• Help school administrators build great schools by:
• respecting privacy and ensuring security while offering useful insights by making smart choices in tech stack, database design, and encryption
• helping school principals understand how teachers are teaching and how students are learning by architecting data warehouse schemas and SQL transforms with just the right CTEs, window functions, and pivots
• analyzing performance and squashing tricky bugs using tools like AWS Redshift, Matillion, Python, SQL, AWS CloudWatch, AWS SNS
• Learn every day by:
• immersing oneself in agile rituals and leveraging our infrastructure
• leading collaboration, pull request-ing, and mentoring on a cross-functional team
• participating in cross-team share-outs, brownbags, and workshop series
• becoming an expert in the data models and standards within Amplify and the educational industry in order to deliver quality and consistent solutions

Example Projects You Might Work On
• Building well-tested and optimized ETL data pipelines for both full and delta extraction
• Collaborating with data scientists to store, aggregate, and calculate captured students’ work
• Contributing to leading industry data standards, such as Caliper Analytics or xAPI
• Improving our deployment and testing automation data pipelines

You Must Have
• BS/MS in Computer Science, Data Science, or equivalent
• 2+ years of professional software development or data engineering experience
• Strong CS and data engineering fundamentals
• Proven fluency in SQL and a development language such as Python
• Understanding of ETL/ELT pipelines and Data Warehousing design, tooling, and support
• Understanding of different data formatting (JSON, CSV, XML) and data storage techniques (3NF, EAV Model, Star Schema, Data Vault)
• Strong communication skills in writing, conversation, and maybe silly gifs

Extra Credit For
• Experience with tools we use every day:
• * Storage: AWS Storage Services (Redshift, Redshift Spectrum, S3, Glacier, DynamoDB), Parquet, Postgres
• ETL/BI: Matillion, Looker
• Experience with tools we don’t use, but should, and the wisdom to know when to recommend them
• Proven passion and talent for teaching fellow engineers and non-engineers
• Proven passion for building and learning: open source contributions, pet projects, self-education, Stack Overflow
• Experience in education or ed-tech

Amplify is an Equal Opportunity Employer of Minorities, Females, Protected Veterans and Individuals with Disabilities.

This position may be funded, in whole or in part, through American Recovery & Reinvestment Act funds.

Amplify Education, Inc. is an E-Verify participant",Amplify,"Brooklyn, NY",$91.6k–145k,Data Engineer
165.4,"Forbes is currently seeking an enthusiastic and passionate Senior Data Science Engineer to join our Digital team. This role requires more than an academic approach to Data Science. It requires the next step of conceptualizing and creating data products. In addition to joining a high performing team, you would be joining a passionate and engaged culture that celebrates:

Entrepreneurship: Freedom to innovate, identify new opportunities and take on responsibilities

Career Development: Formal mentorship program, industry conference attendance and speaking opportunities

Work/life Balance: Regular group outings, celebration of passion projects and side hustles, optional mindfulness sessions twice a month

Positivity: Emphasis on positive communication, transparency and team building

If you're looking for challenges and opportunities similar to those of a startup, with the benefits of a seasoned, successful company, read on.

What you will be doing:
• Participate in agile development of... unique and innovative products
• Develop predictive models using multiple statistical techniques
• Participate in development and architecture of complex systems
• Establish best practices for data collection and analysis
• Work closely with the product owners and stakeholders to discover, design, architect and prototype new features
• Write high quality code that adheres to coding standards
• Constantly learn new technologies, approaches and languages
• Work with cross functional teams including Dev, Product, DevOps and QA
• Participate in code reviews
• Advocate Unit and Integration Testing
• Create maintainable projects with documentation

REQUIREMENTS
• Strong conceptual knowledge and background in probability, statistics, and machine learning
• Strong knowledge of Python - 4+ years
• Proficiency in data science and analytical modeling - 4+ years
• Experience with NoSQL databases. MongoDB is a plus
• Experience with real-time and streaming data processing
• Experience with queuing platforms like Kafka
• Knowledge of BigQuery
• Familiarity with GCP/AWS cloud services
• Familiarity with TensorFlow
• Comfortable with CI/CD Pipelines
• Experience with Git version control",Forbes Media,"Jersey City, NJ",$83.9k–163k,Senior Data Engineer
164.1,"About Starry:

Starry is reinventing the way people connect to the internet. Our mission focuses on two things. First, we’re making the experience of accessing the internet simple, transparent, and delightful. Second, we’re bringing that experience to underserved communities around the world. We approach our mission with a cutting-edge wireless technology, customer service designed to delight, and a culture of innovation and intellectual curiosity.

What we’re looking for:

The analytics team is responsible for maintaining our internal analytics platform used by many departments at Starry. We’re looking for a New York based Data Engineer to join us as we begin to build out our analytics platform, tooling, and pipelines. You’ll work with verticals within the business to automate processes and data pipelines while reporting into a small centralized data engineering team. We are looking for a candidate that relishes autonomy and the design process.

What you'll be doing:
• Partner with... stakeholders to build data pipelines and tooling for their work
• Become a domain expert for certain datasets within Starry
• Build tools to monitor the flow and quality of data moving through our data pipelines
• Lead by example with best practices in coding and automation
• Work on an evolving data infrastructure platform at a fast growing company
• Participate in code reviews and design meetings
• Learn new tools and concepts on the job
• Communicate with analytics and engineering stakeholders to help you get the job done
• Participate in a positive work environment

Qualifications:
• 3+ years data engineering experience
• Python and SQL fluency
• Experience with relational databases like PostgreSQL
• Experience with batch ETL
• Experience with Git and CI/CD within the context of data engineering

Bonus points if...
• GIS data experience
• Spark experience
• Serverless experience
• AWS ecosystem experience
• Streaming data experience with Kafka or Kinesis
• Orchestration experience with frameworks like Airflow
• Front end experience
• Container experience with frameworks like Docker

We work hard, so we take care of each other and try to enjoy ourselves along the way. We have:
• Premium medical, dental, and vision coverage with no employee contribution required
• 12 weeks of paid parental leave
• Catered meals on a weekly basis
• Groups for skiing, biking, running, climbing, stretching, shuffleboard, darts, and more

Happy Interneting!

Qualified Applicants must be legally authorized for employment in the United States. Qualified Applicants will not require employer sponsored work authorization now or in the future for employment in the United States.

Disclaimer: This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee.

Starry, Inc. is an equal employment opportunity employer and does not discriminate against any applicant because of race, creed, color, age, national origin, ancestry, religion, gender, sexual orientation, disability, genetic information, veteran status, military status, application for military service or any other class protected by state or federal law",Starry,"New York, NY",$91.6k–145k,Data Engineer
243.8,"About Foursquare:
Since our inception in 2009, Foursquare has been a leading force in changing how location information enriches our real-world and digital lives. As a location intelligence company, Foursquare is comprised of our thriving media and enterprise products, as well as two well-known consumer apps—Foursquare and Swarm. Our B2B offerings include the Pilgrim SDK, Places, Pinpoint and Attribution (for marketers), and Place Insights (for analysts, based on the world's largest foot traffic panel). With more than 200 people across our offices in New York, San Francisco, Chicago, and in sales offices around the globe, we’re dedicated to our trailblazing mission— to build the most trusted, independent platform for understanding how people move through the real world.
About this Position:
As a member of Foursquare's Places Engineering team you drive the quality and direction of Foursquare’s Places Database, both as a standalone product and as the dataset underpinning Foursquare’s... core visit detection technology, developer API, and other enterprise product offerings. We're passionate about tackling tough challenges in the location space and look for others who like to dive deep into code and help solve hard problems. You should be comfortable running with your own ideas and eager to learn new skills on an emerging platform. We use a variety of tools, technologies, and languages to build software ( Scala, Python, Spark, EMR, MongoDB, Pants, Thrift) but experience with equivalent ones will do just fine.
Join us and help bring our feature ideas (and your own!) off the whiteboard and into reality. As a Staff Engineer on the Places team, you will drive technical and architectural planning, research, implement, and deploy improvements in data collection, feature engineering and algorithmic optimization. While we’re not necessarily looking for a Machine Learning engineer (although that’s definitely a plus!), you would have the opportunity to work on building ML models and big data pipelines.
This role is open for hire in our New York City headquarters.
Qualifications:
Degree in Computer Science, Computer Engineering, or Statistics or commensurate experience
6-8+ years of software engineering and/or team lead experience
Proven track record of building and shipping large-scale engineering products
Experience working with large, complex data sets from a variety of sources
Ability to collaborate with a diverse set of engineers, data scientists and product managers
Experience with functional and object-oriented programming, Scala a plus
Experience with Databricks or Spark, EMR
Effective communication skills (both written and verbal), comfortable presenting to a group
Comfort in a fast-paced startup environment
Foursquare is proud to foster an inclusive environment that is free from discrimination. We strongly believe in order to build the best products, we need a diversity of perspectives and backgrounds. This leads to a more delightful experience for our users and team members. We value listening to every voice and we encourage everyone to come be a part of building a company and products we love.
Foursquare is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other characteristic protected by law",Foursquare,"New York, NY",$77.3k–333k,Staff Data Engineer
202.0,"ABOUT THE TEAM AND ROLE
We’re looking for an experienced data engineer and mentor to join theSkimm's Tech team. Our mission is to enable our partners across theSkimm, from Editorial and Audio to Marketing and Advertising, to achieve their goals with the best systems and processes we can offer. We build tools throughout the stack, share knowledge across departments, and learn quickly so we can take best advantage of what’s coming.
As we grow, we're looking for a Senior Data Engineer who’ll help solidify and expand our data reporting and analysis pipelines. Our business is run on detailed analysis of how our products perform with our members, which features they love and which can be improved, and which product and marketing campaigns are bringing in the best quality users. You'll be responsible for working alongside product, design, and other teams to define new data collection components and measurement schemes that support each new product and feature.
WHAT YOU'LL NEED
4-6+ years of... industry experience measuring product performance and user behavior
Experience working with a variety of data management technologies, including RedShift, Kinesis/Kafka, Glue, Spark, Postgres, Airflow, dbt, and others.
Experience supporting and implementing reporting tools such as Looker and Tableau
Knowledge of a variety of measurement beacons and SDKs, including Google Analytics, Localytics, Moat, ComScore, and Heap
A commitment to building secure, resilient, fault-tolerant architectures, with clear documentation and procedures in place for support
A focus on accuracy and detail as you build, ensuring data pipelines produce clear, predictable results
Ability to thrive in a dynamic, fast-paced, collaborative, and high-growth environment
Facility in presenting and discussing the trade-offs in employing different engineering solutions to a problem, valuing pragmatism over idealism
An empathetic leadership style that encourages open communication and trust
Experience guiding engineers on your team to learn more and expand their skills
WHAT WOULD SET YOU APART
Web programming experience, especially in React, Node, and Rails
Experience with Google Tag Manager and other measurement management tools
Understanding of the typical metrics an advertising-supported business needs to measure success
Experience with ML techniques as applied to behavioral segmentation or anomaly identification
Familiarity using developer tools that increase productivity and facilitate the development of resilient code (eg. Docker, Travis, Jenkins)
A history of leading and guiding high-performing technical teams
Familiarity and enthusiasm for theSkimm: a passion for our audience and mission
WHAT WE OFFER
Medical, Dental, Vision, Life & Disability Insurances, starting on your first day!
Equity for all employees
A flexible vacation policy and generous holiday observances
'MassMutual' 401(k) program
Free 'One Medical' Membership
Up to $400 in annual gym-membership reimbursement
Subsidized family planning and fertility benefits through 'Carrot Fertility'
Catered meals and drinks during our weekly ""Sip’n Skimm"" all-hands meeting and twice-monthly ""Skimm'e A Drink!"" happy hour
Collaborative office space with a mix of snacks, seltzer, caffeine, and a pretty sweet roof deck
This is a full-time role based in New York, NY",theSkimm,"New York, NY",$114k–176k,Senior Data Engineer
,"The Data Intelligence organization aims to make data a strategic asset for the enterprise by providing a platform that enables the structuring, management, integration, control, discovery, usage, and governance of our Data Assets. The team leverages a wide variety of cutting edge technologies including Hadoop, HBase, Spark, Apache Beam, Apache Flink, Kakfa, SQL, OLAP platforms, Presto, Hive, Java and Python. Your impact will be to Curate, design and catalog high quality data models to ensure that data is accessible and reliable. Build highly scalable data processing frameworks for use across a wide range of datasets and applications. Provide data-driven insight and decision-making critical to GS’s business processes, in order to expose data in a scalable and effective manner. Understanding existing and potential data sets in both an engineering and business context.

How You Will Fulfill Your Potential

RESPONSIBILITIES AND QUALIFICATIONS
• Deploy modern data management tools to... curate our most important data sets, models and processes, while identifying areas for process automation and further efficiencies
• Evaluate, select and acquire new internal & external data sets that contribute to business decision making
• Engineer streaming data processing pipelines
• Drive adoption of Cloud technology for data processing and warehousing
• Engage with data consumers and producers in order to design appropriate models to suit all needs
Skills And Experience We Are Looking For
• 2-3 years of relevant work experience in a team-focused environment
• A Bachelor’s degree (Masters preferred) in a computational field (Computer Science, Applied Mathematics, Engineering, or in a related quantitative discipline)
• Working knowledge of more than one programming language (Python, Java, C++, C#, etc.)
• Extensive knowledge and proven experience applying domain driven design to build complex business applications
• Deep understanding of multidimensionality of data, data curation and data quality, such as traceability, security, performance latency and correctness across supply and demand processes
• In-depth knowledge of relational and columnar SQL databases, including database design
• General knowledge of business processes, data flows and the quantitative models that generate or consume data
• Excellent communications skills and the ability to work with subject matter expert to extract critical business concepts
• Independent thinker, willing to engage, challenge or learn
• Ability to stay commercially focused and to always push for quantifiable commercial impact
• Strong work ethic, a sense of ownership and urgency
• Strong analytical and problem solving skills
• Ability to collaborate effectively across global teams and communicate complex ideas in a simple manner
Preferred Qualifications
• Financial Services industry experience
• Experience with the Hadoop eco-system (HDFS, Spark)
About Goldman Sachs

The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

Â© The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.

Division Engineering",Goldman Sachs,"New York, NY",,Data Engineer - Client Data Engineering
153.3,"Data Engineer

Commercialization Information Systems

Amgen, Thousand Oaks

Amgen is seeking a Data Engineer to help realize Amgen’s Commercialization analytics transformation strategy by developing a state-of-the-art analytical platform that will enable generation of cross-capability business insights. The Data engineer will help build leading edge scientific applications with modern technology stacks that will be used by our Commercialization partners to advance Amgen’s pipeline. The extraordinary candidate will join our Commercialization Information Systems team to deliver solutions and products using best practices in Agile and DevOps. This Data Engineer position will report to the Senior Manager Commercialization Information Systems. The position will be based in Amgen’s Thousand Oaks campus.

The Commercialization Information Systems team supports the critical business process in developing innovative product strategies, managing Amgen’s pipeline portfolio, and making effective... portfolio decisions to accelerate and grow Amgen’s pipeline. Our success in analytics and deep insights is driven by our commitment to our patients, to our business clients, and to leverage superlative technologies that deliver critical business value.

Responsibilities:
• Create and improve systems to enable end to end solutions and products in support of Commercialization activities
• Build data micro services which perform data transformation, metadata extraction, workload management and error processing management
• Query, manipulate, and visualize data using R, JavaScript / CSS, and Tableau
• Evaluate and utilize state of the art technologies in the industry to meet business needs
• Contribute to maturing the Planisware system capabilities
• Contribute to continuous development of the data analytics and insights strategy for Commercialization
• Integrate the operations data platform with analytical tools such as Tableau, the Data Scientist Workbench, the Data Marketplace, etc.
• Develop data processing pipelines for large datasets in the cloud (AWS); integrate with other data sources where applicable;
• Collaborate with the other engineering team members to ensure all services are reliable, maintainable, and well-integrated into existing platforms
• Adhere to best practices for testing and designing reusable code
• Ensure effective communication between key partners, including business clients, technical staff and vendors to analyze scientific needs and implement informatics solutions in data acquisition, integration and analysis
• Own and run product backlog delivery

Basic Qualifications:

Master’s degree in computer science or closely relevant degree program and 2 years of software engineering experience

OR

Bachelor’s degree and 3 years of software engineering experience

OR

Associate degree and 6 years of software engineering experience

OR

High school diploma / GED and 8 years of software engineering experience

Preferred Qualifications:
• Bachelor’s or Master’s degree in Life Sciences, Computer Science or Engineering
• Experience in Software Engineering and Development
• Strong learning agility, ability to pick up new technologies used to support Commercialization data analysis needs
• Experience in Planisware
• Strong experience working with agile methodology & DevOps (Jenkins, JIRA, Github) frameworks with successful experience working in a collaborative team environment
• Experience working with container technologies (e.g., Docker) and developing microservices
• Proficiency in software development languages including but not limited to Java and C#
• Experience in Cloud (AWS, databricks platforms) & HPC (high performance computing) environments
• Experience in developing and supporting web applications including familiarity with web technologies and frameworks (EXTJS, D3 JS, React.js)
• Data analysis and reporting experience by using analytics, visualization and database technologies (Oracle, PL SQL, Spotfire, Tableau, Python – NumPy, SciPy, Pandas)
• Expert in R scripting and R development in R Shiny;
• Experience processing and analyzing large NGS data;
• Expertise with translating business requirements to technical requirements and recommend solutions
• Working with leading agile development methodologies such as Sprint and Scrum
• Knowledge of or experience in Life, Physical or Computational Sciences
• Strong written and oral communication skills

Amgen is committed to unlocking the potential of biology for patients suffering from serious illnesses by discovering, developing, manufacturing and delivering innovative human therapeutics. This approach begins by using tools like advanced human genetics to unravel the complexities of disease and understand the fundamentals of human biology.

Our culture is what makes Amgen a special place to work. We have a powerful shared purpose around our mission – to serve patients. We respect one another, recognize contributions, and have embedded collaboration, trust, empowerment and inclusion in all that we do.

We equip all our staff members to live well-rounded, healthy lives. Most recently, Amgen added benefits for transgender employees and continues to pride itself on industry-leading, family-friendly offerings for families of all compositions.

Amgen focuses on areas of high unmet medical need and uses its expertise to strive for solutions that improve health outcomes and dramatically improve people’s lives. A biotechnology pioneer since 1980, Amgen has grown to be one of the world’s leading independent biotechnology companies, has reached millions of patients around the world and is developing a pipeline of medicines with breakaway potential.

Amgen is an Equal Opportunity employer and will consider you without regard to your race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status",Amgen,"Thousand Oaks, CA",$99.3k–108k,Data Engineer
177.5,"Why do we want you?

You are an ambitious, driven developer looking to play a central role in the design, development and implementation of cutting edge, highly-scalable applications based on large amounts of unstructured/structured data. You’re passionate about open source and Big Data technologies. You find yourself thinking about complex, cutting-edge systems in the shower. You want the opportunity to collect, parse, manage, analyze, and visualize large data sets to extract meaningful knowledge. And you dream of working with NoSQL database technologies and fun, emerging data science techniques. As part of a small team of experts, you have a strong desire to contribute to important endeavors to address critical needs of Fortune 500 companies.
Responsibilities
• Work with an amazing team to design and develop modern distributed Big Data processing and analysis systems
• Utilize primarily open source development tools and code frameworks (e.g. Git, Storm)
• Define and manage... development tasks within an agile team
• Utilize unit tests and deploy code to live systems
• Work closely with customers and incorporate feedback into development activities
• Gracefully accept criticism when you make a less-than-stellar lunch recommendation

Skills & Qualifications

Must have:
• Strong object-oriented programming or functional language experience (Clojure preferred, Java also ok)
• Programming with scripting languages (e.g. Python, Perl, Bash, etc)
• Proficient in UNIX/Linux environments (we work on Macs and deploy to Ubuntu on AWS)
• Strong knowledge of programming structures and algorithms
• Excellent oral and written communications skills
• Paint like Bob Ross (or the ability to fake it)

Nice to have:
• Distributed, fault-tolerant architectures (Storm experience preferred, Spark or Hadoop ok)
• Working with NoSQL databases and JSON data (ElasticSearch preferred)
• Working with messaging/queueing systems (RabbitMQ preferred)
• Working with SQL databases and large-scale data integration (MySQL preferred",Signafire,"New York, NY",$110k–135k,Data Engineer
175.0,"Want to build a product that uses data to see and make sense of the future?

If you are a coding fanatic and passionate about programming, we want you to help us make a huge impact. Our clients love our product and are thirsty for more!

At CB Insights we build products that help clients make sense of the future and drive their businesses forward using data. Our system retrieves large amounts of structured and unstructured data and uses scientific methods to extract knowledge and insights from that data. We present those analytics through a sophisticated, dynamic user interface which enables our clients to find answers to their most important questions.

As a Data Engineer at CB Insights you will be part of a cross-disciplinary, self-motivated team with clear ownership and passion to form the future. Our crew uses state-of-the-art technologies and writes quality code that ships often.

Our data software engineers build scalable data pipelines and big data processing systems that run... on AWS cloud.

We focus on modularity and reuse where it makes sense, while ensuring that there are no constraints to delivering world-class software continuously.

Much of our software team has been with us for several years, despite a white hot tech market with options galore. We attribute this to our collaborative teach and learn culture where the role evolves with your interests.

If this sounds interesting to you, reach out and join CB Insights now!
Key Responsibilities:
• Engineer efficient, adaptable and scalable data pipelines for unstructured textual data sets and various types of data
• Take a prototype of a data product built with NLP and/or machine learning models and make it run reliably in production
• Monitor and maintain existing data products running in production including identifying when models need to be retrained
• Design and implement internal tools to make this data processing infrastructure easily accessible to and usable by other software developers
• Develop solutions that are well-engineered, maintainable, tested and delivered on time
• Participate in code reviews and sprint planning, help to identify problems and share knowledge with your colleagues

Required Experience & Qualifications:
• 2+ years professional software/data engineering experience using Python, SQL and at least 1 statically typed language (Go, Java, Scala)
• Knowledgeable about data modeling, data storage techniques, data warehousing and general data architecture
• Experience with engineering data pipelines to capture, store and process unstructured data
• Excellent written and verbal communication skills
• Excellent problem solving and analytical skills
• Believer in Lean and Agile values and principles for building software
• Proficiency developing in a Mac/Linux environment
• Helpful Humble Human

Nice to Haves:
• Experience with Go, AWS services (RDS, S3, SQS, Redshift, Spectrum, Glue)
• Experience building and maintaining a Hadoop or Spark cluster and other related tools in the big data ecosystem

We know that diversity makes for the best problem-solving and creative thinking. We are dedicated to adding new perspectives to the team and encourage everyone to apply if your experience is close to what we are looking for.
Perks and Benefits:
• Subsidized health, dental, and vision insurance
• 401k with up to 4% match
• $1,000 yearly continuing education stipend
• Daily lunch stipend

Happy, Helpful, Humble, and Hungry: Check out more about our company culture here.

Equal Opportunity Employer: CB Insights is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.

If you know someone who'd be perfect for the role,
submit here and you'll be eligible for $5,000",CB Insights,"New York, NY",$60k–230k,"Software Engineer, Data"
133.7,"The team.
We are a rapidly growing and highly capable engineering team building the most popular job site on the planet. Every month, over 250 million people count on us to help them find jobs, publish their resumes, process their job applications, and connect them to qualified candidates for their job openings. With engineering hubs in Seattle, San Francisco, Austin, Tokyo and Hyderabad, we are improving people's lives all around the world, one job at a time.

Your job.

As a Data Engineer at Indeed, your role is to deliver the information your business partners need to grow the business. You will assist business teams in drawing insights from our data. You are someone who wants to see the impact of their work and make a difference every day. You know what it takes to deliver quality reporting and business intelligence solutions to the organization. You understand the value and benefit of solid data practices and how to translate that into satisfied business partners.

You are an... experienced Software Engineer. You are skilled in extracting, transforming, and loading data. You are able to translate business requests into database design. You are someone who is passionate about data-driven approaches. You enjoy exploring large data sets and get excited about learning new technologies and learning in a collaborative environment. You are skilled at eliciting requirements from a wide range of different teams.
Responsibilities
• Create and manage data sources
• Integrate with diverse APIs
• Contribute to the ongoing development of the data warehouse ecosystem
• Work closely with stakeholders on the data demand side (finance, analysts, and data scientists)
• Work closely with stakeholders on the data supply side (domain experts on source systems of the data)
• Design and build optimized OLAP and Star Schema data structures
• Build self-monitoring, robust, scalable batch and streaming data pipelines for 24/7 global operations.
• Create highly reusable code modules and packages that can be leveraged across the data pipeline
• Develop and maintain data dictionaries for governance of published data sources
• Develop and improve continuous release and testing processes
• Elicit requirements from a wide range of different teams

About you.

Requirements
• Bachelor's degree in computer science, computer engineering, or an engineering discipline
• Strong CS fundamentals, problem-solving skills and software engineering skills
• 3+ years industry experience in software development and/or data engineering
• Experience in a hands-on, data centric role with data engineering, streaming, or warehousing.
• Ability to communicate effectively with stakeholders to define requirements
• A strong ability to understand and organize data from various sources.
• Strong expertise in an object oriented language (preferably Python or Java)
• Strong SQL skills
• Experience in columnar relational data stores and NoSQL technologies
• Experience with big data tools such as Hadoop, Hive, Spark, etc, as well as knowledge of more traditional warehouses.
• Experience delivering data pipelines and managing resulting data stores using managed cloud services (like AWS or Google Cloud Services)
• Ability to identify and resolve performance and data quality issues
• Experience with modern data pipelines, data streaming, and real time analytics using tools such as Apache Kafka, AWS kinesis, Spark Streaming, ElasticSearch, or similar tools.
• Knowledge of machine learning tools and concepts a plus.

Indeed provides a variety of benefits that help us focus on our mission of helping people get jobs.

View our bounty of perks: http://indeedhi.re/IndeedBenefits

View Indeed's Applicant Privacy Terms: https://www.indeed.com/legal/applicant-privacy",Indeed,"Austin, TX",$74.2k–119k,Data Engineer
164.1,"At Noom, we use scientifically proven methods to help our users create healthier lifestyles, and manage important conditions like Type-II Diabetes, Obesity, and Hypertension. Our Engineering team is at the forefront of this challenge, solving complex technical problems that center around habits, behavior, and lifestyle.
We are looking for a Data Engineer to join our Data team and help us improve and maintain our Data Warehouse. If you like to work with billions of rows of data and be at the center of data-driven decisions, we’ll love working with you.
What You’ll Like About Us
We work on problems that affect the lives of real people. Our users depend on us to make positive changes to their health and their lives.
We base our work on scientifically-proven, peer-reviewed methodologies that are designed by medical professionals
We’re a respectful, diverse, and dynamic environment in which Engineering is a first-class citizen, and where you’ll be able to work on a variety of interesting... problems that affect the lives of real people.
We offer a generous budget for personal development expenses like training courses, conferences, and books.
You’ll get three weeks’ paid vacation and a flexible work policy that is remote- and family-friendly (about 50% of our engineering team is fully remote). We worry about results, not time spent in seats.
Delicious (and nutritious) daily lunches and snacks prepared by Sam, our NYC office on-site chef.
What We’ll Like About You
You have experience dealing with data at scale, processing and transforming hundreds of millions of data points per day.
You have first-rate SQL skills, but you are aware of its limits. You know when to use it, and when it’s better to find a different solution.
You are familiar with ETL tools and problems. We use Airflow, Redshift, Glue, and many other systems.
You’re used to work alongside data analysts and data scientists to help them prepare complex datasets that can be used to solve difficult problems",Noom,"New York, NY",$91.6k–145k,Data Engineer
472.5,"Who we are
Cityblock Health is a new type of healthcare company, operating out of Brooklyn and backed by Alphabet’s Sidewalk Labs, along with some of the top healthcare investors in the country.
Our mission is to radically improve the health of urban communities, one block at a time. Importantly, our solutions are designed specifically for Medicaid and lower-income Medicare beneficiaries, and we bring the capability to deliver care in the home and neighborhood with our field-based teams.
In close collaboration with community-based organizations and leading commercial partners, we are reorganizing the health system to focus on what matters to our members. We deliver personalized primary care, behavioral health, and social services through a network of neighborhood hubs with deep community-based partnerships and world-class technology.
We partner with payers and at-risk providers, accepting capitated financial risk to care for their most vulnerable, high and rising risk members... Additionally, we invest in strategic partnerships with community-based and social services organizations, in some instances bringing them into the value equation through sub-capitation and performance-based compensation.
Over the next year we’ll grow quickly, including entering new markets, each with their own commercial relationships and field-based teams. This role will be a key contributor to the success of our business.
The role
We are looking for a Staff Data Engineer to help build and maintain the data infrastructure that backs Commons—our digital care management platform. Commons enables our clinical teams to engage with patients, collect structured data about medical, behavioral and social needs, and develop personalized care plans that drive good health. Commons will allow our care operations to scale to eventually support hundreds of thousands of patients in cities across the country.
You will help build Mixer—our data pipeline. You will work closely and cross-functionally with your peers from the product, data, and clinical teams to collect and structure data—both from primary and 3rd party data sources— analyze that data, and make it useful in promoting the medical and social well-being of our patients through automation and decision-support capabilities.
Mixer involves a collection of services that run on Google Cloud Platform, including PubSub, Dataflow (via Scio), Datastore, and BigQuery. These services are provided by Scala, Ruby, and Elixir.
If you’re inspired by such a challenge and are an amazing teammate, we’d love to hear from you!
You will:
Be responsible for the integrity and accuracy of data in our systems.
Focus on building our data pipeline, i.e. the backend features and related data infrastructure that will store, clean and transform data—making it useful to our application and reporting layers.
Develop secure integrations into the data systems of various community and health system partners (e.g., Electronic Health Records (EHRs), Health Information Exchange Data (HIE), ADT feeds, claims/pre-auth feeds, predictive analytics as well as data sets from government services and community-based organizations).
Design our data pipeline to scale up to handle integrations across hundreds/thousands of partners
Write clean, well-tested, code that will stand the test of time.
Participate in creating and maintaining strict compliance, data privacy and security measures.
Help recruit highly capable engineers to the team from diverse backgrounds.
Learn continuously via mentorship (and on your own!) to upgrade your skills and thinking as an engineer.
Mentor junior Data Engineers, new team members, and apply technical expertise to help others grow effectively
Collaborate with clinicians and social workers to understand the challenges that they face and develop solutions to drive better care for patients.
You’d be a good fit if:
You are excited at the prospect of making messy data useful.
You will have knowledge of and experience in one or more statically typed functional languages (Scala, Haskell, ML).
You enjoy doing whatever it takes to execute on complex projects with little guidance.
You have 5+ years experience writing production code.
You have worked in fast-moving startup environments before.
You enjoy taking the initiative.
You have a process-oriented mindset and ability to execute.
You have a passion for doing mission-oriented work.
Nice to have:
You are experienced with data engineering in the healthcare domain.
Experience with the technologies in our stack.
Experience deploying models created by data scientists.
Familiarity with BI tools (e.g. Looker, Tableau, etc).
Experience building user-facing features.
Previous exposure to clinical operations and/or working with physicians.
“Nice to have” really means “nice to have”. It’s completely possible that you don’t have any of these and are still a great fit for the team.
You should include these in your application:
A resume and/or LinkedIn profile.
A cover letter including a one paragraph summary of a technical project you’re most proud to have built and what was hard about it. Optionally include links to public-facing documentation about the project such as a Github repo or blog post, if available.
Cityblock values diversity as a core tenet of the work we do and populations we serve. We are an equal opportunity employer, indiscriminate of race, religion, ethnicity, national origin, citizenship, gender, gender identity, sexual orientation, age, veteran status, disability, genetic information, or any other protected characteristic",Cityblock Health,"Brooklyn, NY",$315k–315k,Staff Data Engineer
202.0,"As a member of the Yieldmo data team you are expected to build innovative data pipelines for processing and analyzing our large user datasets (250 billion + events per month). A unique challenge with the role is being comfortable in developing in varied technologies including: Develop custom transformation/integration apps in Python and Java, and build pipelines in Spark, Kafka, Kinesis, transforming and analyzing in SQL.

Responsibilities:
• Develop ETL (Extract, Transform and Load) Data pipelines in Spark, Kinesis, Kafka, custom Python apps to transfer massive amounts of data (over 20TB/ month) most efficiently between systems
• Engineer complex and efficient and distributed data transformation solutions using Python, Java, Scala, SQL
• Productionalize Machine Learning models efficiently utilizing resources in clustered environment
• Research, plan, design, develop, document, test, implement and support Yieldmo proprietary software applications
• Analytical data validation for... accuracy and completeness of reported business metrics
• Open to taking on, learn and implement engineering projects outside of core competency
• Understand the business problem and engineer/architect/build an efficient, cost-effective and scalable technology infrastructure solution
• Monitor system performance after implementation and iteratively devise solutions to improve performance and user experience
• Research and innovate new data product ideas to grow Yieldmo’s revenue opportunities and contribute to company’s intellectual property

Requirements:
• BS or higher degree in computer science, engineering or other related field
• 5+ years of Object Oriented Programming experience in languages such as Java, Scala, C++
• 3+ years of experience of developing in Python to transform large datasets on distributed and cluster infrastructure
• 5+ years of experience in engineering ETL data pipelines for Big Data Systems
• Prior experience of designing and building ETL infrastructure involving streaming systems such as Kafka, Spark, AWS Kinesis
• Experience of implementing clustered/ distributed/ multi-threaded infrastructure to support Machine Learning processing on Spark or Sagemaker
• Proficient in SQL. Have some experience performing data transformations and data analysis using SQL
• Comfortable in juggling multiple technologies and high priority tasks
• Nice to have: experience with Distributed columnar databases like Veritca, Greenplum, Redshift, or Snowflake

Success in this role:
• Demonstrate a passion for Data
• Eagerness in research and learning new technologies to develop creative and efficient ways to solve business problems
• Take full responsibility of the initiative
• Stay focused on the successful implementation of the task at hand before moving on to next engineering challenge
• Going above and beyond:
• While engineering for current ask, think of big picture, adjustment code bases, processes. Try ways to make systems more robust, fault tolerant, monitor for failures, and program for automated recovery",Yieldmo,"New York, NY",$114k–176k,Senior Data Engineer
202.0,"Senior Data Engineer
Who we are
DoubleVerify is the leader in digital performance solutions, improving the impression quality and audience impact of digital advertising. Built on best practices, DoubleVerify solutions create value for media buyers and sellers by bringing transparency and accountability to the market, ensuring ad viewability, brand safety, fraud protection, accurate impression delivery and audience quality across campaigns to drive performance.
Since 2008, DoubleVerify has helped hundreds of Fortune 500 companies gain the most value out of their media spend by delivering best in class solutions across the digital ecosystem that help build a better industry.
Overview
The Senior Data Engineer will join DV's Programmatic software engineering team. On this the team the Senior Data Engineer will use data analysis and statistical modeling to enhance our products, build reporting solutions and provide meaningful data-driven insights to influence our roadmaps and product... decisions.
As Senior Data Engineer in Social Integrations team, you will lead new initiatives, designs and builds deep integrations with the world’s biggest social platforms and other walled gardens in order to measure ad performance.
You will work directly with engineers from the largest internet companies in order to collaborate on APIs development.
Who you are
Lead by example - design, develop and deliver quality solutions.
At least 6 years’ experience coding in python/scala/java/c#
Deep understanding of web technologies, standards, protocols, etc.
You are passionate about crafting clean code and have a steady foundation in coding and building data pipeline.
Excellent Experience working in distributed environments
Experience integrating with 3rd party APIs
Experience with big data technologies and working with data (ETL, processing)
Very good SQL query writing abilities and data understanding
Have good DevOps skills - working with build servers, docker and containers clusters (kubernetes).
Have B.Sc./M.Sc. in Computer Science or a related field
Excellent communication skills and a team player",DoubleVerify,"New York, NY",$114k–176k,Senior Data Engineer
121.2,"Bunge Limited (www.bunge.com, NYSE: BG) is a leading global agribusiness and food company operating in over 40 countries with approximately 32,000 employees. Bunge buys, sells, stores and transports oilseeds and grains to serve customers worldwide; processes oilseeds to make protein meal for animal feed; produces edible oil products for consumers and commercial customers in the food processing, industrial and artisanal bakery, confectionery, human nutrition and food service categories; produces sugar and ethanol from sugarcane; mills wheat, corn and rice to make ingredients used by food companies; and sells fertilizer in South America. Founded in 1818, the company is headquartered in White Plains, New York.

Position Summary:

The data engineer will be an integral part of the Bunge Global Research team focusing on applying the best data engineering methodologies and practices to capture and maintain large Bunge internal and publicly available data sets for Bunge s data scientists and... analysts globally.

Core Functions:

Work in a cross-disciplinary project team of database specialists, data scientists, and business subject-matter experts to understand requirements and design and implement data integration solutions.

Collect available data from different sources as it relates to the business needs. Design, build, test and deploy data integration solutions to move data from Bunge s internal systems and external data sources to Bunge s data science and analytics platform.

Work with the information technology team to understand data capture and testing needs. Implement architecture tests and maintenance protocols.

Assess new data and sources in terms of availability and quality and clearly document findings on availability and quality of data. Build a portfolio of datasets for analysis by data scientists that will be turned into business insights.

Maintain source code repository of scripts (SQL, Python, R) provide governance and best practices of data structures, data integrity, reporting and querying.

Leverage architecture standards/patterns/API s that will enable simple use of complex datasets

Be able to recommend and implement or help implement, ways to improve data reliability , efficiency and quality.

Help train end-users in database access and data utilization best practices as well as manage users issues that may arise and help solve them. Help design dataset processes for data modeling.

Effectively communicate, through written and oral presentation, data insights to management and functional analysts.

Required qualification and skills:

Skills/Experience Requirements

Bachelor s degree in Computer Science or a technical/quantitative field (e.g. engineering, statistics, physics, math, econometrics)

4+ years of experience with large-scale data management

Excellent knowledge of Oracle and other data warehouse softwards, acquaintained to SAP is definitely a plus. Excellent programming skills in shell scripting, SQL, Python, or other equivalents

Familiar with software integration using RESTful APIs

Strong communication skills, critical thinking, attention to detail, and business acumen

Preferred qualification and skills:

Proficiency in WebApp development, HTML/CSS, and Javascript/node.js

Familiarity with cloud application development, Hadoop/Spark

Familiarity with Selenium

Understanding of agronomy, remote sensing, GIS, economics, and finance is a plus

Bunge is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, gender expression, national origin, citizenship, age, disability or military or veteran status, or any other legally protected status. Bunge is an Equal Opportunity Employer. Minorities/Women/Veterans/Disabled",Bunge North America,"White Plains, NY",$67.2k–108k,Data Engineer in White Plains NY
164.1,"- New York City

About the right team member:

As a Data Engineer at Betterment, you will tackle a diverse set of data-oriented problems, working with a wide range of teams and learning a great deal about finance in the process. One morning you might work with our Finance team to construct data domains in Redshift that will allow them to hone and quickly tweak our financial model, and then spend the afternoon strategizing with our front-end application team on how to coordinate internal APIs that allow us to serve up historical data to customers in our applications. The next day you might assist our Operations team in automating the production of equities reconciliation reports, then close out the evening by hopping over to our Data Science team to get them the data they need to compare the effectiveness of TV advertisements in San Francisco and New York. If there’s data involved, you’re the one curating it - making it accessible, and ensuring it’s correct. A Data Engineer at... Betterment can expect to approach tasks such as these on a daily basis, leveraging our existing processes and using their prior experience to improve the way we handle data at Betterment.

This role can be based out of our headquarters in NYC or our brand new Philadelphia office.
At Betterment you will get to:
• Work on increasing the efficiency of our ETL processes as the size of Betterment’s data grows 10x annually
• Explore new technologies that will allow us to keep our internal API response times low even as throughput grows
• Move quickly to provide analysts with new data before they ask for it
• Investigate how we can enhance our logging and monitoring to discover and resolve issues before they cause problems
• Think about scale and new technologies that will enable us to achieve a high level of service as Betterment is managing hundreds of billions of dollars

You will be effective if you have:
• Have deep expertise in at least one object-oriented language, such as Java, Ruby, or Python
• Know how to handle an explosion of data without missing a beat
• Can optimize a query with the best of them
• Are the person at your current job that everyone goes to for database help, even though you aren’t necessarily a DBA
• Are so good at automating things that you’re constantly programming yourself out of a job
• Have a passion for software engineering, and for creating what doesn’t exist
• Know how to make the tradeoffs required to ship without compromising quality
• Appreciate agility and pragmatism in software development
• Thrive in a startup environment
• Have the grit to see projects through to their conclusion

Tools in your belt:
• Development: OO languages such as Python, frameworks such as Flask or Ruby on Rails, Advanced SQL
• Datastores: Redshift or other columnar stores, Postgres, MySQL, DynamoDB or other NoSQL stores
• Technologies: Event Streaming, Caching tools, MapReduce
• Platforms: AWS!

Betterment’s Data Engineering team spends most of its time with the tools above, but we cast a much wider net in other parts of the engineering team. We strive to always choose the best tool for the job. We maintain most of our ETL and orchestration in Python, but we serve up data to customers through APIs in a lightweight caching application built in Rails. The person for whom we’re looking will be a pro who can guide both our data pipeline development as well as our customer-facing APIs.
About Betterment

Betterment is the largest independent online financial advisor with more than $16 billion in assets under management. The service is designed to help increase customers’ long-term returns and lower taxes for retirement planning, building wealth, and other financial goals. Betterment takes advanced investment strategies and uses technology to deliver them to more than 400,000 customers across its three business lines: direct-to-consumer, Betterment for Advisors, and Betterment for Business",Betterment,"New York, NY",$91.6k–145k,Data Engineer
202.0,"Vimeo empowers video creators to tell exceptional stories and connect with their audiences and communities. Home to more than 90 million members in over 150 countries, Vimeo is the world’s largest ad-free open video platform and provides powerful tools to host, share and sell videos in the highest quality possible
Our platform has robust analytics tools that can tell video creators where people are from, where they click, and even track where in a video someone stopped watching—giving you insight into creating engaging content in the future.
As a Data Engineer on the Data team, you will create infrastructure and services that provide access to event streams. You will work with our business teams as well as other engineering teams to help define data access and discoverability requirements and implement systems to satisfy these requirements.
What You’ll Do:
Work with stakeholders to define new events streamed through our pipeline
Create ETL pipelines for events as needed
Help design... and build a system for data discoverability
Review Kafka specific metrics and key performance indicators for monitoring and performance management
Encourage innovation, implementation of cutting-edge technologies, outside-of-the-box thinking, teamwork, self-organization, and diversity
Skills & knowledge you should possess:
Over 5 years of engineering experience in a fast-paced environment, including at least 3 years experience in scalable data architecture, fault-tolerant ETL, and monitoring data quality in the cloud
Airflow or any other Open Source Data Pipeline
Java, Scala, or other JVM language experience
Apache Kafka
Hadoop, Spark, Flink, or similar data processing framework experience
Relational databases and SQL experience
Additional useful skills; not required:
Python or other scripting language experience a plus
Go experience
NoSQL database experience a plus
Cloud Provider experience (AWS tools, Google Cloud tools, Azure tools)
About us:
At Vimeo, our mission is to empower video creators to tell exceptional stories and connect with their audiences and communities. Home to more than 90 million members in over 150 countries, Vimeo is the world’s largest ad-free open video platform, providing powerful tools to host, share and sell videos in the highest quality possible.
We work hard to enable creators of all kinds to succeed, and to that end, we prioritize attracting diverse talent and cultivating an inclusive environment that encourages collaboration and creativity. We’re committed to building a company and a community where people thrive by being themselves and are inspired to do their best work every day.
Vimeo is based in New York City, with additional offices in Europe and India. Vimeo is an operating business of IAC (NASDAQ: IAC). Learn more at www.vimeo.com",Vimeo,"New York, NY",$114k–176k,Sr. Data Engineer
164.1,"About Us:
At Rent the Runway, our mission is to make women feel empowered and self-confident every single day by combining best in class technology, logistics, and customer service. Since our launch in 2009, we’ve developed proprietary technology, a one-of-a-kind reverse logistics operation, stores of the future, a viral brand, relationships with hundreds of fashion designers - and we are passionate about continuing to innovate our customer experience. We have pioneered the closet in the cloud and believe that every person globally will soon have a subscription to fashion. We are proud to be both a profitable and fast-growing business, with a loyal 10 million members who believe that rental is the future.
About the Team:
The analytics team collects a wide variety of data - structured and unstructured - which are brought together to drive key business decisions. This includes website and mobile app logs to track content displayed and user interactions, transaction tables with full... audit trails, and inventory/fulfillment lifecycles. The end goal is to empower the analyst in mining key insights on the user experience across all devices and marketing channels.
About the Job:
This Data Engineer will support our data logging, processing, reporting, and visualization frameworks which unlock the power of our unique datasets across our business and product
What You'll Do:
Participate in the full lifecycle of ownership for the data warehousing and reporting infrastructure including design, build, and maintenance.
Support new reporting requirements while maintaining system performance and handling increasing complexity by setting standards across the stack.
Increase the reliability and efficiency of backend systems to empower analysts, and provide exceptional level of confidence around data quality.
Familiarity with tools and best practices that support data warehousing and reporting at scale, and continuously seek/implement new tools to improve the infrastructure.
About you:
Academic background in Computer Science, MIS, or other quantitative/engineering field with 2-4 years of relevant work experience.
Worked actively in distributed data warehousing platforms like Vertica, Aster, Redshift etc. – experience in sharding, handling data-shipping.
High proficiency in relational databases (e.g. MySQL) and NoSQL stores (e.g. MongoDB); experience in merging disparate data forms.
Experience in custom ETL development and tools including scripting languages (Python preferable).
Extensive practice using SQL for analytics research and data cleansing.
Benefits:
At Rent the Runway, we’re committed to the happiness and well-being of our employees, and aim to create a workplace that fosters both personal and professional growth. Our benefits include, but are not limited to:
Generous Paid Time Off including vacation, paid bereavement, and family sick leave - every employee needs time to take care of themselves and their family.
Universal Paid Parental Leave for both parents + flexible return to work program - because we know your newest family member(s) deserve your undivided attention.
Paid Sabbatical after 5 years of continuous service - Unplug, recharge, and have some fun! .
Comprehensive health, vision, dental, FSA and dependent care from day 1 of employment - Your health comes first and we’ve got you covered.
Industry leading 401k match - an investment in your future",Rent the Runway,"New York, NY",$91.6k–145k,Data Engineer
186.0,"Data quality and quantity can make or break any machine learning application. Here at OpenAI we are looking for a data engineer to lead dataset creation, curation, and management for a wide variety of applied and research projects, from creating the next ImageNet to scaling our language models with new data sources. You’ll be an integral part of a team of software and machine learning engineers and research scientists working on some of the most cutting-edge AI projects in the field.You will:
• Own the process of finding, cleaning, curating, and storing large-scale datasets, and making them maximally accessible within OpenAI
• Develop and apply machine learning-based cleaning and curation techniques, innovating and pushing the boundaries of existing methods
• Develop and scale data architecture for your team, and design reusable data infrastructure that can be applied across OpenAI
• Partner with software engineering and machine learning experts from across the company

You’ll be a... good fit for this role if you are:
• Results-driven and enjoy working closely with a team
• Comfortable and excited by working in large, distributed systems
• Excited to develop and apply new and existing techniques
• Familiar with the basics of machine learning
• Engaged by OpenAI’s mission of building safe and beneficial artificial general intelligence.

About OpenAI

We’re building safe Artificial General Intelligence (AGI), and ensuring it leads to a good outcome for humans. We believe that unreasonably great results are best delivered by a highly creative group working in concert.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

Benefits
Health, dental, and vision insurance for you and your family
Unlimited time off (we encourage 4+ weeks per year)
Parental leave
Flexible work hours
Lunch and dinner each day",OpenAI,"San Francisco, CA",$104k–164k,Data Engineer
164.1,"The data engineering team at Take-Two Interactive is looking for an exceptional Data Engineer who is passionate about data and the insights that large amounts of data sets can provide. As a data engineer on this team, you will take a leadership role in the evolution of our data platform by applying cloud-based AWS services to solve challenging problems around: big data processing, data warehouse design, and BI self-service. You will be part of a data engineering team that focuses on automation and optimization for all areas of DW/ETL maintenance and deployment. You will work closely with the business and technical teams in analysis on many non-standard and unique business problems and use creative problem solving to deliver actionable output.

The ideal candidate will be someone with sound technical background in data domain – storage / processing / analytics, with solid business acumen and a strong automation / solution-oriented thought process. This individual will be a self-starter... who can start with a business problem and work backwards to conceive & devise best possible solution. We are looking for an individual who has a high sense of ownership over every deliverable by the team, is constantly obsessed with customer delight & business impact / end-result and ‘gets it done’ in business time.

Role Responsibilities
• Develop and manage stable, scalable data pipelines that cleanse, structure and integrate disparate big data sets into a readable and accessible format for end user analyses and targeting using stream and batch processing architectures.
• Develop and improve the current data architecture, data quality, monitoring and data availability.
• Collaborate with Data Scientists to implement advanced analytics algorithms that exploit our rich data sets for statistical analysis, prediction, clustering and machine learning
• Develop data quality framework to ensure delivery of high-quality data and analyses to stakeholders.
• Develop and support continuous integrations build and deployment processes which use Jenkins, Docker, Git, etc.
• Define and implement monitoring and alerting policies for data solutions.

Qualifications
• 5+ years of experience with detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools.
• 4+ years of hands-on experience working with AWS technologies stack including Redshift, RDS, S3, EMR, Snowflake or similar solutions build around Hive/Spark etc.
• 4+ years of hands-on experience in using advanced SQL queries (analytical functions), experience in writing and optimizing highly efficient SQL queries.
• 3+ years of programming or scripting experience using Python, Spark and/or Scala.
• Proven track record of delivering big data solutions – batch and real-time.
• Ability to design, develop and automate scalable ETL and reporting solutions that transforms data into accurate and actionable business information.
• Comfort in working with business customers to gather requirements and gain a deep understanding of varied datasets.
• Experienced in testing and monitoring data for anomalies and rectifying them.
• Knowledge of software coding practices across the development lifecycle, including agile methodologies, coding standards, code reviews, source management, build processes, testing, and operations.
• Bachelor’s degree or equivalent in an engineering or technical field such as Computer Science, Information Systems, Statistics, Engineering, or similar.

Preferred Qualifications
• AWS Certification
• Python
• Spark
• SQL
• Understanding of EMR and related compute engines
• Build and deployment tools (for example Jenkins, Maven, SBT)
• Git
• Developing solutions using Docker
• Developing solutions on Kubernetes clusters
• Developing stream-processing systems, using frameworks such s Spark-Streaming, Kafka Streams or Storm
• Developing microservices
• Data modeling for data warehousing",Take-Two,"New York, NY",$91.6k–145k,Data Engineer
137.5,"Quantitative Data Engineer

We are looking for an outstanding engineer to join our investment team to build out our big data analytics pipeline.

If selected, the candidate will have significant responsibility in extracting signals from structured and unstructured data, and in transforming them into tradeable insights.

Required:

BS, MS, or PhD in Computer Science, Statistics, or related discipline

Proficiency in Python and analytic packages such as Numpy, Pandas, Scikit-Learn

Proficiency in SQL

Preferred:

Experience in:
• processing big data with Python using Spark, Dask, Blaze
• OLAP with AWS Redshift, Google BigQuery
• ETL orchestration with Apache Airflow, AWS Glue, or Google Dataflow",Tudor Investment Corporation,"New York, NY",85k–105k,Quantitative Data Engineer
164.1,"About Graphika
Graphika maps and analyzes the complex fabric of social network structures, or what we call “cyber-social terrain.” We create large-scale maps of social media landscapes and provide in-depth analysis to help clients and partners understand and navigate complex online networks.
Our team of scientists, engineers, analysts, and technologists use Graphika’s unique mapping technology to deliver insights in a variety of fields, including digital marketing, influence analysis, disinformation detection, and electoral integrity.The Role
Graphika seeks a well-rounded, experienced Data Engineer to join our growing technology team.
Responsibilities and Duties
Build and manage services at scale
Help create and optimize large-scale batch and real-time data pipelines
Design and implement ETL processes through cloud based solutions (S3, Lambda, Kinesis)
Contribute to improving the quality of our data and data infrastructure
Candidate Profile
You have demonstrated the ability to build... deploy and maintain large-scale, data-driven solutions. You love to take on complex data-related problems. You think beyond just the task at hand to deeply understand the 'why' behind what you are doing. You can maintain a focus on shipping product, understanding that oftentimes done is better than perfect
Our engineers have deep knowledge and exercise a high degree of leadership in their daily work. You work with confidence and without ego. You have strongly-held, defensible ideas, and advocate for what you believe is right. You are also adept at identifying and evaluating trade-offs, willing to be proven wrong, and quick to walk through fire to support your fellow teammates.
Required Qualifications:
You are a well-rounded engineer with 3+ years of professional programming experience, Python preferred.
You have experience with the Python data science stack (numpy, pandas, matplotlib, sklearn, Jupyter, etc.)
You have knowledge of SQL and common relational database systems such as PostgreSQL and MySQL
You are excellent at debugging and optimizing code
You have a deep understanding of good software engineering practices, including version control, code reviews, testing, and refactoring.
Preferred Qualifications
You have experience with AWS services: S3, Lambda, Kinesis, SQS, etc
You have hands-on experience with Big Data technologies (e.g Spark)
You have experience working in a data-focused organization
You have experience with social media analysis
Education Requirements:Bachelor's degree or equivalent experience, preferably with a quantitative major
Benefits and Perks
Graphika offers a highly competitive, comprehensive benefits package for all full time employees, including but not limited to:
Outstanding health benefits for employees and their families including medical, dental, and vision
Flexible Spending Account (FSA) for medical and dependent care and pre-tax commuter benefit
Unlimited PTO with a required minimum as well as a flexible remote work policy
Generous company equity options",Graphika,"New York, NY",$91.6k–145k,Data Engineer
,"About this job

Location options: Remote

Technologies

amplitude, sql, tableau

Job description
RESPONSIBILITIES
As a Growth Senior Data Engineer / Analyst, you will help lead growth and customer engagement initiatives for our mobile products. You will be data-driven and results focused, thriving at the intersection of data, product, engineering, and marketing. The ideal candidate will have a strong technical, analytical, and product marketing background – with a motivation to use analytic insight to grow Xapo mobile customer base on a Global scale.
The successful candidate will have curiosity, a passion for data, and demonstrate data leadership to impact the business.
• Diligence, perseverance, and proactiveness are key to succeed in this position.
• Lead structure and implementation of metrics tracking systems, working directly with the Sr Director of Growth.
• Utilize Amplitude, Python, Tableau, Looker and SQL to pull data and provide key analytical insights.
• Report, monitor... track, and analyze traffic, revenue, and other KPIs.
• Identify product risks/opportunities and communicate them succinctly and effectively.
• Help product team set up A/B tests and perform statistical analysis on results to provide actionable insights.
• Build statistical models to predict interactions and segment users based on behaviour.
• Provide ad-hoc analytics support as needed, ranging from helping teams develop the question through tracking and implementation to analytics and insights.
• Collaborate with the Business Intelligence team on building shareable data tools, maintain documentation, and participate in deep-dives to understand drivers of success.
• Collaborate with multiple cross-functional teams and work on solutions which have a larger impact on Xapo business.

REQUIREMENTS
• 7 years’ experience in growth marketing, data analytics, or a related field;
• Proven experience of using successfully the analytic platform Amplitude.
• In-depth understanding of data structures and algorithms.
• Proven experience of having worked for a mobile app with large growth.
• Experience in designing and building dimensional data models to improve accessibility, efficiency, and quality of data.
• Strong analytical and communication skills.
• Experience developing, maintaining, automating, visualizing, analyzing, and communicating reporting to management.
• Bachelor’s Degree in computer science or a related field is preferred.
• Fluency in English written/spoken",Xapo,"New York, NY (+13 others)",,Senior Growth Data Engineer / Analyst
139.64999999999998,"GumGum’s Engineering team presides over an ad server that produces over 50 TB of new raw data and 100 billion events every day. Our engineers harness this vast system to provide granular ad reporting data to our internal teams and external customers. In order to help our company succeed, they write scalable, fault-tolerant code under tight deadlines.

GumGum leverages Machine Learning and AI to unlock the potential of online content. Our engineer continue to push the envelope using bleeding-edge technology including Spark, Tensorflow, Keras, Python, Java, MongoDB, Express, React, and Node. As a Data Engineer, you will be building and maintaining exciting systems, services and data tools. You’ll bring your experience with complex distributed systems, passion for performance and optimization, and ability to write highly scalable and fault tolerant code.

Responsibilities:
• Refining our data infrastructure technologies such as Kafka, Spark, Druid, Fluentd to support real time analysis... of data
• Own the core data pipelines and scale our data processing flow.
• Build scalable systems with various AWS & Big Data technologies, lead technical discussions, participate in code reviews, guide the team in engineering best practices.
• Must be able to write quality code and build secure, highly available systems.
• Work on GumGum’s proprietary Reporting Server
• Work on various reports using Groovy, SQL and Java
• Work on GumGum’s proprietary forecasting system
Requirements:
• At least 1 year of Apache Spark experience
• At least a Bachelor's degree in Computer Science or equivalent
• 3+ years of Software Engineering experience (Java/Scala/Python)
• Experience with large scale distributed real-time systems with tools such as AWS, Spark, Kafka, Hadoop
• Familiar with various AWS services, Serverless architecture and containers
• Experience with high volume, high availability production systems.
• Strong problem solving skills, strong verbal and written communication skills

Perks
• Strong medical, dental and vision plans
• Healthcare and dependent care FSA
• 401k plus match
• Pet wellness discount
• Commuter perks
• Generous paid parental leave
• Stock incentive program
• Unlimited paid time off and sick leave
• Fitness reimbursement
• Continuous learning and professional development
• Company-wide hackathons
• Stocked kitchen with plenty of snacks
• Dog-friendly office
• Located in hotbed of tech startups, just blocks from the beach!
• Plenty of gum",GumGum,"Santa Monica, CA",$90.6k–98.1k,Data Engineer
141.6,"Cherre is upending the real estate sector with a cutting edge data science approach to traditional real estate. We built a platform to enable data driven real estate investment. We provide both real time real estate valuation, analytics and information to help investors decide on property purchase opportunities. Over the past 3 years we have become the realtime backend to the largest MLS systems, insurance, finance approval systems. We are also known for cutting edge web applications which allow agents to design & prepare their marketing materials & portfolio & research from their browser, tablet and phone.

We are looking for Jr Data Engineers to join our team. You will ingest and integrate new datasets into our Core schema. This a critical activity as we add to the most comprehensive data set in Real Estate. You will interact work closely with the product organization, exercising your problem solving skills to ensure that customer interactions are efficient and data quality... continues to meet our industry leading benchmarks. Technologies you'll use/learn include Docker, Kubernetes, SQL, BigQuery, Python, Airflow all on Google Cloud Platform, along with our internal tools and frameworks.

You are
• Conversant in SQL
• Have basic Python or OO programming experience
• Passionate about Data Analysis

Bonus points experience
• prior Software Engineering
• Real Estate
• Data Visualization
• Management consulting
• Agile process

Our culture isn't for everyone and that's ok. Our goal is to find people who share our values and can enhance our environment by teaching us the best parts of what they've learned previously.

At the top of the mountain we are all snow leopards - Hunter S. Thompson

Note to Agencies: Please don't send us unsolicited resumes. If we don't have an agreement in place, we will consider that submission a donation. Thank you for understanding",Cherre,"New York, NY (+1 other)",$79.1k–125k,Jr Data Engineer
243.8,"About Foursquare:

Since our inception in 2009, Foursquare has been a leading force in changing how location information enriches our real-world and digital lives. As a location intelligence company, Foursquare is comprised of our thriving media and enterprise products, as well as two well-known consumer apps—Foursquare and Swarm. Our B2B offerings include the Pilgrim SDK, Places, Pinpoint and Attribution (for marketers), and Place Insights (for analysts, based on the world's largest foot traffic panel). With more than 200 people across our offices in New York, San Francisco, Chicago, and in sales offices around the globe, we’re dedicated to our trailblazing mission— to build the most trusted, independent platform for understanding how people move through the real world.

About this Position:

As a member of Foursquare's Places Engineering team you drive the quality and direction of Foursquare’s Places Database, both as a standalone product and as the dataset underpinning Foursquare’s... core visit detection technology, developer API, and other enterprise product offerings. We're passionate about tackling tough challenges in the location space and look for others who like to dive deep into code and help solve hard problems. You should be comfortable running with your own ideas and eager to learn new skills on an emerging platform. We use a variety of tools, technologies, and languages to build software ( Scala, Python, Spark, EMR, MongoDB, Pants, Thrift) but experience with equivalent ones will do just fine.

Join us and help bring our feature ideas (and your own!) off the whiteboard and into reality. As a Staff Engineer on the Places team, you will drive technical and architectural planning, research, implement, and deploy improvements in data collection, feature engineering and algorithmic optimization. While we’re not necessarily looking for a Machine Learning engineer (although that’s definitely a plus!), you would have the opportunity to work on building ML models and big data pipelines.

This role is open for hire in our New York City headquarters.
Qualifications:
• Degree in Computer Science, Computer Engineering, or Statistics or commensurate experience
• 6-8+ years of software engineering and/or team lead experience
• Proven track record of building and shipping large-scale engineering products
• Experience working with large, complex data sets from a variety of sources
• Ability to collaborate with a diverse set of engineers, data scientists and product managers
• Experience with functional and object-oriented programming, Scala a plus
• Experience with Databricks or Spark, EMR
• Effective communication skills (both written and verbal), comfortable presenting to a group
• Comfort in a fast-paced startup environment

Foursquare is proud to foster an inclusive environment that is free from discrimination. We strongly believe in order to build the best products, we need a diversity of perspectives and backgrounds. This leads to a more delightful experience for our users and team members. We value listening to every voice and we encourage everyone to come be a part of building a company and products we love.

Foursquare is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other characteristic protected by law",Foursquare,"New York, NY",$77.3k–333k,Staff Data Engineer
202.0,"Position Overview

Daily Harvest is a direct to consumer brand reimagining frozen food to solve a modern eating dilemma: the desire for convenience without nutritional compromise. We deliver single-serve cups of frozen ingredients (organic, whole ingredients and superfoods, with no preservatives or added sweeteners) that can be turned into crave-worthy eats in just seconds.

We are looking for a data engineer who is passionate about furthering this mission. As a key member of the Data team and our first devoted data engineer, you will be in charge of ensuring a reliable, central data warehouse. You will build and manage data pipelines from a wide variety of sources touching every part of the company. You’ll work in a modern data science stack including GCP/Bigquery, Python, and Looker that sits on top of our rich dataset. All while enjoying tasty smoothies, soups, and bowls!
Requirements:
• There is no minimum experience or education, but this role will require a firm grasp of... software engineering, intuition around data system design, and a broad understanding of modern data tools and services.
• Strong Python and SQL background.
• Understanding of CS fundamentals (data structures, algorithms), and how software scales.
• Ability to thrive in a fast-moving and constantly evolving high-growth environment

Good to Have:
• Looker modeling experience.
• Exposure to ETL tools (e.g. Alooma/Fivetran/Stitch), Google Cloud (Bigquery), Fullstory
• E-commerce experience, especially within a subscription company.

Benefits
• Unlimited Daily Harvest to keep you hustling, not hangry
• Access to everything we make (including recipes in development)
• A dynamic, ambitious, and fun work environment
• Competitive medical, dental, and vision benefits
• Flexible time-off policy

About Daily Harvest

Daily Harvest makes nourishing food built on fruits and vegetables accessible. We do this by delivering thoughtfully sourced, chef-crafted food to customers' doorsteps, all ready to enjoy in minutes. We’re on a mission to take care of food, so food can take care of you.

Since launching in 2015, Daily Harvest has been featured in national publications like Fast Company, The New York Times, and Refinery29, and garnered investments from celebrities like Gwyneth Paltrow, Serena Williams, and Bobby Flay.

Our team is collaborative, driven, and future-thinking. We're constantly learning, experimenting, and iterating, and celebrate failure just as much as success. We take risks, try new things, and we get things done. We love adaptogens and cruciferous vegetables but never say no to cake. Everything we do, we do in the service of our community.

At Daily Harvest, our mission is to take care of food, so that food can take care of you. And it wouldn't be possible without our team. We celebrate the unique POV that each person brings to the table and believe in a collaborative and inclusive environment. As an equal opportunity employer, we prohibit any unlawful discrimination on the basis of race, color, religion, military or veteran status, sex, gender, gender identity or expression, sexual orientation, national origin, age, disability or genetic information. These are our guiding principles and apply across all aspects of employment",Daily Harvest,"New York, NY",$114k–176k,Senior Data Engineer
186.0,"Data EngineerCrunchbase is the leading destination where you can discover innovative companies, connect with the people behind them, and uncover new opportunities. Our mission is to democratize the way innovators connect to opportunities, and over 50 million professionals--including entrepreneurs, investors, market researchers, and sales people--trust Crunchbase to inform their business decisions. Companies all over the world rely on Crunchbase to power their applications, making over one billion API calls on our platform each year.

Data Engineering at Crunchbase
Our data team is responsible for building and maintaining the infrastructure for our data needs. As Crunchbase grows, so too does the amount of data we process, the number of sources it comes from, and the number of ways that people want to slice and dice it. We are currently building out a robust pipeline for our core data and we’re continually expanding use cases for it, so it must be both scalable and flexible.

At its... core, Crunchbase is a data company, and data engineering is at the heart of our platform and will propel us into the future.
The responsibilities of data engineers at Crunchbase include:
• Architect and build new dimensional data models and schema designs to improve accessibility, efficiency, consistency and quality of both internal and production data
• Build, monitor, and maintain analytics and production data ETL pipelines
• Provide the foundation for a data-driven culture by empowering other engineers and the Product team to ask questions of the dataset in an easy, reliable way
• Enable data scientists to implement NLP and ML algorithms at scale, in fault-tolerant, highly available systems

Qualifications
• Solid understanding of computer science and software engineering fundamentals
• Motivated to participate in ongoing learning and growth through pair programming, code reviews, application of new technologies and best practices.
• Excellent verbal and written communication skills.
• Experience with Python

What Crunchbase offers
• Competitive salary and equity
• Generous Reimbursement policy for learning and development activities
• Daily catered lunches and plenty of snacks
• Fitness reimbursement (to work off the catered lunches)
• Flexible Paid Time Off (PTO)
• Volunteering Paid Time Off
• Incredible medical, vision and dental benefits for employees and their families
• Free One Medical Group membership for employees and their families
• 401(k) and Roth plans, and free annual financial adviser check-in
• Monthly commuting stipend
• Free Lyft rides anywhere in the Bay Area after late nights at the office
• Prime location in the Financial District of SF, near BART and Muni stops
• A team of creative, transparent entrepreneurs driven to accomplish our mission

Crunchbase does not discriminate on the basis of race, creed, color, ethnicity, national origin, religion, sex, sexual orientation, gender expression, age, height, weight, veteran status, military obligations, or marital status. Every day our team is honored to work with entrepreneurs and innovators from every corner of the globe, and we aim to build a team that reflects the diversity of our customers. Each individual at Crunchbase brings their own perspectives, work experiences, lifestyles, and cultures with them, and we believe that a more diverse team creates more innovative products, provides a better service to its customers, and helps us all grow and learn as individuals",CrunchBase,"San Francisco, CA",$104k–164k,Data Engineer
,"Summary:
The Senior Data Engineer will play a key role to enhance our a cutting-edge analytics platform for the commercial division of a fast-growing pharmaceutical company. The ideal candidate will have a knack for implementing flexible, robust solutions for self-service data analytics and reporting. They will use data modeling, processing & ETL, data management, and visualization techniques to implement and support solutions that provide data accuracy and ease of access. The right person for the job will have data engineering, modelling and integration experience, and a track record of applying their technical expertise to rapid solutioning of business challenges.

Responsibilities:

This role will be responsible for completing the following essential functions:

• Lead technical workstreams for continuing the build of our data platform (AWS) that his includes data mart, data lake and reporting solutions.

• Develop and maintain data models, reporting systems, data management... systems, dashboards.

• Manage and optimize processes for data intake, validation, visualization and flexibility.

• Support testing of deliverables and provide data analysis as needed.

• Anticipate future demands of initiatives related to this space and provide thoughtful input into planning and solutioning processes.
Requirements:

• Strong development skills in a cloud environment (preferably AWS), including Python, Tableau

• Strong SQL skills, with ability to perform effective querying involving multiple tables and subqueries.

• Hands-on and versatile team player that is comfortable under pressure. Ability to quickly translate thoughts into reality.

• Problem solving, quantitative and analytical abilities.

• Ability to communicate with technical and non-technical audiences

• Desired skills: Data analysis, Spark, RedShift, Informatica,

• Exposure to Artificial Intelligence & Machine Learning preferred

• Knowledge of syndicated Pharmaceutical datasets (e.g. IQVIA) preferred.

• 5+ years of experience, BI, Data Warehousing, Visualization. 1+ yr on AWS platform.

#LI-EG2
#dice

Known for its scientific and operational excellence, Regeneron is a leading science-based biopharmaceutical company that discovers, invents, develops, manufactures, and commercializes medicines for the treatment of serious medical conditions. Regeneron commercializes medicines for eye diseases, high LDL-cholesterol, atopic dermatitis and a rare inflammatory condition and has product candidates in development in other areas of high unmet medical need, including rheumatoid arthritis, asthma, pain, cancer and infectious diseases.

This is an opportunity to join our select team that is already leading the way in the Pharmaceutical/Biotech industry. Apply today and learn more about Regeneron’s unwavering commitment to combining good science & good business.

To all agencies: Please, no phone calls or emails to any employee of Regeneron about this opening. All resumes submitted by search firms/employment agencies to any employee at Regeneron via-email, the internet or in any form and/or method will be deemed the sole property of Regeneron, unless such search firms/employment agencies were engaged by Regeneron for this position and a valid agreement with Regeneron is in place. In the event a candidate who was submitted outside of the Regeneron agency engagement process is hired, no fee or payment of any kind will be paid.

Regeneron is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability status, protected veteran status, or any other characteristic protected by law",Regeneron,"Sleepy Hollow, NY",,"Senior Data Engineer, Commercial IT"
,"Who We Are:

As data engineers in Revenue Science, our mission is to build real-time and offline solutions to make data accessible and reliable while leveraging the largest-scale data processing technologies in the world -- and then apply them to the Revenue’s most critical and fundamental data problems.

Learn more about some of the challenges we tackle on this team:
• Building a Petabyte-scale Data Warehouse (Google Cloud Next '18) https://youtu.be/APBF9Z3uBCc
• How Twitter Migrated its On-Prem Analytics to Google Cloud (Google Cloud Next '18)https://youtu.be/sitnQxyejUg

What You’ll Do:

As a member of the Data Engineering team, you will build and own mission-critical data pipelines that are ‘source of truth’ for Twitter’s fundamental revenue data, as well as modern data warehouse solutions, while collaborating closely with Ads Data Science team.

You will be a part of an early stage team and have a significant stake in defining its future with a considerable potential to impact... all of Twitter’s revenue and hundreds of millions of users.

You will be among the earliest adopters of bleeding-edge data technologies, working directly with Revenue Science and Revenue Platforms teams to integrate your services at scale.

Your efforts will reveal invaluable business and user insights, leveraging vast amounts of Twitter revenue data to fuel numerous Revenue teams including Ads Analytics, Ads Experience, Ads Data Science, Marketplace, Targeting, Prediction, and many others.

Who You Are:

You are passionate about data and driven to take the data organization challenges at the scope of entire Twitter’s Revenue.

What you’ll need:
• Strong programming and algorithmic skills
• Experience with data processing (such as Hadoop, Spark, Pig, Hive, MapReduce etc).
• Proficiency with SQL (Relational, Redshift, Hive, Presto, Vertica)

Nice to have:
• Experience writing Big Data pipelines, as well as custom or structured ETL, implementation and maintenance
• Experience with large-scale data warehousing architecture and data modeling
• Proficiency with Java, Scala, or Python
• Experience with GCP (BigQuery, BigTable, DataFlow)
• Experience with Druid or Apache Flink
• Experience with real-time streaming (Apache Kafka, Apache Beam, Heron, Spark Streaming)
• Ability in managing and communicating data warehouse project plans to internal clients",Twitter,"New York, NY",,"Cloud Data Engineer, Revenue Science"
221.5,"ABOUT THRIVE

Wholesome products at wholesale prices. Thrive Market is a membership e-commerce platform on a mission to make the world’s highest quality natural and organic products affordable for every American family. For $60 per year, Thrive members get access to their favorite healthy snacks, supplements, home, beauty, and baby products at 30-50% off retail pricing—all shipped to their front door. As part of our Thrive Gives initiative, each paid membership on the site also sponsors a free membership for a low-income family.

THE ROLE

As Thrive Market continues to grow at an incredible rate, you'll have the opportunity to build the data infrastructure at the heart of the business. At Thrive, we've created a transparent, data-centric environment from the beginning and we'll need your help building out our existing data warehouse to provide reporting and data for our Business Departments and Data Science team.
Responsibilities
• Own data instrumentation and data quality, and code... enhancements for the Thrive Market web site, APIs and backend platforms for data collection and data integrity.
• Participate in all aspects of a project life-cycle utilizing Scrum methodology
• Performance tuning of JSON APIs, and frontend frameworks.
• Works with dirty and messy data and leads instrumentation, logging and validation of data analytics across the site.
• Architect, Design, and Develop ETL processes between multiple systems using different tools
• Work with other Data Engineers and Analysts to meet business needs in a timely manner

Qualifications
• Bachelor’s degree required (computer science or related field), master’s a plus
• Proficiency in at least one high level programming language like Java / Scala / Python / C++
• Proficiency in a Data Visualization Tool (Tableau preferred) is a plus
• Experience with Apache Spark, Kafka, Elasticsearch, and large scale data analysis is a plus
• Ability to learn quickly and multi-task in a fast-paced, dynamic environment
• Experience with distributed systems and MapReduce architectures a plus
• Excellent communication skills, fun personality and a strong sense of curiosity
• Excellent analytical and problem-solving skills

We Offer
• Competitive Salary (DOE) + equity
• Flexible vacation time
• Free Thrive membership
• Medical, dental and vision plans to choose from
• Stocked kitchen and fridges with Thrive-type foods
• Catered lunches in the office
• Optional Yoga on Wednesdays, Kombucha Thursdays and weekly events
• Casual atmosphere and great people to work with

BELONG TO A BETTER MARKET

We're a community of more than 500,000 members who are united by a singular belief: It should be easy to find better products, support better brands, make better choices, and build a better world in the process. ",Thrive Market,"Los Angeles, CA",$143k–157k,Data Engineer
,"We are looking for an inspiring leader for our Data Insights Engineering team. The team is an entrepreneurial, customer-focused group of engineers and data scientists who drive new data capabilities across Flatiron Health. The team members are staffed cross-functionally, covering our entire product and platform portfolio.
Our Data Insights Engineers come from a wide range of backgrounds and experiences, including academia, tech companies, healthcare consulting, and their own startups, but they share a common passion and drive to solve the most impactful problems in cancer care and research.
As the VP of Data Insights, you will lead a diverse and talented team with expertise in digging into open-ended customer problems, prototyping, and rapidly iterating on data products from initial discovery to production. The team unlocks new value and possibilities from our data, always using the most effective tool for the job. Sometime this means an interactive visualization; sometimes, the... latest techniques in machine learning.
What you'll do:
Drive the strategic direction and priorities for Data Insights at Flatiron
Partner with senior leaders across Quantitative Sciences, Engineering, and Product to establish Flatiron Health as a top data science company in healthcare
Interact with customers, understand their needs to help drive our data products strategy
Immerse yourself in the business and technical context to help your team identify opportunities to unlock new value from our unique data sets
Attract, hire, and retain amazing people, and work with cross-functional leadership teams to deploy this talent to solve our most important problems
Mentor our team members to tackle bigger and harder problems; build a leadership bench to support significant team growth
Scale the culture and mandate of the function as we prepare to enter our next wave of rapid growth
Who you are:
You have experience leading rapidly growing data teams in an entrepreneurial environment, and have spent 10+ years solving technical problems
You are effective representing your company and your team to our most important customers and stakeholders
You are extremely thoughtful about culture and leadership, and you value diversity of thought and experiences
You are a brilliant communicator and a compelling storyteller, both verbally and in writing
You are a natural mentor. You have a passion for teaching those of various backgrounds and levels of experience
You are value-oriented and practical in how you apply data to problems and have empathy for what it means to build sustainable products at scale
You understand a broad spectrum of methods and disciplines in data science and analytics, and you are especially skilled in at least one of these fields: machine learning, data analytics, decision science, operations research, data modeling, business intelligence, or data engineering
You exemplify Flatiron values and are a cultural champion for them: you are vocally self-critical, willing to sit on the floor, and focused on solving problems that matter
Flatiron is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, creed, color, religion, gender, sexual orientation, gender identity/expression, national origin, disability, age, genetic information, or Veteran status. If you have a disability or special need that requires accommodation, please let us know",Flatiron Health,"New York, NY",,"VP, Data Insights Engineering"
160.5,"A career in our Analytics Technology practice, within Data and Analytics Technology services, will provide you with the opportunity to help organisations uncover enterprise insights and drive business results using smarter data analytics. We focus on a collection of organisational technology capabilities, including business intelligence, data management, and data assurance that help our clients drive innovation, growth, and change within their organisations in order to keep up with the changing nature of customers and technology. You’ll make impactful decisions by mixing mind and machine to leverage data, understand and navigate risk, and help our clients gain a competitive edge. Our team helps clients navigate various analytics applications to get the most value out of their technology investment and foster confidence in their business intelligence. As part of our team, you’ll help our clients implement enterprise content and data management applications that improve operational... effectiveness and provide impactful data analytics and insights.

Responsibilities

As a Senior Associate, you’ll work as part of a team of problem solvers with extensive consulting and industry experience, helping our clients solve their complex business issues from strategy to execution. Specific responsibilities include but are not limited to:
• Proactively assist in the management of several clients, while reporting to Managers and above
• Train and lead staff
• Establish effective working relationships directly with clients
• Contribute to the development of your own and team’s technical acumen
• Keep up to date with local and national business and economic issues
• Be actively involved in business development activities to help identify and research opportunities on new/existing clients
• Continue to develop internal relationships and your PwC brand

The candidate will use distributed computing technologies to process and analyze data at scale, identify and use appropriate big data tools to solve specific data processing problems, build pipelines to ingest and clean data using batch and real time frameworks, build infrastructural components and server applications to host machine learning models, explore and evaluate emerging frameworks, libraries and tech stacks, and configure, tune and monitor performance of big data systems.

Basic Qualifications

Job Requirements and Preferences:

Minimum Degree Required

Bachelor Degree

Required Fields Of Study

Computer and Information Science, Management Information Systems

Additional Educational Requirements

Other quantitative fields of study may be considered

Minimum Years Of Experience

3 year(s)

Preferred Knowledge/Skills

Preferred Qualifications:

Demonstrates thorough knowledge and/or a proven record of success in the following areas:
• Cloud computing platforms such as AWS, GCP and Azure;
• Relational databases and writing SQL queries;
• Big data machine learning toolkits such as SparkML, messaging systems (Kafka) and NoSQL databases (Cassandra, HBase, MongoDB); and,
• Building data lakes.

Demonstrates thorough abilities and/or a proven record of success as a team leader including the following areas:
• Background in computer science and comfortable in programming in a variety of languages, including Java, Python, Scala;
• Determining the appropriate software packages or modules to run, and how easily they can be modified;
• Handling large scale structured and unstructured data from internal and third party sources;
• Architecting highly scalable distributed data pipelines using open source tools and big data technologies such as Hadoop, Pig, Hive, Presto, Spark, Drill, Sqoop and ETL frameworks;
• Utilizing Linux shell scripting and containerization technologies (Docker, Kubernetes); and,
• Leading teams in a dynamic work environment while managing stakeholder expectations and scope.

All qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer",PwC,"New York, NY",$104k–113k,Data Engineer
164.1,"The Data Engineer is the universal translator between IT, business, software engineers, and Data Scientists, working directly with clients and project teams. S/he works to understand the business problem being solved and provides the data required to do so, delivering at the pace of the consulting teams and iterating data to ensure quality as understandings crystallize.

Our historical focus has been on high-performance SQL data marts for batch analytics, but we are now driving toward new data stores and cluster-based architectures to enable streaming analytics and scaling beyond our current terabyte-level capabilities. Your ability to tune high-performance pipelines will help us to rapidly deploy some of the latest machine learning frameworks and other advanced analytical techniques at scale.

You will serve as a keystone on our larger projects, enabling us to deliver solutions hand-in-hand with consultants, data science specialists, and software engineers.

Key Role... Attributes

Understand the overall problem being solved and what flows into it

Create and implement data engineering solutions using modern software engineering practices

Scale up from “laptop-scale” to “cluster scale” problems, in terms of both infrastructure and problem structure and technique

Deliver tangible value very rapidly, working with diverse teams of varying backgrounds

Codify best practices for future reuse in the form of accessible, reusable patterns, templates, and code bases

Requirements

Technical background in computer science, data science, machine learning, artificial intelligence, statistics or other quantitative and computational science

A compelling track record of designing and deploying large scale technical solutions, which deliver tangible, ongoing value

Direct experience having built and deployed complex production systems that implement modern, data scientific methods at scale and do so robustly

Comfort in environments where large projects are time-boxed and therefore consequential design decisions may need to be made and acted upon rapidly

Fluency with cluster computing environments and their associated technologies, and a deep understanding of how to balance computational considerations with theoretical properties of potential solutions

Ability to context-switch, to provide support to dispersed teams which may need an “expert hacker” to unblock an especially challenging technical obstacle

Demonstrated ability to deliver technical projects with a team, often working under tight time constraints to deliver value

An ‘engineering’ mindset, willing to make rapid, pragmatic decisions to improve performance, accelerate progress or magnify impact; recognizing that the ‘good’ is not the enemy of the ‘perfect’

Comfort with working with distributed teams on code-based deliverables, using version control systems and code reviews

Demonstrated expertise working with and maintaining open source data analysis platforms, including but not limited to:

Pandas, Scikit-Learn, Matplotlib, TensorFlow, Jupyter and other Python data tools

Spark (Scala and PySpark), HDFS, Kafka and other high volume data tools

SQL and NoSQL storage tools, such as MySQL, Postgres, Cassandra, MongoDB and ElasticSearch

Demonstrated fluency in modern programming languages for data science, covering a wide gamut from data storage and engineering frameworks through to machine learning libraries

Deep understanding of the architecture, performance characteristics and limitations of modern storage and computational frameworks, with experience implementing solutions that leverage: HDFS/Hive; Spark/MLlib; Kafka, etc.

A history of compelling side projects or contributions to the Open Source community is valued but not required

Willingness to travel as required for cases (up to ~50%)

Oliver Wyman is a global leader in management consulting. With offices in 50+ cities across 26 countries, Oliver Wyman combines deep industry knowledge with specialized expertise in strategy, operations, risk management, and organization transformation. Our 4700+ professionals help clients optimize their business, improve their operations and risk profile, and accelerate their organizational performance to seize the most attractive opportunities. Oliver Wyman’s thought leadership is evident in our agenda-setting books, white papers, research reports, and articles in the business press.

Our clients are the CEOs and executive teams of the top Global 1000 companies.

Visit Our Website For More Details About Oliver Wyman

www.oliverwyman.com

R_043800-en R_043800 R_043800-en",Oliver Wyman,"New York, NY",$91.6k–145k,Data Engineer
150.7,"THE BEST JOB YOU’VE EVER HAD

It’s not often you get the chance to change industries by bringing truly breakthrough technology to market – technology that not only disrupts existing markets but enables vast new ones. It’s hard and challenging work, but it’s also exciting, energizing, satisfying, and rewarding!

Echodyne’s Metamaterials Electronically Scanning Array (MESA) is changing the way people think about commercial radar. Built on decades of research, Echodyne brings high-performance radar to commercial markets and applications constrained by Cost, Size, Weight or Power. Drones, autonomous cars and trucks, heavy machines, even flying cars. To operate safely, these machines need sensors that work flawlessly across all environmental conditions. Echodyne has applied the physics of metamaterials to create a new category of sensor that provides the critical high-resolution, ground-truth data that autonomous machines and vehicles need to operate safely at any time and in any weather... Bringing this technology to market requires a team that works together.

Echodyne is seeking a Data Engineer with strong experience in relational databases and Python to join its team working in a fast-paced startup environment developing its next generation RADAR systems. The Data Engineer will work with world class scientists and will be responsible for helping to grow our data infrastructure, maintaining databases, creating and managing data pipelines, and building APIs for data consumption. In addition, this Data Engineer will maintain existing database systems and assist in the development of new ones. This position requires a person with high attention to detail, and who thrives in an interactive fast-design-cycle environment.

Echodyne is an equal opportunity employer. We want to build a team with a diverse mix of people whose common denominator is talent. We consider all applicants for employment without regard to race, religion, color, national origin, sex, sexual orientation, gender identity, age, veteran status or disability. If you have a disability or special need that requires accommodation, please do not hesitate to let us know.

Responsibilities:
• Operate under limited supervision to build, operate, and maintain database systems to track and improve the performance of our products and to provide training data for our machine learning systems.
• Make the data available to the rest of the team via APIs and web interfaces.
• Build tools to facilitate data cleaning, ingest, access, queries, and analysis.
• Use those tools to organize, clean, and ingest radar field test data into our databases in a timely manner.
• Perform ongoing semi-automated data analysis and batch processing on the data to generate radar-system performance reports.
• Assist in directing interns and junior staff in executing that data processing.
• Provide user support and database development support to other team members, writing documentation and interactively communicating with the team.
• Ensure data quality, integrity, and backups of our database systems.

Required skills / experience:
• Relational database familiarity and experience, strong knowledge of SQL.
• Knowledge of data-engineering best practices including error handling and logging, system monitoring, building human-fault-tolerant data pipelines, data cleaning, and database management.
• Experience with Python, Python environments, Git, Linux.
• Experience in website generation and management.

Desired skills / experience (looking for one or more as a complement to the core skills):
• Familiarity with PostgreSQL, Flask, Sqlalchemy, Javascript, HTML, Apache.
• NoSQL database experience or familiarity (such as MongoDB).
• Distributed-analytics/big-data frameworks (such as Spark or Hadoop).
• Cloud-based frameworks (such as AWS or GCP).
• Scientific programming experience.

A successful candidate should be good at:
• verbal and written communication,
• writing documentation,
• teamwork and leading interns and junior staff,
• attention to detail,
• performing independently with limited oversight,
• taking initiative,
• time-management and context switching,
• troubleshooting.

Qualifications:
• Bachelor’s or master’s degree in computer engineering, computer science, data science, or related field with 2-4 years’ experience in industry working with database systems.

Echodyne’s technology is export controlled by the U.S. Government and we must evaluate an applicant’s eligibility to handle export-controlled information or obtain required Government authorizations. Therefore, we will ask you as part of the application process to identify whether you are a U.S. Citizen or green card holder, or have asylum/refugee status in the U.S",Echodyne,"Kirkland, WA",$84.7k–132k,Data Engineer
472.5,"Namely’s mission is to help mid-sized companies build a better workplace. We’re an HR, payroll, and benefits platform that provides the technology, data, and support that HR professionals need and employees love to use. People are at the center of everything we do, and we believe every company and employee deserves a great workplace, supported by innovative HR technology. At Namely, we are problem solvers, self-starters, and obsessed with creating the best experience for our clients.

Do you have a passion for building data architectures that enable smooth and seamless product experiences? Are you an all-around data enthusiast with a knack for ETL? At Namely, we're hiring a Staff Data Engineer to help build and optimize the foundational architecture of our product's data.

We’ve built a strong data engineering team to date, but have a lot of work ahead of us, including:
• Migrating from relational databases to a streaming and big data architecture, including a complete overhaul of our... data feeds
• Defining streaming event data feeds required for real-time analytics and reporting
• Leveling up our platform, including enhancing our automation, test coverage, observability, alerting, and performance
As a Staff Data Engineer, you will work with the development team to construct a data streaming platform and data warehouse that serves as the data foundations for the Namely product.

Help us scale our business to meet the needs of our growing customer base and develop new products on the Namely platform. You'll be a critical part of our growing company, working on a cross-functional team to implement best practices in technology, architecture, and process. You’ll have the chance to work in an open and collaborative environment, receive hands-on mentorship and have ample opportunities to grow and accelerate your career!

Responsibilities
• Build our next generation data warehouse
• Build our event stream platform
• Translate user requirements for reporting and analysis into actionable deliverables
• Enhance automation, operation, and expansion of real-time and batch data environment
• Manage numerous projects in an ever-changing work environment
• Extract, transform, and load complex data into the data warehouse using cutting-edge technologies
• Build processes for topnotch security, performance, reliability, and accuracy
• Provide mentorship to fellow team members

Requirements
• Bachelor’s or Master’s degree in Computer Science, Information Systems, Operations Research, or related field required
• 10+ years of experience building data pipelines
• 4+ years of experience building data frameworks for unit testing, data lineage tracking, and automation
• Fluency in Scala and at least one other server-side programming languages (e.g. Python, Java, Go)
• Proficient on Apache Spark
• Familiarity with streaming technologies (e.g., Kafka, Kinesis, Flink)
Nice to Have
• Familiarity with Machine Learning
• Familiarity with Looker a plus

About Namely

Namely was founded in 2012 to create an HR platform as intuitive as social media, but powerful enough to support the complexity of today’s workforce. Our belief is that great companies are built on a great employee experience, which is why we created the first HR platform employees love to use. In fact, unlike most traditional HR software, 78% of our clients’ employees log in to Namely at least once per month! Namely is backed by some amazing VCs including Sequoia, and serves companies in just about every industry and state nationwide. We love mid-sized companies because they’re mission-driven, client-obsessed, and care deeply about their employees... just like us. We believe in giving you the tools you need to do the best work of your career, and we’re just getting started.

We invite you to fill out the EEO survey below as part of our ongoing diversity initiatives at Namely. Your participation in the survey is completely optional and voluntary, and none of the information you provide will be considered in the hiring process or with respect to any employment decision made by Namely. Namely will have access only to anonymized data submitted through these surveys",Namely,"New York, NY",$315k–315k,Staff Data Engineer
226.5,"At Medium, words matter. We are building the best place for reading and writing on the internet—a place where today’s smartest writers, thinkers, experts, and storytellers can share big, interesting ideas; a place where ideas are judged on the value they provide to readers, not the fleeting attention they can attract for advertisers.

We are looking for a Senior Data Engineer that will design, build, ship and maintain our business critical Data Platform. In this role you will lead development of both transactional and data warehouse designs mentoring our team of cross functional engineers and Data Scientists. You'll also design, implement and tune tables, queries, stored procedures, and indexes.

At Medium, we are proud of our product, our team, and our culture. Medium’s website and mobile apps are accessed by millions of users each day. Our mission is to move thinking forward by providing a place where individuals, along with publishers, can share stories and their perspectives... Behind this beautifully-crafted platform is our engineering team who works seamlessly together. From frontend to API, from data collection to product science, Medium engineers work multi-functionally with open communication and feedback.
What Will You Do
• You’ll work on high impact projects that improve data availability and quality, and provide reliable access to data for the rest of the business
• Design, architect and support new and existing data and ETL pipelines and recommend improvements and modifications.
• Create optimal data pipeline architecture and systems.
• Assemble large, complex data sets that meet functional and non-functional business requirements.
• Be responsible for ingesting data into our data warehouse and providing frameworks and services for operating on that data including the use of Spark.
• Analyze, debug and correct issues with data pipelines
• Communicate strategies and processes around data modeling and architecture to multi-functional groups and senior level management.
• Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
• Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, Spark and AWS technologies.
• You will build widely used data pipelines and tools making critical business data available to other teams.

About You
• You have at least 5 years of experience implementing complex ETL pipelines preferably in connection with Hadoop or Spark.
• You have lots of experience writing complex SQL and ETL processes
• You have exceptional coding and design skills, particularly in Java/Scala and Python.
• You've worked with large data volumes, including processing, transforming and transporting large-scale data
• You have hands-on experience with AWS and services like EC2, SQS, SNS, RDS, Cache etc.
• You have a BS in Computer Science / Software Engineering or equivalent experience.
• You have knowledge of Apache Hadoop, Apache Spark (including pyspark), Spark streaming, Kafka, Scala, Python, and similar technology stacks
• You have a strong understanding & usage of algorithms and data structures.

Nice To Have
• Spark data pipeline and or streaming experience
• Redshift knowledge and operational experience
• Machine Learning expertise

At Medium, we foster an inclusive, supportive, fun yet challenging team environment. We value having a team that is made up of a diverse set of backgrounds and respect the healthy expression of diverse opinions. We embrace experimentation and the examination of all kinds of ideas through reasoning and testing. Come join us as we continue to change the world of digital media. Medium is an equal opportunity employer.

Interested? We'd love to hear from you",Medium,"San Francisco, CA",$128k–197k,Senior Data Engineer
,"Data Engineering and InfrastructureNetflix makes up 1/3 of internet traffic, and we're proud to deliver entertainment that over 125 million global customers enjoy. Behind the scenes, Netflix is delivered by Open Connect, a custom-built content delivery network that connects our content to thousands of ISPs around the world.

Decisions on how to improve streaming performance for our customers and evolve Open Connect's architecture are driven by data. We're looking for someone to transform petabytes of incoming data on video performance and network efficiency into well-designed, high-quality data structures that empower critical decision-making for teams within Netflix.

We track every customer action and each byte of data transferred, so you'll work with data at incredible scale and collaborate with best-in-class data engineers and analytic experts. You'll become an authority in the world of video streaming delivery (no prior knowledge necessary, but curiosity to learn is a must), and... the projects you'll work on will be truly impactful.

In the meantime, learn more about the Streaming Data Engineering team.

What you'll do
• You’ll take ownership and increase automation and scale of complex data sets that drive use cases by our analytical partners such as hardware capacity planning and failure prediction, understanding network topology and forecasting network traffic flow, and monitoring/improving efficiency of deployment and turnover of Netflix-encoded assets to the network.
• You’ll build robust data pipelines of high data quality in a scalable fashion (both data and maintainability).
• Every video or audio file you stream from Netflix started as a file living on an editor’s hard drive and became a Netflix-encoded asset sitting on a server ready to be played. You’ll create the data pipelines that will let us quantify and understand that life cycle by merging system activity and customer behavior.
• We need to process data more quickly than ever to enable rapid experimentation in an increasingly nimble engineering organization. You’ll help implement our business logic to be compatible with real-time/stream processing frameworks.
Who you are
• Have several of the characteristics/skills listed below and have passion and self-drive to quickly learn in areas of less familiarity. We believe the experience in your years is more important than your years of experience.
• Enjoy a high level of autonomy in managing cross-functional engineering projects. We enjoy a culture of Freedom & Responsibility.
• Have experience building production data pipelines using one or more frameworks such as Spark, Flink or Hive/Hadoop. Have hands on experience with schema design and data modeling.
• Have programming proficiency in at least one major language such as Java, Scala or Python. You have a software engineering mindset and strive to write elegant, maintainable code and you're comfortable working in a variety of tech stacks. You may even be a software engineer with a focus or passion for data-driven solutions.
• Have strong SQL skills and knowledge and familiarity with other distributed data stores such as ElasticSearch or Druid.
• Have excellent communication in sharing context to effectively collaborate with analytical partners, domain experts and other consumers of your work, preferably in supporting an engineering or product function. We like to collaborate across teams and so should you.
• Ambitious and willing to take action, but not stubborn. Awareness to recognize when you're wrong and move past your own mistakes. We are humbly confident in ourselves and our work.
Netflix Culture

Our culture is unique, and we live by our values. You will need to be comfortable working in the most agile of environments. Requirements will be vague, and iterations will be rapid. You will need to be nimble and take smart risks. Learn more about Netflix’s culture",Netflix,"Los Gatos, CA",,Engineer - Content Delivery Network
207.5,"At Addepar, we believe better technology can transform global finance. Our mission is to empower investors to analyze portfolios, communicate, and act with unparalleled insights. To tackle this ambitious goal, we have built a team of technologists fundamentally rethinking how tools in finance should work.
As an experienced Data Engineer at Addepar, you will own the design, development and maintenance of Addepar’s data warehouse that will enable us to make effective data-informed decisions across various business disciplines. You will lead initiatives to formalize data governance and management practices, rationalize our information lifecycle and key company metrics, mapping the metrics to Addepar’s data warehouse and the various data sources feeding into it. In this role, you will have the opportunity to interact with different functional areas within the business and influence decision-making in a fast-paced, high-growth FinTech company.

Our ideal candidate will have a proven track... record of building data solutions that assist product and business decisions. You will provide mentorship and hands-on technical support to build trusted and reliable domain-specific datasets and metrics. You love building elegant and efficient data solutions and want to make an immediate impact on a fast-growing company. You are a self-starter, detail and quality oriented, passionate about having a huge impact at Addepar. You have deep technical skills and are comfortable working with data of all shapes and sizes needed to build a strong data foundation for Addepar. You can take broad organizational goals and turn them into concise, effective deliverables. You will help set the vision for how we use data at Addepar and empower others in contributing toward that vision.
Responsibilities
• Manage Addepar’s internal data warehouse that empowers our product and business leaders to make data-based business decisions
• Translate business requirements into data models that are easy to understand and used by different business functions across Addepar.
• Partner with business domain experts, product & business analysts, product managers and engineering teams to build foundational data sets that are trusted, well understood, aligned with business strategy and enable self-service analytics
• Partner with BI Engineering to develop core data sets that empower operational and exploratory analysis as well as canned BI products
• Design, implement and build pipelines that deliver new data with measurable quality under the SLA
• Be a champion of the overall strategy for data governance, security, privacy, quality and retention that will satisfy business policies and requirements
• Support existing processes running in production
• Own and document foundational company metrics with a clear definition and data lineage
• Define and manage SLA for all data sets in Addepar’s business/internal data warehouse.
• Work with data infrastructure to triage infra issues and drive to resolution
• Identify, document and promote best practices

Requirements
• BS/MS degree in Computer Science, Engineering or related field, or equivalent training, or work experience
• 5+ years of experience working in data architecture, data modeling, master data management, metadata management
• A consistent track record of close collaboration with business partners and crafting data solutions to meet their needs
• Very strong experience in scaling and optimizing schemas, performance tuning SQL and ETL pipelines in OLTP, OLAP and Data Warehouse environments
• Deep understanding of relational as well as NoSQL data stores, methods and approaches (logging, columnar, star and snowflake, dimensional modeling)
• Hands-on experience with Big Data technologies (e.g. Hadoop, Hive, Spark)
• Proficiency with object-oriented and/or functional programming languages is a big plus (e.g. Java, Scala, Python, Go)
• Excellent written and verbal communication and interpersonal skills, able to effectively collaborate with technical and business partners
• Excellent understanding of trade-offs
• Demonstrated ability to navigate between big-picture and implementation details
• Ability to initiate and drive projects to completion with minimal guidance
• Good to have: Product Analytics, Business & Operations Analytics and previous experience in investment management software

Addepar is a leading provider of technology for the wealth management industry. The company’s performance reporting and analytics platform aggregates portfolio, market, and client data all in one place. It provides asset owners and advisors a clearer financial picture at every level, allowing them to make more informed and timely investment decisions. Addepar works with leading financial advisors, family offices, and large financial institutions that manage over $1.3 trillion of assets on the company’s platform. In 2018, Addepar was named as a Forbes Fintech 50 and received Morgan Stanley's Fintech Award for making significant impact to the firm’s mission of continuous innovation. Addepar is headquartered in Silicon Valley and has offices in New York City, Chicago, and Salt Lake City.

At Addepar, we rely on a range of backgrounds, experiences, and ideas. We value diversity, and we’re proud to be an inclusive, equal opportunity workplace",Addepar,"Sunnyvale, CA",$111k–193k,Data Engineer
202.0,"OVERVIEW

We are a global leader in entertainment media seeking a Data Engineering Manager to lead the architecture, design and implementation of projects focused on collecting, aggregating, storing, reconciling and making data accessible from disparate sources to enable analysis and decision making. This individual will collaborate within a team of technologists to produce enterprise scale solutions to meet our clients’ needs.

In this role, responsibilities will touch on all stages of the software engineering lifecycle: understanding customer requirements, designing the application, reviewing code, and monitoring the system for issues.

There will also be an opportunity to exercise highly developed collaboration abilities interacting with other technology teams on broader cross-organization initiatives. We work with the latest cloud, big data and open-source technologies and you play a leading role in selecting the best tools and platforms to help takee us into the future.

PRIMARY... RESPONSIBILITIES
• Be a thought leader across the Data Engineering & Services organization to build new and improved data tools and services that can scale with the company.
• Take loosely defined business questions and translate them into clearly defined technical/data specifications for implementation.
• Write complex ETL (Extract / Transform / Load) processes, design and develop solutions for real-time and offline analytic processing on AWS-hosted Data and Analytics Platform.
• Ensure that data pipelines are scalable, repeatable and secure.
• Ensure all deliverables are of high quality throughout the project by adhering to coding standards and best practices and participating in code reviews.
• Implement activities that generally impact multiple components / processes and the work of own and possibly other units / teams / projects.
• Interacts with internal and external peers and management to share highly complex information related to areas of expertise and/or to gain acceptance of new or enhanced technology / business solutions.
• Mentor less experienced members of the team",C2R Ventures,"New York, NY",$114k–176k,Senior Data Engineer
210.0,"At Datadog, we’re on a mission to build the best monitoring platform in the world. We operate at high scale—trillions of data points per day—and high availability, providing always-on alerting, visualization, and tracing for our customers' infrastructure and applications around the globe.

If you’re excited to work on a fast-moving data engineering team with the best open-source data tools at high scale, we want to meet you.

What You Will Do
• Build distributed, high-volume data pipelines that power the core Datadog product
• Do it with Spark, Luigi, Kafka and other open-source technologies
• Work all over the stack, moving fluidly between programming languages: Scala, Java, Python, Go, and more
• Join a tightly knit team solving hard problems the right way
• Own meaningful parts of our service, have an impact, grow with the company

What We're Looking For
• You have a BS/MS/PhD in a scientific field or equivalent experience
• You have built and operated data pipelines for real... customers in production systems
• You are fluent in several programming languages (JVM & otherwise)
• You enjoy wrangling huge amounts of data and exploring new data sets
• You value code simplicity and performance
• You want to work in a fast, high growth startup environment that respects its engineers and customers
Bonus Points
• You are deeply familiar with Spark and/or Hadoop
• In addition to data pipelines, you’re also quite good with Chef or Puppet
• You’ve built applications that run on AWS
• You’ve built your own data pipelines from scratch, know what goes wrong, and have ideas for how to fix it
Is this you? Send your resume and link to your GitHub if available",Datadog,"New York, NY",$135k–150k,Software Engineer - Data Engineering
180.6,"Enterprising – and not defined by conventional roles. Our goal is to innovate, and therefore we welcome diverse skills, experiences, and thinking. In Global Data’s Technical Operations, our focus is on data engineering and how data is acquired, processed and validated.
This is not your typical Data Engineering position.

We are looking for a Data Engineering Tech Lead to own the technical direction of our team from the ground up. You will be a developer, an architect and a thought-leader of our team focused on system integrations and data enrichment. Your projects will be focused on evolving our data processing capabilities to allow us to onboard and manage data efficiently and accurately. You’ll collaborate with engineers, data analysts and business stakeholders to solve large-scale problems.
We’ll trust you to:
• Lead the development of systems that span across open source, proprietary tools and legacy applications.
• Build reusable client libraries for interacting with internal... APIs and systems
• Design data driven processes leveraging enterprise-wide ML and NLP solutions for information extraction and anomaly detection.
• Engage with the broader technical community within Bloomberg and represent Global Data at industry engagements including conferences, meetups, hackathons, blogs and written articlesAs a leader, we expect you to:
• Facilitate collaboration and provide strategic leadership from single component design to the overall architecture and strategy of our technology stack
• Provide technical guidance and mentorship to junior team members, leading by example and promoting best software development practices such as continuous integration and deployment
• Communicate and maintain relationships with non-technical business stakeholders throughout development lifecycleYou’ll need to have:
• A BA/BS degree or higher in Engineering, Information Systems, Mathematics, or relevant data technology field
• A proven passion for teaching and motivating
• Demonstrated history of collaborating across varied groups of people, including both technical and non-technical audiences
• Previous engagement experience within the industry and open source community
• You have 2-5 years of professional Python development experience and familiarity with distributed systems, REST APIs, databases and linux",Bloomberg LP,"Princeton, NJ",$89.1k–183k,Data Engineer - Tech Lead
203.0,"The Bloomberg Global Data Alternative Data team seeks an experienced Data Engineer to design, develop, and deploy foundational data processing systems. In this role you will work with a range of non-financial data sets as well as proprietary and open source technologies. As our first hire your interests and enthusiasm will have a significant impact on the strategy and direction of our team.

In the short run, the primary need of the GDAD team is to develop ETL pipelines. Candidates must have demonstrated experience with modern techniques for data acquisition, extraction, normalization, validation, enrichment, and storage.

In the long run, the GDAD team will expand to include originating alternative data (mainly from government and web sources), creating primary research (mainly through survey methods), and supplementing data products with analytical products (mainly by generating systematic signals and ad hoc analysis to support systematic and discretionary investors, respectively... This role can be in either New York, NY or Princeton, NJ.

We expect you to:
• Have experience with data engineering best practices. Professional experience in a data-oriented role is the best way to demonstrate this.
• Have experience with standard software engineering methodologies. We value code quality and strive to develop reliable, maintainable systems.
• Have strong programming experience. We develop primarily in Python and will expect you to do so as well. You should have experience munging and pipelining data, querying databases, and monitoring and troubleshooting systems.
• Communicate effectively with non-technical partners at all stages of a project, from sourcing ideas and figuring out requirements to explaining methodological decisions and implementation details.
• Acquire subject-matter expertise as needed in order to deliver data engineering projects. For example, you might shadow data specialists to learn about their product and workflows.

We'd love to see any of the following:
• Experience working in a large organization. We take care of messy data from disparate sources and legacy systems with limited documentation.
• Experience with financial data or alternative data, preferably from a quantitative role in investment finance or at an alternative data provider.
• Formal training in statistics, machine learning, or software engineering.

Note that we are unable to sponsor candidates requiring visas.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status",Bloomberg LP,"New York, NY",$132k–142k,Data Engineer
164.1,"In addition to advocating the benefits of using data for business insights and transforming attitudes towards using data to drive better business decisions within GBM, objectives 1-4 are the key responsibility of this new role which consists of implementing four types of analytical principles for prioritized business challenges;
• Descriptive
• Diagnostic
• Predictive
• Prescriptive

This advertised role is key in the overall Data Science team with a reporting line to the Head of MI & Anlaytics. It is therefore essential that the incumbent has a broad view of the latest and most effective tools and techniques to execute scalable actionable insights as well as a good understanding of the Hadoop ecosystem architecture.

The main focus of the role is on the provision of advice and guidance on implementation of the Data Science Strategy and on implementation of prioritized business challenges using one or more analytics principles. Fundamental to the success of this role will be the... ability to deliver business benefit through build and change analytic solutions / models / dashboards / reports within the Data Science Operating Model

Customers / Stakeholders

Senior business heads (Markets, Banking, Operations, IT)

Leadership & Teamwork
• Motivate and collaborate with the data science team.
• Establishing and engaging a trusted network of data practitioners/champions across GBM

Operational Effectiveness & Control
• Identification and escalation of weaknesses in the accuracy of regulatory reporting
• Implementation of a risk based approach in the provision of advice and guidance
• Adherence to project & program life-cycles with appropriate quality gates that enforce best data practice

Major Challenges
• Understanding complex regulatory reform across a range of GBM businesses including front-office
• Articulating complex data issues in a simple way to senior stakeholders
• Sensitivity to regulators and HSBC’s stance to the deferred prosecution agreement
• Timely implementation of the Data Science Strategy Framework in tight fiscal circumstances

Management of Risk
• Management of data security
• Identification of weaknesses in regulatory reporting
• Appropriate controls and discipline for aspects of data change within Projects and Programs
• Management of controls around data access and processing of high risk information

Observation of Internal Controls
• Adherence to ITSR and data security
• Data Sharing and production controls

Suitable candidate will be heavily involved in the developing Big-data driven Application-Framework with strong focus on interaction with underlying systems, building data assets and extraction of critical data elements to cater key business requirements.

Strong Knowledge of
• Python, Scala, SPARK
• Unix - Shell Scripting, SED, AWK (etc.) for tooling. Ability to develop/automate repetitive jobs.
• Hadoop knowledge: HDFS, Hive, SQOOP etc. should be familiar with File systems of both UNIX and Hadoop. Experience in dealing with large data sets
• SQL

Good with
• Experience in Elastic Search and Indexing
• Control-m (Job Schedulers)
• GitHub (Other repositories)
• Issue tracking & documentation tools - Confluence and JIRA

Preferable but not mandatory
• Workflow Automation (e.g Drake) tools knowledge is a plus but not required
• Financial systems Knowledge is a plus - Reference & transaction Systems

Others Qualities
• Knowledgeable in various aspects of software engineering, inferring requirement documentations, data modelling and object oriented programming frameworks is a huge plus
• Strong Interpersonal skills, ability to ""learn & evolve"" and passionate to learn and deliver
• Experience in implementing Data Science
• Exposure to the financial & banking sector across combinations of client onboarding, risk functions, operations, Client MI, Regulatory data change, data quality, front office operations
• Exposure to Global Banking & Markets
• Exposure to formal Data Science process lifecycles and methodologies (e.g. CRISP-DM, KDD, TDSP)

Job Field: Information Technology

Primary Location: North America-United States-New York-NEW YORK

Schedule: Full-time

Type of Vacancy: Country vacancy

Job Posting: 15-Apr-2019, 20:05:42",HSBC,"New York, NY",$91.6k–145k,Data Engineer
160.5,"Sony Music Entertainment is a global recorded music company with a roster of current artists that includes a broad array of both local artists and international superstars, as well as a vast catalog that comprises some of the most important recordings in history. Sony Music Entertainment is a wholly owned subsidiary of Sony Corporation of America.

Sony Music Entertainment is undergoing a huge transformation driven by Data, Analytics, and Cognitive technologies. We have created a new team, lead by our CDO, focused on advancing Sony Music’s data strategy & capabilities around the world.

The Teams Goals Are To

Using data-centric strategies, with design and technology principles in mind, we are developing products, tools, apps, and more that deliver actionable insights that help shape strategic thinking and drive business solutions for our labels.
• Provide business intelligence and technology solutions for use across the company’s labels and business units
• Create predictive tools... and analytics platforms to enhance Sony Music’s capabilities in artist and partner discovery, solutions, commercial strategy, marketing, and A&R
• Enhance service to Sony Music’s artists and business partners
• Grow commercial opportunities
The Data Engineer will join the newly created Data Strategy team and will report to the EVP of Data Strategy, Chief Data Officer. This role will explore and build products using the latest and greatest in Data Analytics, Cognitive Services, Machine Learning and more. This role is based at Sony Music’s offices in New York.

Sony Music is committed to providing equal employment opportunity for all persons regardless of age, disability, national origin, race, color, religion, sex, sexual orientation, gender, gender identity or expression, pregnancy, veteran or military status, marital and civil partnership/union status, alienage or citizenship status, creed, genetic information or any other status protected by applicable federal, state, or local law.
• Create and maintain systems to load and transform very large data sets from digital media retailers (iTunes, Spotify, YouTube, etc) as well as social media sources.
• Work with a cross-functional team to create data-driven insights and reports for business stakeholders.
• Work with the Software Engineering team to create customer-facing analytics tools and visualizations.
• Process millions of rows of data daily to provide analytics to our end users.
• Take advantage of our continuous integration and deployment.
• Participate in technical design and peer review for new projects.
• Experience using Snowflake and Google Cloud Platform preferred.
• Experience with AWS ecosystem. Some preferred services are Redshift, RDS, S3, and SWF.
• Proven experience with ETL frameworks (Airflow, Luigi, or our own open sourced garcon ).
• Expertise with at least one distributed data stores (Redshift, Cassandra, Snowflake).
• Familiarity with noSQL technologies (mongoDB, DynamoDB).
• Proficient in scripting language of choice. Python is strongly preferred, PHP a plus.
• Highly proficient in writing SQL for a relational datastore (MySQL, PostgreSQL).
• Knowledge of technologies that can deal with Big Data is a Big Plus (Kafka, Spark, Hive, Hadoop/MapReduce).
• Ability to write automated tests (unit, functional, and integration) to ensure code works as expected.
• Desire to collaborate with other engineers through peer code reviews.
• Deep understanding of data structures and schema design.
• Detail-oriented, proactive problem solving skills",Sony Music Entertainment,"New York, NY",$104k–113k,Data Engineer
,"Consumer and Investment Management (CIMD)

The Consumer and Investment Management Division includes Goldman Sachs Asset Management (GSAM), Private Wealth Management (PWM) and our Consumer business (Marcus by Goldman Sachs). We provide asset management, wealth management and banking expertise to consumers and institutions around the world. CIMD partners with various teams across the firm to help individuals and institutions navigate changing markets and take control of their financial lives.

Consumer

Consumer, externally known as Marcus by Goldman Sachs, is comprised of the firm’s digitally-led consumer businesses, which include our deposits and lending businesses, as well as our personal financial management app, Clarity Money. Consumer combines the strength and heritage of a 150-year-old financial institution with the agility and entrepreneurial spirit of a tech start-up. Through the use of machine learning and intuitive design, we provide customers with powerful tools that are... grounded in value, transparency and simplicity to help them make smarter decisions about their money.

Your Impact

Clarity Money is a business recently acquired by Goldman Sachs that offers a revolutionary AI- and machine learning-based product for consumers to improve their financial health. Ushering in a new era of mobile personal finance management apps, Clarity Money uses artificial intelligence and data science to help consumers make smarter financial decisions and get the most from their money. The revolutionary features allow users to cancel bills, get a better credit card and create a savings account, all from within the app, and all at the push of a button.

The Clarity Money team is actively seeking a motivated Data Engineer with proven industry experience in machine learning, data engineering and building robust data pipelines. Strong computer science fundamentals are key to success in this role. The ideal candidate should be an individual contributor, a positive team player and willing to get things done.

Required Qualifications

RESPONSIBILITIES AND QUALIFICATIONS
• 5+ years of relevant industry experience.
• Solid engineering and programming skills. Ability to write high performance production quality code with Python and/or Scala.
• Experience building efficient large-scale data pipelines and data warehousing solutions.
• Strong SQL and NoSQL skills.
• Knowledge about Docker, CircleCI, Jenkins, testing, version control systems is required.
• Proficiency in building data pipeline from scratch and deploying them on cloud environment (AWS or similar).
• Proven experience in processing large sets of data in batch and real time fashion using Hadoop, MapR, Spark, Kafka, Kinesis etc.
• Ability to engage with other teams to design the optimal data architecture for a new scale platform and infrastructure to support data and analytics-based product innovation.

Preferred Qualifications
• Proficiency in probability, statistics, feature engineering and exploratory data analysis and extracting insights using Python, R or similar tools. Eager to get your hands dirty.
• Experience with consumer products/services (including but not limited to: consumer lending, mobile banking applications, deposits etc.).
• Experience with deploying, maintaining and iterating over large-scale machine learning systems to drive improvement of current products and the development of novel features and products within the company’s vision.
• Experience with data visualization libraries like seaborn, D3Js or tools like Tableau.
• Self-starter and curious.

About Goldman Sachs

The Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world.

© The Goldman Sachs Group, Inc., 2018. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.

Division Consumer and Investment Management Division",Goldman Sachs,"New York, NY",,CIMD - Personal Financial Management - Data Engineer
159.4,"At WeWork, data sits in the center of our business, providing insights into the effectiveness of our physical and digital product & features. We believe data brings everything together and it is the only way we make decisions.

Data Engineering is a team constantly striving to create an amazing experience for our customers and internal teams. We regard culture and trust highly and believe you will have positive influence in your own way as a data engineer.

We're looking for a Senior Data Engineer with extensive experience designing, implementing data solutions for various business challenges, and leading database and data warehousing initiatives.
• You’re excited to solve data challenges that combines digital and physical aspects, across multiple business verticals like Sales & Marketing, Social Media
• You're motivated to enable and collaboration with engineering, analytics and product teams to tackle the most challenging business needs
As a Data Engineer at WeWork, you will be a... part of an early stage team and work with passionate leaders on highly impactful business challenges.

Responsibilities
• Leverage data expertise to help evolve data models in various components of the data stack
• Work on architecting, building, and launching highly scalable and reliable data pipelines to support WeWork's exponential data growth
• Collaborate closely with analysts and data scientists, as well as cross-functional teams, to leverage huge amounts of WeWork data, for data-driven business and user behavior insights
• Consistently evolve data model & data schema based on business and engineering needs
• Implement systems tracking data quality and consistency
• Develop tools supporting self-service data pipeline management (ETL)

Requirements
• Strong communication skills, empathy and initiative
• 5+ years of experience in data engineering
• Strong background in data warehouse concept and design
• Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle)
• Good understanding of SQL Engine and able to conduct advanced performance tuning
• Familiar with at least one scripting language (Python, Ruby, Perl, Bash)
• Experience with Git and the pull request workflow
• Experience working closely with Analytics/Data Science teams
• Experience with modern BI tools (Looker, Tableau, etc)
Nice to have
• Experience leading / managing full project lifecycles
• Experience with Salesforce and related tools
We are an equal opportunity employer and value diversity in our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status",WeWork,"New York, NY",$82.4k–154k,Data Engineer - Modeling
202.0,"Do you want to work on cutting-edge technologies like Relational and NoSQL data stores, Big Data and Advanced Analytics? If any of this interests you, then read on to learn more about the AzureCAT Data team, and how you can contribute to the excitement of data and advanced analytics at Microsoft!

Responsibilities

The Azure Customer Advisory Engineering team (Azure CAT) has a mission to accelerate customer success on Azure and to drive platform improvements. AzureCAT’s Data team is looking for an experienced, customer-focused Senior Data Engineer to help expand our data and analytics programs. In this role, you'll work closely with some of Microsoft's largest and most strategic customers to help them identify, define, and architect solutions on Azure. You'll leverage a combination of business and technical skills to build strategic relationships with these key customers and become their go-to person and adviser in the data, analytics, and AI spaces. You'll also work with engineers... and business partners across Microsoft to define business scenarios and metrics, design optimal technical solutions, and ensure the solutions are delivered to market successfully.

This position will be located in New York, NY. Moderate domestic and international travel is required, 25-50%.

Qualifications

Basic qualifications:

7+ years software engineering and development experience. Expertise with relational and NoSQL databases, Hadoop, or Data Warehousing systems, and experience building end-to-end data solutions and pipelines. BS degree or higher in computer science or related field.

Preferred Qualifications

Experience driving strategic technical goals with customers or partners. Proven ability to collaborate across teams and across levels of management.

Excellent communication skills with ability to influence business and technical decision makers.

Ability to represent Microsoft’s data and analytics platform capabilities at events and conferences.

Requirements

These requirements include, but are not limited to the following specialized security screenings

Ability to meet Microsoft, customer and/or government security screening requirements are required for this role.

Microsoft Secure Screen: This position will be required to pass the Microsoft Cloud background check and credit history analysis upon hire/transfer and every year thereafter

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances.

We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.

Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.

#AZCAT",Microsoft,"New York, NY",$114k–176k,Senior Data Engineer
139.5,"1010data travels at the speed of thought to make Big Data discovery easy; we power sub second responses to analyses run on billions of rows of data.

An essential tool to more than 750 of the world’s top retail, distribution, manufacturing, telecom, government and financial services enterprises including The New York Stock Exchange, Dollar General, RiteAid, and Sysco; 1010data is a highly differentiated product that is becoming the industry standard for Big Data Discovery and Data Sharing.

As Software Data Engineer at 1010data, you will build and manage large scale ELT processes, managing data assets that we either create or administer for our clients.

You will primarily be working with data, creating, refactoring and monitoring scripts. This role is situated within the team that manages all ELT processes for clients and data assets internally, creating tools to help make this process easier and building enhancements to 1010data’s product.

This role will eventually progress into a... very integrate Data Engineering position, where you will take ownership of the ELT process for 1010data. Although experience in specific programming languages is not required, it’s important that you can program in order to understand how our system works.

With more than 25 trillion rows of data in our private cloud, 1010data is designed to scale to the largest volumes of granular data, the most disparate and varied data sets, and the most complex advanced analytics. All while delivering lightning-quick system performance.

If you appreciate the end-end ownership of managing ELT processes from scratch and are confident embracing new technologies, come evolve our product as we continue to change the rules of big data discovery.

Responsibilities
• Design and implement parallel processing systems that operate on multiple terabytes of data
• Manage enterprise ELT systems
• Maintain and support business critical applications
• Manage ELT processes for critical enterprise applications

Requirements

Education & Experience
• 2 years of professional experience excluding internships and academic work
• STEM Bachelor’s required, graduate degree is a big plus
• Examples of exciting internships and hobby projects

Skills
• Experience with distributed systems, info retrieval and/or Production experience is a big plus
• Experience with Apache Airflow is highly preferred
• Understanding of parallel and distributed systems
• Interest, willingness and demonstrated ability to learn new programming languages, with familiarity with functional/vector programming
• DBA experience is desirable, but not essential
Commercial Awareness
• SaaS experience is a plus
Personal Impact
• Passion for creating the best possible product
• Capable of working independently toward shared goals
• Positive, “roll-up your sleeves” attitude
• Strong intellectual curiosity
• Excellent interpersonal and communication skills
• Able to confidently present ideas to peers, management, and customers in both verbal and written form
__________________________________________________

Though we try our best to respond to everyone, sometimes due to the volume of applications received, only those selected for interviews will be contacted. If you really think we’ve missed the mark, please follow up to recruiting@1010data.com and let us know why you’re the perfect fit",1010data,"New York, NY",$90.3k–98.4k,Software Data Engineer
202.0,"JOB DESCRIPTION:

Lead Data Engineer

We’re the obstacle overcomers, the problem get-arounders. From figuring it out to getting it done… our innovative culture demands “yes and how!” We are UPS. We are the United Problem Solvers.

About Information Management at UPS Technology:
Our Information Management teams are responsible for designing and supporting data solutions to meet UPS’s rapidly changing business needs. Our team is comprised of individuals who are experts in data management, compliance and governance. We ensure quality, completeness, availability, protection, understanding and effective use of our data assets. Our ability to organize and design the wealth of data we receive each day provides the foundation which enables many of UPS’ core processes.

About this role:

The Lead Data Engineer interacts with Data Scientists, Business Owners, Business Data Analysts, Data Modelers, Architects, and Application Developers, to design, build and manage large-scale batch and... real-time data pipelines utilizing various data analytics processing frameworks in support of Data Science practices. He/She sets direction for data engineering best practices in continuous integration and delivery. This position is responsible for managing the integration of data from various data sources, both internal and external. They also perform ETL, data conversion and facilitate data cleansing and enrichment. This position supports I.S. leadership by planning and championing the execution of broad advanced analytics initiatives aimed at delivering value to internal and external stakeholders.

Leads the Development and Design of Data Engineering Projects
• Acts as subject matter expert on UPS business data processes, data rules, and metadata
• Ensures that projects follow UPS data management, metadata and data quality governance practices
• Establishes data validation and cleansing framework as it pertains data engineering and analytics
• Analyzes and solves problems at their root, stepping back to understand the broader context
• Designs efficient, flexible, extensible, and scalable ETL solutions
• Tunes application and query performance using profiling tools and SQL
• Supports the data science community by enabling data availability in environments that provide advanced analytical capabilities
• Maintains a broad understanding of implementation, integration, and inter-connectivity issues with emerging technologies to define data strategies
• Presents findings and recommendations to management to define problems, propose business cases and data strategies, and gain support
• Responsible for understanding and adhering to current security and regulatory compliance as it pertains to data
• Performs peer reviews of large and highly complex solutions with the stakeholders ensure requirements are met
• Creates repeatable solutions through written project documentation, process flowcharts, layouts, diagrams, charts, code comments, and clear code to produce datasets that can be used in analytics and/or predictive modeling

Provides Leadership for Data Engineering Projects
• Provides technical leadership for project teams to evaluate strategic alternatives, determine impact, recommend courses of action, and to design and implement solutions
• Recommends data engineering patterns that are recognized in the industry and are compliant with our security standards
• Provides technical guidance to ensure tasks are completed within established timeline, having established realistic timelines during the planning process
• Recognizes and adopts best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation
• Leads the improvement of ongoing data engineering processes, automating or simplifying self-service support for datasets

Directs Integration of Data for Data Engineering Projects
• Leads the analysis and integration of tools and methods to provide desired results from data engineering requirements
• Provides consultation of the data integration to stakeholders in order to support the design of advanced analytic environments
• Oversees procedures, maintenance, and support functions to monitor the advanced analytic environments for performance/integrity and to facilitate problem resolution

Leads Adoption of Emerging Technology Products and Tools
• Encourages the adoption of data engineering tools and techniques to derive useful information from data
• Keep up to date with advances in analytics technologies
• Assists teams in the training and use of data engineering processes and technologies

Minimum Qualifications:
• Bachelor’s degree in a technical field
• 8+ years of industry experience in IT or software-related roles
• Expert knowledge and experience with relational data systems and SQL
• Familiarity with Hadoop/Spark systems and related technologies such as HIVE
• Knowledge of cloud platforms such as GCP, Azure, etc
• Extensive hands-on experience with ETL and data engineering

Preferred Qualifications:
• Programming experience in R or Python
• Experience supporting data science initiatives
• Understanding of data governance procedures with a focus on data loss prevention

This position offers an exceptional opportunity to work for a Fortune 50 industry leader. If you are selected, you will join our dynamic technology team in making a difference to our business and customers. Do you think you have what it takes? Prove it! At UPS, ambition knows no time zone.

BASIC QUALIFICATIONS:
• Must be a U.S. Citizen or National of the U.S., an alien lawfully admitted for permanent residence, or an alien authorized to work in the U.S. for this employer
• Now or in the future UPS employment sponsorship, such as H1B, TN, J-1, F-1, etc., is not needed in order to start or continue temporary or permanent employment with UPS.
• Master's Degree in a quantitative or computational field such as statistics, operations research, computer science, physics or related discipline.
• Extensive expertise and experience in SQL
• Experience wrangling data from a relational database (e.g. Oracle) or distributed processing (e.g. Hadoop/Spark) system

OTHER CRITERIA: Employer will not sponsor visas for position.

UPS is an equal opportunity employer. UPS does not discriminate on the basis of race/color/religion/sex/national origin/veteran/disability/age/sexual orientation/gender identity or any other characteristic protected by law",UPS,"Mahwah, NJ",$114k–176k,Lead Data Engineer
,"Job Description

Lead Data Engineer

Intelletec has teamed up with one of the US's Leading Healthcare firms going through a massive transformation and are looking for a Lead Data Engineer, for their NYC or Massachusetts office.
• As a key member of the Data team, you’ll get in on the ground floor and help architect a brand-new platform powered by leading big data technologies.
• You’ll own your work and create far-reaching impact on an Analytics platform with real revenue from enterprise customers.
• You will have a broad impact and true ownership
• Data engineering team lead for large and complex projects involving multiple resources and tasks, providing individual mentoring in support of company objectives
• Designs and develops complex and large-scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs.
• Building real-time data pipelines using Redshift, S3, Kinesis, Spark structured streaming, Akka
• streams... and similar stacks on leading cloud platforms.
• Writes complex ETL (Extract / Transform / Load) processes, designs database systems and develops tools for realtime and offline analytic processing.

What You'll Bring
• 8+ years of related industry experience.
• MS / Ph. D in computer science or related field
• Advanced knowledge in Hadoop or Spark architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment
• Advanced knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar
• Experience with bash shell scripts, UNIX utilities & UNIX Commands
• Proficiency with technologies such as Scala, Python, Java, Redshift, Spark, and SQL
• Knowledge or familiarity with: Redshift, Amazon AWS, Postgres, Mysql, and/or JSON
• Mathematical aptitude ?and mindset
• Passion for taking huge amounts of data and making it into useful information

On Offer
• Competitive base salary + bonus & outstanding benefits package
• A tight-knit team of passionate people and a tech-first business
• Autonomy and end-to-end ownership
• Very competitive pay, equity, full medical, dental & vision benefits and more
• Opportunity for fast growth & promotion
• Opportunity to work in one of our other offices

NO REMOTE - NO CONTRACT - NO C2C - NO 3rd PARTIES",Intelletec,"New York, NY",,Lead Data Engineer - NYC or Boston
164.1,"Data Engineer
New York, NY
Cyber Security
Compensation: $130,000-$170,000

(Unlimited PTO, Daily Catered Lunches, Remote Work Options)

This innovative and well established Cyber Security organization is actively looking to add a Data Engineer to their Analytics team. They are looking for someone to come in and create new data architecture for BI solutions. The right candidate should be up to the challenge of blending the fast changing technology of big data analytics within the complex space of Cyber Security. You will report to the Senior Manager, who’s team assist the businesses decisions using Big Data technology and machine learning techniques. The team works out of NYC, which has modern office spaces and first class amenities.

You will be responsible for:
• Work closely with analysts and data scientists to transform threat analytics into production-level code.
• Participate in requirements gathering and transformation from prototype to product design.
• Helping us stay current... on the latest data processing tools and trends.
• Utilize machine learning, statistical data analytics, and predictive analytics to help implement analytics tied to cyber security and hunting methodologies and applications
• Applying latest technologies in machine learning, data mining, and predictive analytics to correlate the big datasets and events, and derive dynamic cybersecurity rules.
Minimum Qualifications:
• 4+ years industry experience focused on Data Science
• Experience writing and optimizing streaming and batch analytics.
• Proven knowledge of industry leading scripting tools such as Python, Powershell, R and SQL
• Strong hands-on programming skills, with expertise in multiple implementation languages/frameworks including a subset of Python, Java, and Scala with delivery background in middleware, and backend implementations.
• BS/BA in Computer Science, Engineering, or relevant field experience.
• Strong English verbal and written communication skills.

Ideal candidates will:
• Thrive in a small fast paced product driven enviornment
• Deliver features and fixes on tight schedules and under pressure
• Possess a strong interest or background in cyber security
• Create systems that are maintainable, flexible and scalable
• Experience with cloud based big data platforms such as AWS or Google a plus",Glocomms,"New York, NY",$91.6k–145k,Data Engineer
164.1,"Role: Big Data Engineer

Location: New York City, NY

Duration: Full Time (With Emids)

Job Description:

The ideal candidate is an experienced data pipeline builder who enjoys optimizing data systems and building them from the ground up. The Data Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing and/or re-designing our data architecture to support next generation of products and data initiatives.

Duties:

Create and maintain optimal data pipeline architecture, assemble large, complex data sets that meet functional / non-functional business requirements. -- Identify, design, and implement internal process improvements: automating manual processes... optimizing data delivery, etc.

Build the infrastructure required for optimal extraction, transformation, and loading of data from traditional/legacy data sources.

Work with stakeholders including the Management team, Product owners, and Architecture teams to assist with data-related technical issues and support their data infrastructure needs.

Create data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.

Qualifications:

Experience building and optimizing ‘big data’ data pipelines, architectures and data sets. Advanced hands-on SQL knowledge and experience working with relational databases for data querying and retrieval.

Experience with big data frameworks/tools: Hadoop, AWS, Kafka, Spark, etc. Experience with relational SQL and NoSQL databases, including Oracle, Hive, HBase.

Experience performing root cause analysis on data and processes to answer specific business questions and identify opportunities for improvement. Experience with data security.

Experience with building processes supporting data transformation, data structures, metadata, dependency and workload management.

Experience supporting and working with cross-functional teams in a dynamic environment. Experience with Java and/or Python a plus",emids,"New York, NY",$91.6k–145k,Big Data Engineer
202.0,"Senior Data Engineer Engineering New York, NY 10017, USA Cyndx is hiring an experienced and talented Senior Data Engineer! The Senior Data Engineer will construct a scalable, stable, and performant data infrastructure to support our multiple SaaS products and data science team. The Senior Data Engineer will collaborate with a team of engineers and product managers to identify and build data layer components and best-practice frameworks for data ingestion, cleansing, normalization, manipulation, and expunction. Responsibilities Develop software to ingest and store large datasets from varied sources Build ETL processes to translate data from native format to usable, application-ready data Collaborate across teams to gather data requirements for our applications Employ state-of-the-art frameworks and technologies to deliver reliable and maintainable data services Assist in real-time support of production services Participate in agile ceremonies such as sprint planning, daily standup... demos, and retrospectives Technologies SQL Python Git PostgreSQL, Neo4j, Redis Apache SPARK ElasticSearch AWS - VPC, EC2, S3, Glue, Redshift, Kinesis, RDS, Athena Qualifications 3+ years experience building and maintaining large scale data systems Expert-level knowledge of Python and SQL programming languages Demonstrated strength in data modeling, ETL development, and data warehousing Proficiency in Named Entity Resolution Bachelor's degree in computer science or engineering; master's degree preferred Perks and Benefits Competitive health care plans (medical, dental and vision) Paid time off and paid holidays 401(k) retirement savings program Complimentary lunch, snacks and drinks Brand new MacBook Pro when you start Dynamic, stimulating and open environment About Us Cyndx enables financial transactions through software. We don't simply connect companies with capital, we foster relationships between businesses and investors based on data-driven metrics and algorithms that predict success. We are a low-maintenance, high-performance team. Independence and autonomy is the norm. Our headquarters is located in New York City, where our team of engineers, designers and product managers collaborate to build the future of fundraising. We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status or disability status",Cyndx,"New York, NY",$114k–176k,Senior Data Engineer
143.0,"Data EngineerAnalyst GRT Corporation is seeking Data Engineer to join our team to create data pipelines on AWS for leading Brand Management Company. This individual will be responsible for implementing data ingestion system capable of processing and transformation data for robust Enterprise Datawartehouse. This is a contract to hire postion with a client. Responsibilities Organize business needs into ETLELT logical models and ensure data structures are designed for flexibility to support scalability of business solutions Craft and implement data pipelines utilizing Talend, Glue, Python Define and deliver reusable components for ETLELT framework. Define optimal data flow for system integration and data migration Integrate new data management technologies and software engineering tools into existing structures Qualifications 2+ yearsrsquo experience with ETLELT Pipelines, Experience with Talend Data Integration design, develop Talend ETL scripts, creating and deploying end to end Talend... Data Integration solution Strong knowledge and experience of SQL, Python Experience working with AWS tools - Data Pipeline, Glue, Lambda, Redshift, S3 Ability to design ETLELT solutions based on user reporting and archival requirements Strong sense of customer service to consistently and effectively address client needs Self-motivated comfortable working independently under general direction If you are interested, please apply to the positions providing the following Your currentdesired compensation Day time phone number Authorization status Regards, Justin Griffis HR Specialist GRT Corporation Stamford, CT 06901 Web httpwww.grtcorp.com httpwww.grtcorp.com",GRT Corporation,"San Diego, CA",$79.5k–127k,Data Engineer/Analyst
186.0,"Cloud Data Engineer:

Job Description:

This role will design, develop and deliver Data Solutions for data applications for operational reporting, data analytics, Data Science and partner integration to enable strategic decision making. As a Senior Member of Data Engineering team within the Data and Analytic organization. We will be responsible for the development and implementation of Data Development architecture, innovation is data engineering and supporting data related tasks for data sources across the enterprise. This individual will understand business requirements and help architect optimal data solutions in Google Cloud Platform. He / She will understand complex data requirements based on analysis of current systems and help enhance the existing data model to accommodate new data requirements. He / She will work cross-functionally with Data Analytics, Product, PMO, various Business Units, Enterprise Architecture & Data Warehousing teams to implement BI and Data related... initiatives. The right candidate will have experience in developing Data structures, processes in the BI & Data domain and be intimately familiar with software development lifecycle and Agile methodologies. This position requires excellent communication and partnering skills along with the ability to influence and lead solutions.

REQUIRED SKILLS:

Provide technical leadership and contribute to the definition, development, integration, test, documentation, and support across multiple platforms (Cloud, Big-data, RDBMS, etc.)

Analyze, assimilate and integrate multi-technology data systems by building a unified back-end

platform and a web-enabled front-end

Instill best practices for software development and documentation, assure designs meet

requirements, and deliver high-quality work on tight schedules

Involve in all phases of software development from the review of the functional specification through

assisting with test plans and final QA cycle. Actively participates in monitoring and troubleshooting of production platform related issues

Perform day-to-day tasks that ensure technology platform remains stable and available to users. Closely work with a cross-functional team to enhance systems, troubleshoot data issues, etc3+ years of experience and knowledge of (Cloud, API’s, Python, RDMS, Rub, PHP, Javascript, Docker, NodeJS, etc.)Bachelor’s degree in Computer Engineering or equivalent is desired4+ years of post-college working experience with full life cycle of ETL development in Data Integration projects using various ETL tool and Big Data technologies3+ years Linux/Unix/Perl experience, including scripting and version control with working knowledge of RDBMS (Prefer Oracle)3+ years of Unix development experience is required2+ years of Hadoop using Map Reduce or Hive or Pig or any other Big Data eco-system tool. Expertise in Shell scripting, system/process automation is required expertise in data research/analysis with a focus on data quality and consistency is required

Proficient with Service Oriented Architecture Principles - Micro Services, JSON Structures, SOA integration patternsUnderstand and have some experience working with cloud and other server-less technologiesExperience designing scalable cloud servicesExperience developing microservices on public cloud (Google Cloud, AWS, Azure, AliCloud)Experience developing highly available, scalable distributed systems expertise with Java, Python and other clouds compatible programming languagesExcellent communication skills for internal and external presentationsExperience with mentoring, architectural design reviews, and code reviews).Understands how to manage availability and performance. Understands how to manage and trade off latency and bandwidth in services",Stealth Mode Startup Company,"San Francisco, CA",$104k–164k,Cloud Data Engineer
226.5,"EasyPost is a San Francisco based start-up that provides an API to revolutionize the entire shipping process for e-commerce companies. Any online merchant can plug in our state of the art technology into their existing framework to provide a world-class shipping experience for consumers. Today, we help hundreds of leading e-commerce companies ship and track packages. We also have a fulfillment business that offers end to end shipping solutions.

We are looking for a Senior Data Engineers with experience in or who are comfortable in a polyglot environment to join the easypost team. You will be a key member of our small but growing engineering team making important technical decisions that will shape the company's future. If you love to code and work on unique challenges within a collaborative team of developers to build meaningful products, then we’d love to meet you!

About You:
• 5+ years of professional software development experience
• Expert in SQL and deep understanding of... relational databases
• Strong experience in performance tuning SQL and ETL pipelines in OLTP, OLAP, and Data Warehouse environments
• Expert knowledge of data architecture, data modeling, and building data tools for ETL
• Experience with modern big data technologies - e.g. Hadoop, Hive, Spark
• Integrations experience with REST, HTTP/HTTPS protocols, and 3rd party APIs a plus
• Strong desire to work in a fast-paced, start-up environment with multiple releases a day
• A passion for working as part of a team - you love connecting and collaborating with others

What We Offer:
• Competitive salary and equity
• Comprehensive medical, dental, and vision and commuter benefits
• Flexible work schedule and paid time off
• Collaborative culture with a supportive team
• The opportunity to make massive technical contributions at a fast-growing start-up
• A great place to work with unlimited growth opportunities

Do you want to come join us? You'd be a critical member of our growing engineering team to solve some of the hardest problems out there. Please send in an application and we will contact you.

EasyPost is proud to be an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status",EasyPost,"San Francisco, CA",$128k–197k,Senior Data Engineer
164.1,"About Your Role

As a Data Engineer, you will empower users at Dotdash to make data-driven decisions and gain valuable insights into our business. The DataOps team facilitates the free and open use of data across the organization and encourages data culture through weekly meetings, outreach and technical consulting. We are a small, passionate team looking for a data geek with a broad skill set and data chops.

About Your Contributions
• Build and maintain data integrations
• Deploy code and machines to AWS to create and improve our existing data tools
• Curate our Data Lake to constantly improve our user experience
• Understand scalability issues and adapt systems to evolving user and business needs
• Find ways to use new and exciting technology to solve engineering challenges

About You
• Experience building data integrations with Python, Spark or Java
• Experience with engineering in the Cloud – preferably on AWS
• Experience with Hadoop, Hive, Spark and/or AWS EMR
• Comfortable... writing SQL queries to analyze data
• Proficient in the Linux CLI
• Ansible, Puppet, Chef experience a plus
• Strong analytical and problem solving skills
• Bachelor’s degree in Computer Science, Computer Engineering, Information Technology, or a closely related technical field or foreign equivalent

About Us

For more than 20 years, Dotdash brands have been helping people find answers, solve problems, and get inspired. We are one of the top-20 largest content publishers on the Internet according to comScore, a leading Internet measurement company, and reach more than 30% of the U.S. population every month. Our brands collectively have won more than 20 industry awards in the last year alone and, most recently, Dotdash was named Publisher of the Year by Digiday, a leading industry publication. Our brands include Verywell, The Spruce, The Balance, Investopedia, Lifewire, Byrdie, MyDomaine, TripSavvy and ThoughtCo.

Dotdash embraces inclusivity and values our diverse community. We are committed to building a team based on qualifications, merit and business need. We are proud to be an equal opportunity employer and do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status",Dotdash,"New York, NY",$91.6k–145k,Data Engineer
83.5,"At Bloomberg, our products are fueled by powerful information. We combine data and context to paint the whole picture for our clients, around the clock – from around the world. In Global Data, we're responsible for delivering this data, news and analytics through innovative technology - quickly and accurately.

Our team is at the forefront of designing, building and supporting the systems that power the contribution of research content to the Bloomberg platform. We partner closely with our team of technical account managers, content specialists, research contributors and business stakeholders to deliver creative software tools and solutions every day. This is not just any python development role! Your multifaceted abilities and efforts will help us on-board new providers in diverse markets around the globe, and empower content acquisition professionals to identify new possibilities and opportunities for research data content.

What's the role?

We have an exciting opportunity for an... astute technical and client oriented professional like you, to join our New York team. This Python development role is supported by a unique three month market experience program working in our research contributions team. This will allow you to get first-hand experience with all the challenges faced by our research business. Through this program, you will become fully equipped with the knowledge necessary to take full end to end ownership of identifying problems in the contributed research space through data analysis and product discovery techniques. This will lead you to design and build systems using modern Python in a micro- services architecture that solve these problems and enable you to directly realize a significant and measurable impact to the business.

For the first three months, you will:
• Gain exposure to investment research content acquisition. You'll learn how we get it, how we selectively distribute it, and how our customers discover it.
• Learn about our customers, product offerings, and operational best practices.
• Proactively partner with internal customers and utilize business intelligence to help spot trends, identify gaps and opportunities.
• Ensure content is delivered to Bloomberg in the optimized fashion.
• Own implementation of new content projects from start to finish.
• Collaborate with content providers to suggest areas of improvements as it relates to headline composition, tagging and content positioning to our mutual clients.
• Build influential and meaningful working relationships.

After this rotation, we'll trust you to:
• Develop a complete understanding of our problems by being deeply embedded with our business teams, meeting clients and data contributors and learning how the world's largest financial data operation works.
• Be empowered to solve business problems through quantifiable objectives, with the freedom to design workflows and select the right technologies for the job.
• Write code to develop robust and scalable technology solutions for data management problems using modern micro services, databases and user interface technology through test-driven and iterative processes.

As a valued member of our team, you'll need to have:
• A BA/BS degree or higher in Engineering, Information Systems, Mathematics, or relevant data technology field, or equivalent experience.
• Up to two years of professional work experience in information technology, engineering, or data analysis.
• Minimum 2 years of production level coding, with advanced Python programming proficiency outside of academia.
• Aptitude for problem solving, particularly to modify and enhance processes and workflows.
• Communication, communication, communication! (Especially when explaining technical processes and solutions to customers internally and externally).
• System design skills with competency both in back-end and UI.
• Passion for data, and the know-how to wrangle large amounts of data.
• Knowledge of tests, linting and version control.
• Willingness to be outside of your comfort zone, and adaptability.
• A can-do attitude with intellectual curiosity.
• Ability to legally work in the US without visa sponsorship.

We'd love to see:
• Understanding of the financial markets, in particular sell-side content distribution.
• Knowledge of Tableau, QlikSense or similar technologies.

Does this sound like you?
Apply! We'll get in touch and let you know the next steps.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status",Bloomberg LP,"New York, NY",$25.5k–116k,Data Engineer - Contributed Research
