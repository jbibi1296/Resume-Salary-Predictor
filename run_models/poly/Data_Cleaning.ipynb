{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning (Poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- ## [Data Cleaning](./Data_Cleaning.ipynb)\n",
    "    - ### [Import Libraries](./Data_Cleaning.ipynb#Import-Libraries)\n",
    "    - ### [Import Data](./Data_Cleaning.ipynb#Import-Data)\n",
    "    - ### [Clean the \"Average Salary\" Column](./Data_Cleaning.ipynb#Clean-the-Average-Salary-Column)\n",
    "    - ### [Create Stop Words](./Data_Cleaning.ipynb#Create-Custom-Stop-Words)\n",
    "    - ### [Prepare words to be vectorized](./Data_Cleaning.ipynb#Tokenize%2C-Remove-Stop-Words%2C-Remove-Punctuation%2C-Lemmatize)\n",
    "    - ### [Vectorize Word Data](#Vectorize-Word-Data)\n",
    "- ## [Modeling](./Models.ipynb)\n",
    "    - ### [Import Libraries](./Models.ipynb#Import-Libraries)\n",
    "    - ### [Models](./Models.ipynb#Models)\n",
    "      - #### [Linear Regression](./Models.ipynb#Linear-Regression)\n",
    "      - #### [Lasso](./Models.ipynb#Lasso)\n",
    "      - #### [Ridge](./Models.ipynb#Ridge)\n",
    "      - #### [Random Forest Regressor](./Models.ipynb#Random-Forest-Regressor)\n",
    "      - #### [Gradient Boost Regressor](./Models.ipynb#Gradient-Boost-Regressor)\n",
    "      - #### [Neural Network](./Models.ipynb#Neural-Network))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, HashingVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import clean as cl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs_df = pd.read_csv('../../get_data/Jobs/full_jobs_df.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Average Salary Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs_df['avg_salary']=[cl.avg(i) for i in all_jobs_df.salary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop all rows with Null values in the Salary Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_jobs_df.dropna(subset = ['salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop = True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list_of_stop_words = {'york'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stop_words(word):\n",
    "    my_list_of_stop_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ENGLISH_STOP_WORDS:\n",
    "    add_stop_words(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['york','religion','identity','sexual','orientation','veteran','ll',\n",
    "          'status','equal','national','gender','expression','real','affirmative',\n",
    "          'race', 'color','age', 'belief', 'chance', 'disability', 'ethnic',\n",
    "          'fair', 'lyft', 'nationality',\n",
    "          'ordinance', 'ordinances', 'origin', 'policy',\n",
    "          'prohibited', 'pursuant', 'sex','kind','conviction']:\n",
    "    add_stop_words(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_extras = '''is an Equal Employment Opportunity that proudly \n",
    "and hires a diverse does not make hiring or   on the basis of race, color, religion or religious belief, \n",
    "ethnic or national origin, nationality, sex, gender, gender-identity, \n",
    "sexual orientation, disability, age, military or veteran status, or any \n",
    "other basis protected by local, state, or federal laws or \n",
    "prohibited by policy. also strives for a healthy and safe \n",
    "and strictly prohibits harassment of any kind Pursuant to the \n",
    "Fair Chance Ordinance and other similar state laws and local \n",
    "ordinances, and its policy, will also consider for applicants with arrest and\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in basic_extras.split(\" \"):\n",
    "    add_stop_words(i.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( my_list_of_stop_words, open( \"../models/word cleaning/custom_stop_words.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize, Remove Stop Words, Remove Punctuation, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_cleaned = [cl.token_stop_lemm(i,my_list_of_stop_words) for i in df.body]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = corpus_cleaned\n",
    "pickle.dump( X, open( \"../corpus_for_skills.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Word Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = TfidfVectorizer(stop_words = my_list_of_stop_words,ngram_range=(1,3),min_df=.15,max_df = .9)\n",
    "cvec.fit(X)\n",
    "vectors = cvec.transform(X)\n",
    "\n",
    "word_df = pd.DataFrame(data = vectors.todense(), columns = cvec.get_feature_names())\n",
    "\n",
    "pickle.dump( cvec, open( \"../models/word cleaning/body.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_titles = TfidfVectorizer(stop_words = my_list_of_stop_words,min_df = .05,ngram_range=(1,2))\n",
    "cvec_titles.fit(df.title)\n",
    "vectors_titles = cvec_titles.transform(df['title'])\n",
    "\n",
    "title_df = pd.DataFrame(data = vectors_titles.toarray(), columns = cvec_titles.get_feature_names())\n",
    "\n",
    "pickle.dump( cvec_titles, open( \"../models/word cleaning/title.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_location = TfidfVectorizer(stop_words = my_list_of_stop_words,min_df = .05,ngram_range=(2,4))\n",
    "cvec_location.fit(df.location)\n",
    "vectors_location = cvec_location.transform(df['location'])\n",
    "\n",
    "location_df = pd.DataFrame(data = vectors_location.toarray(), columns = cvec_location.get_feature_names())\n",
    "\n",
    "pickle.dump( cvec_location, open( \"../models/word cleaning/location.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = word_df\n",
    "full_df = pd.merge(full_df,title_df,how = 'outer', left_index=True,right_index=True)\n",
    "full_df = pd.merge(full_df,location_df,how = 'outer', left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree = 2)\n",
    "\n",
    "X = poly.fit_transform(full_df)\n",
    "\n",
    "pickle.dump( poly, open( \"../models/poly/poly_features.pkl\", \"wb\" ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(.95)\n",
    "\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "pickle.dump( pca, open( \"../models/poly/pca.pkl\", \"wb\" ),protocol=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set X and y Variables\n",
    "- Because y is a dollar value, we will be taking the natural log of it\n",
    "- When we predict the price, we will be using $e ^ n$ with $n$ being the predicted value from our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X_pca)\n",
    "y = [np.log(i) for i in df.avg_salary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( X, open( \"../models/poly/X_95.pkl\", \"wb\" ) )\n",
    "pickle.dump( y, open( \"../models/poly/y.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go To:\n",
    "[Original Modeling Process](../original/Models.ipynb) \n",
    "\n",
    "[Original Data Cleaning](../original/Data_Cleaning.ipynb)\n",
    "\n",
    "[Poly Modeling Process](../poly/Models.ipynb)\n",
    "\n",
    "[Poly Data Cleaning](../poly/Data_Cleaning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
